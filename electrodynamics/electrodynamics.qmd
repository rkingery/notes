# Electrodynamics

In the previous chapter we started moving away from statics, and started our study of electrodynamics, the study of time-varying electric and magnetic fields. We finished that chapter with the following quasistatic field equations
$$
\begin{align*}
&\nabla \cdot \mathbf{E} = 4\pi \rho \ ,\\
&\nabla \times \mathbf{E} = -\frac{1}{c} \frac{\partial\mathbf{B}}{\partial t} \ ,\\
&\nabla \cdot \mathbf{B} = 0 \ ,\\
&\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} \ . \\
\end{align*}
$$
In this chapter we will continue on with the topic of electrodynamics by allowing the final equation for $\nabla \times \mathbf{B}$ to depend on a time-varying E-field, just as in the previous chapter we modifying $\nabla \times \mathbf{E}$ to depend on a time-varying B-field. This will lead us to the notion of displacement current, and finally to Maxwell's equations, the crown jewel of electromagnetism.

Once we finally derive Maxwell's equations we will study their implications pretty much for the rest of this course. We will start by studying the conservation laws they imply for charge, energy, and momentum. We will then to show how Maxwell's equations become wave equations, which leads to the topic of electromagnetic radiation, something that we'll see in a future chapter is the underlying theory of classical optics.

From now on in this course, we'll always assume that all fields and distributions depend explicitly on time unless otherwise stated. That is, we'll assume that $\mathbf{E} = \mathbf{E}(\mathbf{x},t)$ and $\mathbf{B} = \mathbf{B}(\mathbf{x},t)$, and that $\rho = \rho(\mathbf{x},t)$ and $\mathbf{J} = \mathbf{J}(\mathbf{x},t)$ as well. This pulls us away from statics and puts us squarely in the realm of electrodynamics. An immediate implication of this is that many of the expressions we derived before for the fields in terms of source distributions will no longer apply, in particular Coulomb's law and the Biot-Savart law. We'll see in later chapters how these formulas can be modified to account for electrodynamics.

## Maxwell's Equations

Before studying the implications of Maxwell's equations we first need to finalize them by introducing the *displacement current*, which modifies the equation for $\nabla \times \mathbf{B}$ to allow for a time-varying E-field. 

### Displacement Current

In the mid 19th century, Maxwell observed that the quasistatic equations as modified by Faraday were mathematically inconsistent with the most fundamental principle of electromagnetism, namely conservation of charge. Specifically, Maxwell noticed that Ampere's law was inconsistent with the continuity equation
$$
\frac{\partial \rho}{\partial t} + \nabla \cdot \mathbf{J} = 0 \ .
$$
To see why this is the case, recall that the differential form of Ampere's states that
$$
\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} \ .
$$
If we take the divergence of both sides of this equation, we get
$$
\nabla \cdot (\nabla \times \mathbf{B}) = \frac{4\pi}{c} \nabla \cdot \mathbf{J} \ .
$$
Now, the divergence of a curl must always vanish, which means the left-hand side must be zero. We thus must evidently conclude from Ampere's law that
$$
\nabla \cdot \mathbf{J} = 0 \ .
$$
But, as we saw in the chapter on magnetostatics, this can only be consistent with conservation of charge when the current density is time-independent. Of course, there's no reason at all to assume the current density must always be time-independent. In fact if it is we necessarily get a steady current, which means the B-field must be time-independent, which necessarily conflicts with Faraday's law.

To resolve this problem, Maxwell proposed adding a new term to Ampere's law, which he called the *displacement current*,
$$
\nabla \times \mathbf{B} = \frac{4\pi}{c} (\mathbf{J} + \mathbf{J}_d) \ .
$$
If we now take the divergence of both sides as we did above, we get
$$
0 = \nabla \cdot \mathbf{J} + \nabla \cdot \mathbf{J}_d \ .
$$
We can now make this equation consistent with the continuity equation by insisting that
$$
\nabla \cdot \mathbf{J}_d = \frac{\partial \rho}{\partial t} \ .
$$
But since Gauss's law requires that $\nabla \cdot \mathbf{E} = 4\pi\rho$, we also have
$$
\nabla \cdot \mathbf{J}_d = \frac{1}{4\pi} \frac{\partial}{\partial t} \nabla \cdot \mathbf{E} \ .
$$
This means, up to an additive constant that we can set to zero, the displacement current must then be
$$
\mathbf{J}_d = \frac{1}{4\pi} \frac{\partial \mathbf{E}}{\partial t} \ .
$$
Plugging this into the modified Ampere's law above, we finally have
$$
\boxed{
\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t}
} \ .
$$
With this modification, we've thus managed to make the field equations consistent with the conservation of charge. Just as Faraday's law allows $\nabla \times \mathbf{E}$ to depend on a time-varying E-field, this modified Ampere's law allows $\nabla \times \mathbf{B}$ to depend on a time-varying E-field, thus coupling together the four field equations and making them self-consistent.

So why wasn't the displacement current noticed in the experiments that established Ampere's law. Why did it take Maxwell's use of theory to establish its existence? Fundamentally, the reason has to do with the fact that the displacement current in typical settings is much smaller than the ordinary current. To see why this is the case, consider the following dimensional analysis.

Suppose the EM fields both change on some characteristic distance scale $L$ and some characteristic time scale $T$. In terms of these dimensions, the generalized Ampere's law essentially says
$$
\frac{B}{L} \sim \frac{J}{c} + \frac{E}{cT} \ .
$$
Similarly, Faraday's law says
$$
\frac{E}{L} \sim \frac{B}{cT} \ .
$$
Now, in typical experimental settings we have $B/L \sim J/c$. Putting all of this information together we then have
$$
\frac{B}{L} \sim \bigg[1 + \bigg(\frac{L}{cT}\bigg)^2\bigg] \frac{B}{L} \ .
$$
Thus, for the displacement current to be experimentally significant in a lab setting we'd need $L/cT \sim 1$, or
$$
L \sim cT \ .
$$
Now, in a typical lab setting the distance scales of measurement are often fairly small, usually on the order of meters or less, and the time scales of measurement are usually on the order of seconds. This means $L/T$ is usually on the order of $1 \ \text{m}/\text{s}$ or so. But the speed of light $c$ is on the order of $10^{8} \ \text{m}/\text{s}$, meaning $L/T \ll c$. Thus, for all practical purposes, in most lab experiments we can completely neglect the displacement current and just use the quasistatic equations instead.

There are two situations where we can't neglect the displacement current. One is when doing measurements on large distance scales, say on the order of miles. Eventually, $L$ will be large enough so that $L \sim cT$, so as long as we measure far enough away we will need to worry about the displacement current contribution. The other scenario is when $L/T \sim c$. This is essentially a relativistic limit, where the effects of special relativity start to become important.

### Maxwell's Equations

With the final modification to Ampere's law in place, we're finally able to state *Maxwell's Equations*, the fundamental field equations of classical electrodynamics. In a nutshell, these equations say that E-fields are caused by exactly two things: charges and changing B-fields. Similarly, B-fields are caused by exactly two things: currents and changing E-fields. 

In differential form, Maxwell's equations are

$$
\boxed{
\begin{align*}
&\nabla \cdot \mathbf{E} = 4\pi \rho \\
&\nabla \times \mathbf{E} = -\frac{1}{c} \frac{\partial \mathbf{B}}{\partial t} \\
&\nabla \cdot \mathbf{B} = 0 \\
&\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \\
\end{align*}
} \ .
$$

We can also state Maxwell's equations in integral form if we like by using the divergence and Stokes theorems in reverse,
$$
\begin{align*}
&\oint \mathbf{E} \cdot d\mathbf{a} = 4\pi Q_{\text{enc}} \ , \\
&\oint \mathbf{E} \cdot d\boldsymbol{\ell} = -\frac{1}{c} \frac{d\Phi_B}{dt} \ , \\
&\oint \mathbf{B} \cdot d\mathbf{a} = 0 \ , \\
&\oint \mathbf{B} \cdot d\boldsymbol{\ell} = \frac{4\pi}{c} I_{\text{enc}} + \frac{1}{c} \frac{d\Phi_E}{dt} \ . \\
\end{align*}
$$
Here $Q_{\text{enc}}$ refers as usual to the charges enclosed in a closed surface, and $I_{\text{enc}}$ to the currents enclosed inside a closed loop, while $\Phi_E$ and $\Phi_B$ refers to the flux of electric and magnetic fields through any surface bounded by the closed loop.

Maxwell's equations provide us with eight linear partial differential equations for the EM fields, two equations from the divergences and six equations from the curls. The entire dynamics of the EM fields is contained in these equations.

In fact, Maxwell's equations almost complete the theory of classical electromagnetism. There are only a few other things we need to add to complete the theory. First, we need to say how the fields modify the dynamics of moving particles. This coupling between particles and fields is of course is done via the Lorentz force law,
$$
\mathbf{F} = q\mathbf{E} + q\frac{\mathbf{v}}{c} \times \mathbf{B} \ .
$$
Second, we need to say something about how the fields get modified in the presence of electromagnetic materials. This we will spend a whole chapter on. Last, we need to say something about how exactly the source charges give rise to the fields. This is known as *causality*. It's one of the most fundamental principles in physics. Causality states that past actions can impact future actions, but future actions can never impact past actions.

### Time Reversal

To see why causality is an issue we will now show that Maxwell's equations have no sense of the arrow of time, of what causes what. That is, they're *time-reversal invariant*, meaning the equations remain unchanged under a *time reversal* transformation $t \to -t$. To show this is the case, we first need to figure out how the fields and distributions behave under time reversal.

First let's ask how the position $\mathbf{x}$ changes under time reversal. Since the position is only a function of the underlying coordinate system, under a time reversal the position $\mathbf{x}(t)$ of any particle must remain unchanged. That is, $\mathbf{x}(t) \to \mathbf{x}(-t)$. In particular, this means that the *velocity* $\mathbf{v}(t)$ of a particle must transform under time reversal as
$$
\mathbf{v}(t) = \frac{d\mathbf{x}(t)}{dt} \to \frac{d\mathbf{x}(-t)}{d(-t)} = -\frac{d\mathbf{x}(t)}{dt} = -\mathbf{v}(-t) \ .
$$
Similarly, the *acceleration* $\mathbf{a}(t)$ of a particle must transform under time reversal as
$$
\mathbf{a}(t) = \frac{d^2\mathbf{x}(t)}{dt^2} \to \frac{d^2\mathbf{x}(-t)}{d(-t)^2} = \frac{d^2\mathbf{x}(t)}{dt^2} = \mathbf{a}(-t) \ .
$$
Thus, the position and acceleration of a particle remain *invariant* under time-reversal, while the velocity of a particle changes sign.

What about scalar quantities like the mass $m$ or charge $q$ of a moving particle? Since both mass and charge are conserved (at least in classical dynamics), they can't suddenly change with time, meaning these must also be invariant under time reversal. By implication, this means the charge density $\rho(\mathbf{x},t)$ must be invariant as well since it's proportional to charge. In fact, *all* scalar quantities must be time-reversal invariant.

The current density $\mathbf{J}(\mathbf{x},t)$, however, is a different story. Since $\mathbf{J} = \rho\mathbf{v}$, we must have the current density like the velocity must change sign under time reversal,
$$
\mathbf{J}(\mathbf{x},t) \to -\mathbf{J}(\mathbf{x},-t) \ .
$$
Now that we know how the distributions behave under time reversal, we can now ask how the fields behave. By Gauss's law we must have $\nabla \cdot \mathbf{E} = 4\pi\rho$. Since both position and charge density are invariant, the E-field must evidently be invariant as well,
$$
\mathbf{E}(\mathbf{x},t) \to \mathbf{E}(\mathbf{x},-t) \ .
$$
We might suspect the B-field to be invariant as well, but surprisingly that's not the case. By the generalized Ampere's law we have
$$
\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \ .
$$
Since $\mathbf{J}$ reverses direction under time reversal and $\mathbf{B}$ is proportional to $\mathbf{J}$, it too must reverse sign,
$$
\mathbf{B}(\mathbf{x},t) \to -\mathbf{B}(\mathbf{x},-t) \ .
$$
We've thus now established how the distributions and fields behave under time reversal, which means we can now ask how Maxwell's equations themselves behave. For Gauss's law, we have
$$
\nabla \cdot \mathbf{E}(\mathbf{x},-t) = 4\pi \rho(\mathbf{x},-t) \ .
$$
This means Gauss's law is invariant under time reversal, meaning it has the same functional from.

Next we have Faraday's law, which under a time reversal transformation gives
$$
\nabla \times \mathbf{E}(\mathbf{x},-t) = -\frac{1}{c} \frac{\partial}{\partial(-t)} \big[-\mathbf{B}(\mathbf{x},-t)\big] \ .
$$
Since the minus signs cancel, we see that Faraday's law is invariant under time reversal as well.

Next we have Gauss's law for magnetism, which under time reversal gives
$$
-\nabla \cdot \mathbf{B}(\mathbf{x},-t) = 0 \ .
$$
Since we can multiply both sides by $-1$, we see that this equation is also invariant.

Finally we have the generalized Ampere's law, which under time reversal says
$$
-\nabla \times \mathbf{B}(\mathbf{x},-t) = \frac{4\pi}{c} \big[-\mathbf{J}(\mathbf{x},-t)\big] + \frac{1}{c} \frac{\partial}{\partial(-t)} \mathbf{E}(\mathbf{x},-t) \ .
$$
Again, multiplying both sides by $-1$ we see that this equation is invariant as well. We've thus managed to show that all six of Maxwell's equations are time-reversal invariant, and by implication that the continuity equation is also invariant.

For what it's worth, the Lorentz force law is also time-reversal invariant, since by Newton's second law we have
$$
m\mathbf{a}(\mathbf{x},-t) = q\mathbf{E}(\mathbf{x},-t) + q\frac{\mathbf{v}(\mathbf{x},-t)}{c} \times \mathbf{B}(\mathbf{x},-t) \ .
$$
Here, everything is invariant except $\mathbf{v}$ and $\mathbf{B}$, but the minus signs in $\mathbf{v}$ and $\mathbf{B}$ cancel out, leaving the Lorentz force law invariant. 

Taken together, these mean that neither Maxwell's equations nor the Lorentz force law contain any understanding of the arrow of time, of what causes what. That is, they have no notion of causality. However, we intuitively expect that it's the source charges that should give rise to the fields and not the other way around. If we want to bake this requirement into the theory we'll need to do it in some other way, which we'll discuss later.

## Electrodynamic Potentials

Just as in statics it was useful to express the EM fields in terms of potentials, it'll be useful to do the same in electrodynamics. Recall that we derived the scalar potential $\phi(\mathbf{x})$ from the assumption that $\nabla \times \mathbf{E} = \mathbf{0}$, and the vector potential $\mathbf{A}(\mathbf{x})$ from the assumption that $\nabla \cdot \mathbf{B} = 0$. Provided these equations hold, we can write
$$
\mathbf{E} = -\nabla \phi \quad , \quad \mathbf{B} = \nabla \times \mathbf{A} \ .
$$
As we'll see, we need to modify these relations somewhat in electrodynamics.

### Electrodynamic Potentials

In electrodynamics it's still true that $\nabla \cdot \mathbf{B} = 0$ even for time-varying B-fields, which means we can still find a time-varying vector field $\mathbf{A}(\mathbf{x},t)$ such that $\mathbf{B} = \nabla \times \mathbf{A}$. We will still refer to $\mathbf{A}(\mathbf{x},t)$ as the *vector potential*.

However, it's no longer true that $\nabla \times \mathbf{E} = \mathbf{0}$ for time-varying E-fields, which means we no longer find a time-varying scalar field $\phi(\mathbf{x},t)$ such that $\mathbf{E} = -\nabla \phi$. However, we can still find a related vector field whose curl vanishes. Consider Faraday's law,
$$
\nabla \times \mathbf{E} = -\frac{1}{c} \frac{\partial \mathbf{B}}{\partial t} \ .
$$
If we write $\mathbf{B} = \nabla \times \mathbf{A}$ and move everything to the left-hand side, we have
$$
\nabla \times \bigg(\mathbf{E} + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t}\bigg) = 0 \ .
$$
Since we now have a vector field whose curl vanishes, we can find a scalar field $\phi(\mathbf{x},t)$ such that
$$
\mathbf{E} + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t} = -\nabla \phi \ .
$$
This means we can express $\mathbf{E}(\mathbf{x},t)$ in terms of the time-varying scalar and vector potentials as
$$
\mathbf{E} = -\nabla \phi - \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t} \ .
$$
As before, we call $\phi(\mathbf{x},t)$ the *scalar potential*. Thus, in electrodynamics we relate the EM fields to the potentials by
$$
\boxed{
\begin{align*}
\mathbf{E} &= -\nabla \phi - \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t} \\ 
\mathbf{B} &= \nabla \times \mathbf{A}
\end{align*}
} \ .
$$
The dynamical behavior of these potentials must then be satisfied by the remaining two Maxwell equations.

### Field Equations for Potentials

We still have two more of Maxwell's equations that the potentials need to satisfy, namely the inhomogeneous equations
$$
\nabla \cdot \mathbf{E} = 4\pi \rho \quad , \quad \nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \ .
$$
If we plug the expression for $\mathbf{E}$ in terms of the potentials into the first equation, we must have
$$
\nabla \cdot \bigg(\nabla \phi + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t}\bigg) = -4\pi \rho \ .
$$
Simplifying the left-hand side, this gives the following differential equation for the scalar potential,
$$
\nabla^2 \phi + \frac{1}{c} \frac{\partial}{\partial t} \nabla \cdot \mathbf{A} = -4\pi \rho \ .
$$
Similarly, if we plug the expressions for both $\mathbf{E}$ and $\mathbf{B}$ into the second inhomogeneous equation, we have
$$
\nabla \times (\nabla \times \mathbf{A}) = \frac{4\pi}{c} \mathbf{J} - \frac{1}{c} \frac{\partial}{\partial t} \bigg(\nabla \phi + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t}\bigg) \ .
$$
Since $\nabla \times (\nabla \times \mathbf{A}) = \nabla (\nabla \cdot \mathbf{A}) - \nabla^2 \mathbf{A}$, upon rearranging we get
$$
\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} - \nabla \bigg(\nabla \cdot \mathbf{A} + \frac{1}{c} \frac{\partial \phi}{\partial t} \bigg) = -\frac{4\pi}{c} \mathbf{J} \ .
$$
As things stand, the differential equations for $\phi$ and $\mathbf{A}$ look quite different, but notice each equation contains a term proportional to $\nabla \cdot \mathbf{A}$, which means we can potentially make them have the same form by exploiting gauge invariance.

### Gauge Invariance

Recall that we can always add to the vector potential the gradient of any scalar field $\chi$ without changing the underlying B-field,
$$
\mathbf{A}' = \mathbf{A} + \nabla \chi \ .
$$
This is called a *gauge transformation*. This follows from the relation $\mathbf{B} = \nabla \times \mathbf{A}$ along with the fact that the curl of any gradient is always zero. This is still true in electrodynamics, except now when we do a gauge transformation we have to be careful not to alter the E-field as well. The only way we can ensure this is to require that $\phi$ also gauge transform simultaneously as
$$
\phi' = \phi - \frac{1}{c} \frac{\partial \chi}{\partial t} \ .
$$
Thus, in electrodynamics, a gauge transformation is any transformation on both the scalar and vector potentials of the form
$$
\begin{align*}
\phi' &= \phi -\frac{1}{c} \frac{\partial \chi}{\partial t} \ , \\ 
\mathbf{A}' &= \mathbf{A} + \nabla \chi \ .
\end{align*}
$$
As we mentioned in magnetostatics, we can uniquely specify a given gauge by fixing $\chi$, or equivalently by fixing $\nabla \cdot \mathbf{A}$. This ability to specify the divergence of the vector potential gives us a gauge freedom we can use to simplify various formulas without altering any of the underlying physics. We can fix $\nabla \cdot \mathbf{A}$ to be anything we like so long as we're consistent within a given theory.

In magnetostatics we fixed the gauge by choosing the *Coulomb gauge* $\nabla \cdot \mathbf{A} = 0$, which was a natural choice in that setting. We could still do that in electrodynamics as well if we like. If we did, the differential equations for the potentials could be written
$$
\begin{align*}
\nabla^2 \phi &= -4\pi \rho \ , \\
\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} &= -\frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \nabla \frac{\partial \phi}{\partial t} \ .
\end{align*}
$$
This choice of gauge makes the equation for the scalar potential especially simple. We just recover Poisson's equation, the same equation we saw in electrostatics. In the absence of boundary conditions, the solution to Poisson's equation is just given by
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \frac{\rho(\mathbf{x},t)}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
However, the equation for the vector potential $\mathbf{A}$ is much more complex. It's coupled to the scalar potential $\phi$, which means we need to first solve for $\phi$ before we can solve for $\mathbf{A}$. Assuming we've done so, we can treat $4\pi\mathbf{J}_\ell \equiv \nabla \partial_t \phi$ as a kind of current and solve for $\mathbf{A}$ that way.

While this is physically allowed though it's not intuitively pleasing. For one thing, we'd like the equations for $\phi$ and $\mathbf{A}$ to have the same form if possible. But more importantly, Poisson's equation when $\phi$ is time dependent is *instantaneous*. It suggests that the scalar potential can propagate at infinite speeds, which violates special relativity if we insist $\phi$ have any physical meaning at all.  Thus, in electrodynamics we'd prefer to choose a different gauge that fixes both of these issues.

By staring at the original differential equations for the potentials in the previous section one can identify such a gauge,
$$
\boxed{
\nabla \cdot \mathbf{A} = - \frac{1}{c} \frac{\partial \phi}{\partial t} 
} \ .
$$
This choice of gauge is called the *Lorentz gauge*, because it makes both potentials Lorentz invariant. That is, it ensures that both potentials can only propagate at finite speeds. In this gauge, the differential equations for the potentials can be written
$$
\boxed{
\begin{align*}
&\nabla^2 \phi - \frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = -4\pi \rho \\ 
&\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} = -\frac{4\pi}{c} \mathbf{J}
\end{align*}
} \ .
$$
Differential equations of this form are known as *inhomogeneous wave equations*. As we'll soon see, functions satisfying this kind of partial differential equation necessarily must propagate at a finite speed, namely $c$.

To see what kind of condition the Lorentz gauge places on the gauge potential $\chi$, suppose
$$
\nabla \cdot \mathbf{A}' = - \frac{1}{c} \frac{\partial \phi'}{\partial t} \ ,
$$
where $\mathbf{A}' = \mathbf{A} + \nabla \chi$ and $\phi' = \phi - \partial_t \chi / c$ is some gauge transformation. Plugging in and simplifying, we get
$$
\nabla^2 \chi - \frac{1}{c^2} \frac{\partial^2 \chi}{\partial t^2} = -\nabla \cdot \mathbf{A} - \frac{1}{c} \frac{\partial \phi}{\partial t} \ .
$$
Thus, if we have potentials $\phi$ and $\mathbf{A}$ that don't satisfy the Lorentz gauge, we can always convert them into potentials that do by finding a gauge potential $\chi$ that solves this inhomogeneous wave equation.

## Wave Equation

We now want to better understand the solutions of the wave equation, both its homogeneous and inhomogeneous form. We'll see that this discussion applies not just to the potentials, but the fields as well in certain cases. Indeed, the wave equation shows up all over physics, so understanding its solutions is very important.

### Initial-Boundary Value Problems

Suppose $\phi(\mathbf{x},t)$ is some arbitrary scalar field that happens to satisfy the inhomogeneous wave equation
$$
\nabla^2 \phi - \frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = -4\pi \rho(\mathbf{x},t) \ ,
$$
where $\rho(\mathbf{x},t)$ is some arbitrary source function. Here we assume $\phi(\mathbf{x},t)$ is some arbitrary scalar field, or perhaps one of the components of some other vector (or tensor) field. Such solutions are sometimes called *wave functions* for reasons we'll soon see.

The wave equation is a second order linear partial differential equation. Since it involves time and not just space, the exact solution for a given source will depend both on the initial conditions of $\phi(\mathbf{x},t)$ and its time derivative, as well as any imposed boundary conditions. A PDE subject to both initial conditions and boundary conditions is often called an *initial-boundary value problem*, or *IBVP* for short.

Note that people often denote the *wave operator* above using the short-hand symbol $\square$, with
$$
\square \equiv \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \ .
$$
This operator is called the *d'Alembertian*. In this notation the inhomogeneous wave equation is then written simply as
$$
\square \phi = -4\pi\rho(\mathbf{x}, t) \ .
$$
As with any linear PDE, the general solution $\phi(\mathbf{x},t)$ to the wave equation can be express as the sum of two solutions, a *particular solution* $\phi_p(\mathbf{x},t)$ that satisfies the inhomogeneous wave equation $\square \phi_p = -4\pi\rho$ free of any conditions, and a *homogeneous solution* $\phi_h(\mathbf{x},t)$ that satisfies the *homogeneous wave equation* $\square \phi_h = 0$ subject to the initial and boundary conditions of the problem. Provided we can find such solutions, we can always then write
$$
\phi(\mathbf{x},t) = \phi_h(\mathbf{x},t) + \phi_p(\mathbf{x},t) \ .
$$
That we can always express the general solution as the sum of a particular and homogeneous solution follows as usual from the principle of superposition, which applies to *any* linear PDE.

Let's now investigate the nature of these solutions one by one, starting with the homogeneous solution.

### Homogeneous Wave Equation

Suppose that $\phi(\mathbf{x},t)$ is some scalar field satisfying the *homogeneous wave equation*
$$
\nabla^2 \phi - \frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = 0 \ ,
$$
subject to some set of given initial conditions
$$
\phi_0 = \phi(\mathbf{x}, 0) \quad , \quad \psi_0 = \frac{\partial}{\partial t} \bigg|_{t=0} \phi(\mathbf{x}, t) \ .
$$
We could impose boundary conditions as well, but for now we'll ignore them.

First, we'll show that any function of the form $\phi_{\mathbf{k},\omega}(\mathbf{x},t) = e^{i (\mathbf{k} \cdot \mathbf{x} - \omega t)}$ satisfies the inhomogeneous wave equation under certain conditions. Differentiating twice with respect to space and time, it's easy to see that
$$
\nabla^2 \phi_{\mathbf{k},\omega} = -\mathbf{k}^2 \phi_{\mathbf{k},\omega} \quad , \quad \frac{\partial^2 \phi_{\mathbf{k},\omega}}{\partial t^2} = -\omega^2 \phi_{\mathbf{k},\omega} \ .
$$
Plugging this into the wave equation, we then have
$$
\bigg(-\mathbf{k}^2 + \frac{\omega^2}{c^2}\bigg) \phi_{\mathbf{k},\omega}(\mathbf{x},t) = 0 \ .
$$
Now, notice that $\phi_{\mathbf{k},\omega}(\mathbf{x},t)$ can never be zero since we always have $|\phi_{\mathbf{k},\omega}| = 1$. The only way $\phi_{\mathbf{k},\omega}(\mathbf{x},t)$ can satisfy the wave equation then is if we require that the parameters $\mathbf{k}$ and $\omega$ satisfy $\mathbf{k}^2 - \omega^2/c^2 = 0$, or equivalently that
$$
\omega(\mathbf{k}) = \pm \frac{|\mathbf{k}|}{c} \ .
$$
 Such a relation between $\omega$ and $\mathbf{k}$ is called a *dispersion* relation for reasons we'll understand more in later chapters.

At any rate, we've now found *two* linearly independent solutions to the homogeneous wave equation. This means any linear superposition of these two solutions will also be a solution,
$$
\phi_{\omega(\mathbf{k})}(\mathbf{x},t) \equiv Ae^{i [\mathbf{k} \cdot \mathbf{x} - \omega(\mathbf{k}) t]} + Be^{i [\mathbf{k} \cdot \mathbf{x} + \omega(\mathbf{k}) t]} \ .
$$

- Clean this up some more and verify.
- Make sure to plug in the initial conditions at some point.
- Don't forget to mention what $k$ and $\omega$ are somewhere, and the relation to wavelength and period
- Consider some simple examples, and maybe mention the relation to separation of variables
- Mention spherical harmonics version of the solution as well


$$
\phi(\mathbf{x},t) = \int \frac{d^3\mathbf{k}d\omega}{(2\pi)^4} \big[A(\mathbf{k},\omega) e^{i [\mathbf{k} \cdot \mathbf{x} - \omega(\mathbf{k})t]} + B(\mathbf{k},\omega) e^{i [\mathbf{k} \cdot \mathbf{x} + \omega(\mathbf{k})t]}\big] \ .
$$




### Green's Function

Just as we did with Poisson's equation, we can find the particular solution to the inhomogeneous equation by first finding the Green's function $G(\mathbf{x}, t)$ that solves the wave equation
$$
\nabla^2 G - \frac{1}{c^2} \frac{\partial^2 G}{\partial t^2} = -4\pi\delta(\mathbf{x})\delta(t) \ .
$$
As usual, we'll require that the Green's function go to zero at infinity, both in space and in time. Notice that now the Green's function depends on both position and time since the wave equation itself involves both position and time. 

Assuming we can find such a Green's function we can obtain a particular solution $\phi(\mathbf{x},t)$ via space-time convolution of the Green's function with the source function $\rho(\mathbf{x},t)$,
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' dt' \ \rho(\mathbf{x}', t') G(\mathbf{x}-\mathbf{x}', t-t') \ .
$$
To find the Green's function we'll use the same approach we did to find the Green's function for Poisson's equation in electrostatics. That is, we'll take the Fourier transform of both sides of the wave equation, solve for the Green's function in the frequency domain, and then inverse transform to get the original Green's function in the space-time domain.

When working in space and time, we can get a suitable Fourier transform by multiplying both sides by $e^{-i (\mathbf{k} \cdot \mathbf{x} - \omega t)}$ and integrating with respect to both $\mathbf{x}$ and $t$. We'll denote the Fourier transform of $G(\mathbf{x},t)$ by
$$
G(\mathbf{k},\omega) \equiv \int d^3\mathbf{x} dt \ G(\mathbf{x},t) e^{-i (\mathbf{k} \cdot \mathbf{x} - \omega t)} \ .
$$
Here the spatial Fourier conjugate $\mathbf{k}$ represents the usual wave vector, and the time Fourier conjugate $\omega$ represents the usual angular frequency. Now, if we apply this Fourier transform to both sides of the wave equation above, we get
$$
\bigg(-|\mathbf{k}|^2 + \frac{\omega^2}{c^2}\bigg) G(\mathbf{k}, \omega) = -4\pi \ .
$$
The left-hand side follows by using integration by parts inside the Fourier transform, and the right-hand side follows from the fact that the integral of a delta function over all space and time is one. Thus, the Green's function in frequency space is given by
$$
G(\mathbf{k}, \omega) = -\frac{4\pi}{(\omega/c)^2 - |\mathbf{k}|^2} = -\frac{4\pi c^2}{\omega^2 - c^2|\mathbf{k}|^2} \ .
$$
To recover the original Green's function we need to take the inverse Fourier transform of this function, which can be found by multiplying both sides by $1/(2\pi)^4 \ e^{i(\mathbf{k} \cdot \mathbf{x} - \omega t)}$ and integrating over all $\mathbf{k}$ and $\omega$,
$$
G(\mathbf{x},t) = \int \frac{d^3\mathbf{k}d\omega}{(2\pi)^4} \ G(\mathbf{k},\omega) e^{i(\mathbf{k} \cdot \mathbf{x} - \omega t)} \ .
$$
All that remains now is to perform this integration. We'll find it convenient to integrate over $\omega$ first. We'll thus write
$$
G(\mathbf{x},t) = -\frac{4\pi c^2}{(2\pi)^4} \int d^3\mathbf{k} \  e^{i\mathbf{k} \cdot \mathbf{x}} \int_{-\infty}^\infty d\omega \ \frac{e^{-i\omega t}}{\omega^2 - c^2|\mathbf{k}|^2} \ .
$$
Performing the integration over $\omega$ requires the tools of complex analysis. We showed in the appendix that
$$
\int_{-\infty}^\infty dx \ \frac{e^{-itx}}{x^2 - a^2} = -\frac{\pi}{a} \sin a|t| \ .
$$
Identifying $x \leftrightarrow \omega$ and $a \leftrightarrow c|\mathbf{k}|$, we can see that the solution to the $\omega$ integral is thus
$$
\int_{-\infty}^\infty d\omega \ \frac{e^{-i\omega t}}{\omega^2 - c^2|\mathbf{k}|^2} = -\frac{\pi}{c|\mathbf{k}|} \sin c|\mathbf{k}t| \ .
$$

Plugging this result back into the Green's function and simplifying, we have
$$
G(\mathbf{x},t) = \frac{c}{(2\pi)^2} \int d^3\mathbf{k} \  e^{i\mathbf{k} \cdot \mathbf{x}} \frac{\sin c|\mathbf{k}t|}{|\mathbf{k}|} \ .
$$
All that remains now is the integral over $\mathbf{k}$. We'll orient the axes in $\mathbf{k}$ space so that $\mathbf{x} = r\mathbf{e}_z$ and $\mathbf{k} \cdot \mathbf{x} = kr\cos\theta_k$, where $k = |\mathbf{k}|$. Writing the volume element in spherical coordinates $(k, \theta_k, \varphi_k)$ as $d^3 \mathbf{k} = k^2 \sin\theta_k dk d\theta_k d\varphi_k$ and integrating over $\varphi_k$, we get
$$
G(\mathbf{x},t) = \frac{c}{2\pi} \int_0^\infty k^2 dk \ \frac{\sin ck|t|}{k} \int_0^\pi d\theta_k \  \sin\theta_k e^{ikr\cos\theta_k} \ .
$$
We saw this same $\theta_k$ integral before when calculating the Green's function in electrostatics, where we showed
$$
\int_0^\pi d\theta_k \  \sin\theta_k e^{ikr\cos\theta_k} = \int_{-1}^1 d\mu \ e^{ikr\mu} = 2\frac{\sin kr}{kr} \ .
$$
All that remains now is the integral over $k$. Canceling the factors of $k^2$, we have
$$
G(\mathbf{x},t) = \frac{c}{\pi r} \int_0^\infty dk \ \sin ck|t| \sin kr \ .
$$
This integral can be done by writing the expanding the sines in terms of complex exponentials and exploiting the properties of the delta function. We have
$$
\begin{align*}
G(\mathbf{x},t) &= \frac{c}{\pi r} \int_0^\infty dk \ \frac{1}{2i} \big(e^{ick|t|} - e^{-ick|t|}\big) \frac{1}{2i} \big(e^{ikr} - e^{-ikr}\big) \\
&= -\frac{c}{4\pi r} \int_0^\infty dk \ \big[e^{ik(c|t| + r)} - e^{ik(c|t| - r)} - e^{ik(-c|t| + r)} + e^{ik(-c|t| - r)}\big] \\
&= -\frac{c}{4\pi r} \big[2\pi\delta(c|t| + r) - 2\pi\delta(c|t| - r) - 2\pi\delta(-c|t| + r) + 2\pi\delta(-c|t| - r)\big] \\
&= \frac{c}{2r} \big[\delta(c|t| - r) - \delta(c|t| + r)\big] \\
&= \frac{1}{2r} \big[\delta(|t| - r/c) - \delta(|t| + r/c)\big] \ .
\end{align*}
$$
Now, since $|t| > 0$ and $r > 0$, the second delta function must vanish since we can never have $|t| = -r/c$, which means
$$
G(\mathbf{x},t) = \frac{1}{2r} \delta(|t| - r/c) = \frac{1}{2r} [\delta(t - r/c) + \delta(t + r/c)] \ .
$$
This is called the *symmetrized Green's function* for the wave equation for reasons we'll understand in a moment.
### Retarded and Advanced Solutions

Let's now examine each term in this Green's function more closely. It'll be useful to define
$$
t_\text{ret} \equiv t - \frac{r}{c} \quad , \quad t_\text{adv} \equiv t + \frac{r}{c} \ .
$$
We call $t_\text{ret}$ the *retarded time* and $t_\text{adv}$ the *advanced time* for reasons that'll become clear in a moment. Similarly, we define
$$
\begin{align*}
&G_\text{ret}(\mathbf{x}, t) \equiv \frac{\delta(t - r/c)}{r} = \frac{\delta(t_\text{ret})}{r} \ , \\
&G_\text{adv}(\mathbf{x}, t) \equiv \frac{\delta(t + r/c)}{r} = \frac{\delta(t_\text{adv})}{r} \ ,
\end{align*}
$$
where $G_\text{ret}(\mathbf{x}, t)$ is called the *retarded Green's function* and $G_\text{adv}(\mathbf{x}, t)$ the *advanced Green's function*. In this notation, we can rewrite the symmetrized Green's function found above as the average of the retarded and advanced Green's functions,
$$
G(\mathbf{x}, t) = \frac{1}{2}[G_\text{ret}(\mathbf{x}, t) + G_\text{adv}(\mathbf{x}, t)] \ .
$$
Now, one can easily verify that both $G_\text{ret}(\mathbf{x}, t)$ and $G_\text{adv}(\mathbf{x}, t)$ each individually satisfy the wave equation
$$
\nabla^2 G - \frac{1}{c^2} \frac{\partial^2 G}{\partial t^2} = -4\pi\delta(\mathbf{x})\delta(t) \ .
$$
That is, the retarded and advanced Green's functions are each individually valid Green's function. By the principle of superposition then, any weighted average of the two will also be a valid Green's function to the wave equation. This means that in some sense the retarded and Green's functions are more fundamental than the symmetrized Green's function found before.

Physically, the *retarded* Green's function $G_\text{ret}(\mathbf{x}, t) = \delta(t_\text{ret})/r$ represents a *spherical wave* propagating *outwards* from the origin at time $t = 0$ with increasing time at speed $c$. That is, the retarded Green's function represents a spherical wave moving *forward* from a unit source with time.

Similarly, the *advanced* Green's function $G_\text{adv}(\mathbf{x}, t) = \delta(t_\text{adv}) / r$ represents a spherical wave propagating *inward* from infinity toward the origin at speed $c$, reaching the origin at time $t=0$. Equivalently, the advanced Green's function represents a spherical  wave moving *backwards* from a unit source with time.

Now, in electromagnetism we like to think of the source as being the *cause* of a field and not the field as being the cause of the source. The retarded Green's function satisfies this idea. It's *causal*. However, the advanced Green's function does not. It's *acausal*. For this reason, we must reject the advanced Green's function on physical grounds, and insist that only the retarded Green's function be chosen. This causality requirement is another ingredient to the theory that we need to add in. As we said before, it doesn't follow from Maxwell's equations or the Lorentz force law, which are both time-reversal invariant.

Thus, from here on we will *always* assume that the only valid Green's function for the wave equation is the *retarded* Green's function. That is, we'll set $G(\mathbf{x}, t) \equiv G_\text{ret}(\mathbf{x}, t)$. Shifting the Green's function back to $\mathbf{x}'$ and $t'$, we thus finally have
$$
\boxed{
G(\mathbf{x} - \mathbf{x}', t - t') \equiv \frac{\delta(t' - t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|}
} \ ,
$$
where the retarded time $t_\text{ret}$ is now given by
$$
\boxed{
t_\text{ret} \equiv t - \frac{|\mathbf{x} - \mathbf{x}'|}{c}
} \ .
$$
We can now convolve this Green's function with the source function $\rho(\mathbf{x},t)$ to get the particular solution for the wave equation,
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' dt' \ \rho(\mathbf{x}', t') G(\mathbf{x}-\mathbf{x}', t-t') = \int d^3\mathbf{x}' dt' \ \rho(\mathbf{x}', t') \frac{\delta(\mathbf{x} - \mathbf{x}', t' - t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Notice that we can go ahead and integrate with respect to $t'$ to get a volume integral. If we do that we get
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \ \frac{\rho(\mathbf{x}', t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Evidently, the particular solution to the wave equation looks exactly like the one for Poisson's equation, except that now there's a time delay $|\mathbf{x} - \mathbf{x}'|/c$ for the source at $\mathbf{x}'$ to affect the field at $\mathbf{x}$. That is, at time $t'$, the field propagates outward from $\mathbf{x}'$ as a wave traveling at a finite speed $c$. This means any source disturbance will take time to affect the field. Nothing is instantaneous.

With this particular solution in hand we can now immediately write down the particular solutions for the scalar and vector potentials in terms of the source charges. For the scalar potential $\phi(\mathbf{x},t)$, we have exactly what we wrote down above,
$$
\boxed{
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \frac{\rho(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|}
} \ .
$$
For the vector potential $\mathbf{A}(\mathbf{x},t)$, each component $A_i$ will satisfy its own inhomogeneous wave equation
$$
\square A_i = -\frac{4\pi}{c} J_i \ .
$$
Identifying the source as $J_i/c$, we can thus express the particular solution for the vector potential in vector form as
$$
\boxed{
\mathbf{A}(\mathbf{x},t) = \frac{1}{c} \int d^3\mathbf{x}' \frac{\mathbf{J}(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} 
} \ .
$$
Again, each potential depends on the value of the source at a time in the past, meaning as the source changes a wave disturbance will propagate outward from the source and eventually affect the field. The speed of propagation of this disturbance is evidently just the speed of light $c$. As we'll see later, this disturbance is in fact what we call *electromagnetic radiation*, or *light*.

## Conservation Laws

- Conservation of energy (Poynting's theorem), conservation of momentum

$$
\nabla \times (\nabla \times \mathbf{B}) = \frac{4\pi}{c} \nabla \times \mathbf{J} + \frac{1}{c} \frac{\partial}{\partial t} \nabla \times \mathbf{E}
$$

$$
\nabla(\nabla \cdot \mathbf{B}) - \nabla^2 \mathbf{B} = \frac{4\pi}{c} \nabla \times \mathbf{J} - \frac{1}{c^2} \frac{\partial^2 \mathbf{B}}{\partial t^2}
$$

$$
\frac{1}{c^2} \frac{\partial^2 \mathbf{B}}{\partial t^2} - \nabla^2 \mathbf{B} = \frac{4\pi}{c} \nabla \times \mathbf{J}
$$

- E-field

$$
\nabla \times (\nabla \times \mathbf{E}) = -\frac{1}{c} \frac{\partial}{\partial t} \nabla \times \mathbf{B}
$$

$$
\nabla(\nabla \cdot \mathbf{E}) - \nabla^2 \mathbf{E} = -\frac{1}{c} \frac{\partial}{\partial t} \bigg[\frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t}\bigg]
$$

$$
\frac{1}{c^2} \frac{\partial^2 \mathbf{E}}{\partial t^2} - \nabla^2 \mathbf{E} = -\frac{4\pi}{c^2} \frac{\partial \mathbf{J}}{\partial t} - 4\pi \nabla \rho
$$

