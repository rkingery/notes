# Electrodynamics

In the previous chapter we started moving away from statics, and started our study of electrodynamics, the study of time-varying electric and magnetic fields. We finished that chapter with the following quasistatic field equations
$$
\begin{align*}
&\nabla \cdot \mathbf{E} = 4\pi \rho \ ,\\
&\nabla \times \mathbf{E} = -\frac{1}{c} \frac{\partial\mathbf{B}}{\partial t} \ ,\\
&\nabla \cdot \mathbf{B} = 0 \ ,\\
&\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} \ . \\
\end{align*}
$$
In this chapter we will continue on with the topic of electrodynamics by allowing the final equation for $\nabla \times \mathbf{B}$ to depend on a time-varying E-field, just as in the previous chapter we modifying $\nabla \times \mathbf{E}$ to depend on a time-varying B-field. This will lead us to the notion of displacement current, and finally to Maxwell's equations, the crown jewel of electromagnetism.

Once we finally derive Maxwell's equations we will study their implications pretty much for the rest of this course. We will start by studying the conservation laws they imply for charge, energy, and momentum. We will then to show how Maxwell's equations become wave equations, which leads to the topic of electromagnetic radiation, something that we'll see in a future chapter is the underlying theory of classical optics.

From now on in this course, we'll assume that all fields and distributions depend explicitly on time unless otherwise stated. That is, we'll assume that $\mathbf{E} = \mathbf{E}(\mathbf{x},t)$ and $\mathbf{B} = \mathbf{B}(\mathbf{x},t)$, and that $\rho = \rho(\mathbf{x},t)$ and $\mathbf{J} = \mathbf{J}(\mathbf{x},t)$. This pulls us away from statics and puts us squarely in the realm of electrodynamics. An immediate implication of this is that many of the expressions we derived before for the fields in terms of source distributions will no longer apply, in particular Coulomb's law and the Biot-Savart law. We'll see in later chapters how these formulas can be modified to account for electrodynamics.

## Maxwell's Equations

Before studying the implications of Maxwell's equations we first need to finalize them by introducing the *displacement current*, which modifies the equation for $\nabla \times \mathbf{B}$ to allow for a time-varying E-field. 

### Displacement Current

In the 1860s, Maxwell observed that the quasistatic field equations were mathematically inconsistent with the most fundamental principle of electromagnetism, namely conservation of charge. Specifically, Maxwell noticed that Ampere's law was inconsistent with the continuity equation
$$
\frac{\partial \rho}{\partial t} + \nabla \cdot \mathbf{J} = 0 \ .
$$
To see why this is the case, recall that the differential form of Ampere's states that
$$
\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} \ .
$$
If we take the divergence of both sides of this equation, we get
$$
\nabla \cdot (\nabla \times \mathbf{B}) = \frac{4\pi}{c} \nabla \cdot \mathbf{J} \ .
$$
Now, the divergence of a curl must always vanish, which means the left-hand side must be zero. We thus must evidently conclude from Ampere's law that
$$
\nabla \cdot \mathbf{J} = 0 \ .
$$
But, as we saw in the chapter on magnetostatics, this can only be consistent with conservation of charge when the current density is time-independent. Of course, there's no reason at all to assume the current density must always be time-independent. In fact if it is we necessarily get a steady current, which means the B-field must be time-independent, which necessarily conflicts with Faraday's law.

To resolve this problem, Maxwell proposed adding a new term to Ampere's law, which he called the *displacement current*,
$$
\nabla \times \mathbf{B} = \frac{4\pi}{c} (\mathbf{J} + \mathbf{J}_d) \ .
$$
If we now take the divergence of both sides as we did above, we get
$$
0 = \nabla \cdot \mathbf{J} + \nabla \cdot \mathbf{J}_d \ .
$$
We can now make this equation consistent with the continuity equation by insisting that
$$
\nabla \cdot \mathbf{J}_d = \frac{\partial \rho}{\partial t} \ .
$$
But since Gauss's law requires that $\nabla \cdot \mathbf{E} = 4\pi\rho$, we also have
$$
\nabla \cdot \mathbf{J}_d = \frac{1}{4\pi} \frac{\partial}{\partial t} \nabla \cdot \mathbf{E} \ .
$$
This means, up to an additive constant that we can set to zero, the displacement current must then be
$$
\mathbf{J}_d = \frac{1}{4\pi} \frac{\partial \mathbf{E}}{\partial t} \ .
$$
Plugging this into the modified Ampere's law above, we finally have
$$
\boxed{
\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t}
} \ .
$$
With this modification, Maxwell managed to make the field equations consistent with the conservation of charge. Just as Faraday's law allows $\nabla \times \mathbf{E}$ to depend on a time-varying E-field, this modified Ampere's law allows $\nabla \times \mathbf{B}$ to depend on a time-varying E-field, thus coupling together the four field equations and making them self-consistent.

So why wasn't the displacement current noticed in the experiments that established Ampere's law? Why did it take Maxwell's use of theory to establish its existence? Fundamentally, the reason has to do with the fact that the displacement current in typical lab settings is much smaller than the ordinary current. To see why this is the case, consider the following dimensional analysis.

Suppose we observe the EM fields at some characteristic distance scale $L$ and time scale $T$. That is, when we do experiments, the experiments typically happen on distance scales of $L$ and time scales of $T$. In terms of these dimensions, the generalized Ampere's law morally says that
$$
\frac{B}{L} \sim \frac{J}{c} + \frac{E}{cT} \ .
$$
Similarly, Faraday's law says that
$$
\frac{E}{L} \sim \frac{B}{cT} \ .
$$
Now, in typical experimental settings we have $B/L \sim J/c$. That is, the distance scale of measurement $L$ is such that measurements of the current $J$ are significant. Putting all of this information together we then have
$$
\frac{B}{L} \sim \bigg[1 + \bigg(\frac{L}{cT}\bigg)^2\bigg] \frac{B}{L} \ .
$$
Thus, for the displacement current to be experimentally significant in a lab setting we'd need $L/cT \sim 1$, or
$$
L \sim cT \ .
$$
Now, in a typical lab setting the distance scales of measurement are often fairly small, usually on the order of meters or less, and the time scales of measurement are usually on the order of seconds. This means $L/T$ is usually on the order of $1 \ \text{m}/\text{s}$ or so. But the speed of light $c$ is on the order of $10^{8} \ \text{m}/\text{s}$, meaning $L/T \ll c$. Thus, for all practical purposes, in typical lab experiments we can practically neglect the displacement current and just use the quasistatic equations instead.

There are two situations where we can't neglect the displacement current. One is when doing measurements on large distance scales, say on the order of miles. Eventually, $L$ will be large enough so that $L \sim cT$, which means far enough away from the source we should be able to detect a displacement current effect, provided our devices are sensitive enough. This, as we'll see, is electromagnetic radiation, or light. The other scenario is when $L/T \sim c$, which can occur even at short distance scales if we can force the fields to oscillate quickly enough. This is the relativistic limit, when the effects of special relativity become important.

### Maxwell's Equations

With the final modification to Ampere's law in place, we're finally able to state *Maxwell's Equations*, the fundamental field equations of classical electrodynamics. In a nutshell, these equations say that E-fields are caused by exactly two things: charges and changing B-fields. Similarly, B-fields are caused by exactly two things: currents and changing E-fields. 

In differential form, Maxwell's equations are

$$
\boxed{
\begin{align*}
&\nabla \cdot \mathbf{E} = 4\pi \rho \\
&\nabla \times \mathbf{E} = -\frac{1}{c} \frac{\partial \mathbf{B}}{\partial t} \\
&\nabla \cdot \mathbf{B} = 0 \\
&\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \\
\end{align*}
} \ .
$$

We can also state Maxwell's equations in integral form if we like by using the divergence and Stokes theorems in reverse,
$$
\begin{align*}
&\oint \mathbf{E} \cdot d\mathbf{a} = 4\pi Q_{\text{enc}} \ , \\
&\oint \mathbf{E} \cdot d\boldsymbol{\ell} = -\frac{1}{c} \frac{d\Phi_B}{dt} \ , \\
&\oint \mathbf{B} \cdot d\mathbf{a} = 0 \ , \\
&\oint \mathbf{B} \cdot d\boldsymbol{\ell} = \frac{4\pi}{c} I_{\text{enc}} + \frac{1}{c} \frac{d\Phi_E}{dt} \ . \\
\end{align*}
$$
Here $Q_{\text{enc}}$ refers as usual to the charges enclosed in a closed surface, and $I_{\text{enc}}$ to the currents enclosed inside a closed loop, while $\Phi_E$ and $\Phi_B$ refers to the flux of electric and magnetic fields through any surface bounded by the closed loop.

Maxwell's equations provide us with eight linear partial differential equations for the EM fields, two equations from the divergences and six equations from the curls. The entire dynamics of the EM fields is contained in these equations.

In fact, Maxwell's equations almost complete the theory of classical electromagnetism. There are only a few other things we need to add to complete the theory. First, we need to say how the fields modify the dynamics of moving particles. This coupling between particles and fields is of course is done via the Lorentz force law,
$$
\mathbf{F} = q\mathbf{E} + q\frac{\mathbf{v}}{c} \times \mathbf{B} \ .
$$
Second, we need to say something about how the fields get modified in the presence of electromagnetic materials. This we will spend a whole chapter on. Last, we need to say something about how exactly the source charges give rise to the fields. This is known as *causality*. It's one of the most fundamental principles in physics. Causality states that past actions can impact future actions, but future actions can never impact past actions.

### Time Reversal

To see why causality is an issue we will now show that Maxwell's equations have no sense of the arrow of time, of what causes what. That is, they're *time-reversal invariant*, meaning they remain unchanged under a *time reversal* transformation $t \to -t$. To show this is the case, we first need to figure out how the fields and distributions behave under time reversal.

First let's ask how the position $\mathbf{x}$ changes under time reversal. Since the position is only a function of the underlying coordinate system, under a time reversal the position $\mathbf{x}(t)$ of any particle must remain unchanged. That is, $\mathbf{x}(t) \to \mathbf{x}(-t)$. In particular, this means that the *velocity* $\mathbf{v}(t)$ of a particle must transform under time reversal as
$$
\mathbf{v}(t) = \frac{d\mathbf{x}(t)}{dt} \to \frac{d\mathbf{x}(-t)}{d(-t)} = -\frac{d\mathbf{x}(t)}{dt} = -\mathbf{v}(-t) \ .
$$
Similarly, the *acceleration* $\mathbf{a}(t)$ of a particle must transform under time reversal as
$$
\mathbf{a}(t) = \frac{d^2\mathbf{x}(t)}{dt^2} \to \frac{d^2\mathbf{x}(-t)}{d(-t)^2} = \frac{d^2\mathbf{x}(t)}{dt^2} = \mathbf{a}(-t) \ .
$$
Thus, the position and acceleration of a particle remain *invariant* under time-reversal, while the velocity of a particle changes sign.

What about scalar quantities like the mass $m$ or charge $q$ of a moving particle? Since both mass and charge are conserved (at least in classical physics), they can't suddenly change with time, meaning these must also be invariant under time reversal. By implication, this means the charge density $\rho(\mathbf{x},t)$ must be invariant as well since it's proportional to charge. In fact, *all* scalar quantities must be time-reversal invariant.

The current density $\mathbf{J}(\mathbf{x},t)$, however, is a slightly different. Since $\mathbf{J} = \rho\mathbf{v}$ and $\rho$ is time-reversal invariant, the current density must transform like the velocity $\mathbf{v}$ under time reversal,
$$
 \mathbf{J}(\mathbf{x},t) \to -\mathbf{J}(\mathbf{x},-t) \ .
$$
Now that we know how the distributions behave under time reversal, we can now ask how the fields behave. By Gauss's law we must have $\nabla \cdot \mathbf{E} = 4\pi\rho$. Since both position and charge density are invariant, the E-field must evidently be invariant as well,
$$
\mathbf{E}(\mathbf{x},t) \to \mathbf{E}(\mathbf{x},-t) \ .
$$
We might suspect the B-field to be invariant as well, but surprisingly that's not the case. By the generalized Ampere's law we have
$$
\nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \ .
$$
Since $\mathbf{J}$ reverses direction under time reversal and $\mathbf{B}$ is proportional to $\mathbf{J}$, it too must reverse sign,
$$
\mathbf{B}(\mathbf{x},t) \to -\mathbf{B}(\mathbf{x},-t) \ .
$$
We've thus now established how the distributions and fields behave under time reversal, which means we can now ask how Maxwell's equations themselves behave. For Gauss's law, we have
$$
\nabla \cdot \mathbf{E}(\mathbf{x},-t) = 4\pi \rho(\mathbf{x},-t) \ .
$$
This means Gauss's law is invariant under time reversal, meaning it has the same functional from.

Next we have Faraday's law, which under a time reversal transformation gives
$$
\nabla \times \mathbf{E}(\mathbf{x},-t) = -\frac{1}{c} \frac{\partial}{\partial(-t)} \big[-\mathbf{B}(\mathbf{x},-t)\big] \ .
$$
Since the minus signs cancel, we see that Faraday's law is invariant under time reversal as well.

Next we have Gauss's law for magnetism, which under time reversal gives
$$
-\nabla \cdot \mathbf{B}(\mathbf{x},-t) = 0 \ .
$$
Since we can multiply both sides by $-1$, we see that this equation is also invariant.

Finally we have the generalized Ampere's law, which under time reversal says
$$
-\nabla \times \mathbf{B}(\mathbf{x},-t) = \frac{4\pi}{c} \big[-\mathbf{J}(\mathbf{x},-t)\big] + \frac{1}{c} \frac{\partial}{\partial(-t)} \mathbf{E}(\mathbf{x},-t) \ .
$$
Again, multiplying both sides by $-1$ we see that this equation is invariant as well. We've thus managed to show that all six of Maxwell's equations are time-reversal invariant, and by implication that the continuity equation is also invariant.

For what it's worth, the Lorentz force law is also time-reversal invariant, since by Newton's second law we have
$$
m\mathbf{a}(\mathbf{x},-t) = q\mathbf{E}(\mathbf{x},-t) + q\frac{\mathbf{v}(\mathbf{x},-t)}{c} \times \mathbf{B}(\mathbf{x},-t) \ .
$$
Here, everything is invariant except $\mathbf{v}$ and $\mathbf{B}$, but the minus signs in $\mathbf{v}$ and $\mathbf{B}$ cancel out, leaving the Lorentz force law invariant. 

Taken together, these mean that neither Maxwell's equations nor the Lorentz force law contain any understanding of the arrow of time, of what causes what. That is, they have no notion of causality. However, we intuitively expect that it's the source charges that should give rise to the fields and not the other way around. If we want to bake this requirement into the theory we'll need to do it in some other way, which we'll discuss in a moment.

## Electrodynamic Potentials

Just as in statics it was useful to express the EM fields in terms of potentials, it'll be useful to do the same in electrodynamics. Recall that we derived the scalar potential $\phi(\mathbf{x})$ from the assumption that $\nabla \times \mathbf{E} = \mathbf{0}$, and the vector potential $\mathbf{A}(\mathbf{x})$ from the assumption that $\nabla \cdot \mathbf{B} = 0$. Provided these equations hold, we can write
$$
\mathbf{E} = -\nabla \phi \quad , \quad \mathbf{B} = \nabla \times \mathbf{A} \ .
$$
As we'll see, we need to modify these relations somewhat in electrodynamics.

### Electrodynamic Potentials

In electrodynamics it's still true that $\nabla \cdot \mathbf{B} = 0$ even for time-varying B-fields, which means we can still find a time-varying vector field $\mathbf{A}(\mathbf{x},t)$ such that $\mathbf{B} = \nabla \times \mathbf{A}$. We will still refer to $\mathbf{A}(\mathbf{x},t)$ as the *vector potential*.

However, it's no longer true that $\nabla \times \mathbf{E} = \mathbf{0}$ for time-varying E-fields, which means we no longer find a time-varying scalar field $\phi(\mathbf{x},t)$ such that $\mathbf{E} = -\nabla \phi$. However, we can still find a related vector field whose curl vanishes. Consider Faraday's law,
$$
\nabla \times \mathbf{E} = -\frac{1}{c} \frac{\partial \mathbf{B}}{\partial t} \ .
$$
If we write $\mathbf{B} = \nabla \times \mathbf{A}$ and move everything to the left-hand side, we have
$$
\nabla \times \bigg(\mathbf{E} + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t}\bigg) = 0 \ .
$$
Since we now have a vector field whose curl vanishes, we can find a scalar field $\phi(\mathbf{x},t)$ such that
$$
\mathbf{E} + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t} = -\nabla \phi \ .
$$
This means we can express $\mathbf{E}(\mathbf{x},t)$ in terms of the time-varying scalar and vector potentials as
$$
\mathbf{E} = -\nabla \phi - \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t} \ .
$$
As before, we call $\phi(\mathbf{x},t)$ the *scalar potential*. Thus, in electrodynamics we relate the EM fields to the potentials by
$$
\boxed{
\begin{align*}
\mathbf{E} &= -\nabla \phi - \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t} \\ 
\mathbf{B} &= \nabla \times \mathbf{A}
\end{align*}
} \ .
$$
The dynamical behavior of these potentials must then be satisfied by the remaining two Maxwell equations.

### Field Equations for Potentials

We still have two more of Maxwell's equations that the potentials need to satisfy, namely the inhomogeneous equations
$$
\nabla \cdot \mathbf{E} = 4\pi \rho \quad , \quad \nabla \times \mathbf{B} = \frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \ .
$$
If we plug the expression for $\mathbf{E}$ in terms of the potentials into the first equation, we must have
$$
\nabla \cdot \bigg(\nabla \phi + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t}\bigg) = -4\pi \rho \ .
$$
Simplifying the left-hand side, this gives the following differential equation for the scalar potential,
$$
\nabla^2 \phi + \frac{1}{c} \frac{\partial}{\partial t} \nabla \cdot \mathbf{A} = -4\pi \rho \ .
$$
Similarly, if we plug the expressions for both $\mathbf{E}$ and $\mathbf{B}$ into the second inhomogeneous equation, we have
$$
\nabla \times (\nabla \times \mathbf{A}) = \frac{4\pi}{c} \mathbf{J} - \frac{1}{c} \frac{\partial}{\partial t} \bigg(\nabla \phi + \frac{1}{c} \frac{\partial \mathbf{A}}{\partial t}\bigg) \ .
$$
Since $\nabla \times (\nabla \times \mathbf{A}) = \nabla (\nabla \cdot \mathbf{A}) - \nabla^2 \mathbf{A}$, upon rearranging we get
$$
\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} - \nabla \bigg(\nabla \cdot \mathbf{A} + \frac{1}{c} \frac{\partial \phi}{\partial t} \bigg) = -\frac{4\pi}{c} \mathbf{J} \ .
$$
As things stand, the differential equations for $\phi$ and $\mathbf{A}$ look quite different, but notice each equation contains a term proportional to $\nabla \cdot \mathbf{A}$, which means we can potentially make them have the same form by exploiting gauge invariance.

### Gauge Invariance

Recall that we can always add to the vector potential the gradient of any scalar field $\chi$ without changing the underlying B-field,
$$
\mathbf{A}' = \mathbf{A} + \nabla \chi \ .
$$
This is called a *gauge transformation*. This follows from the relation $\mathbf{B} = \nabla \times \mathbf{A}$ along with the fact that the curl of any gradient is always zero. This is still true in electrodynamics, except now when we do a gauge transformation we have to be careful not to alter the E-field as well. The only way we can ensure this is to require that $\phi$ also gauge transform simultaneously as
$$
\phi' = \phi - \frac{1}{c} \frac{\partial \chi}{\partial t} \ .
$$
Thus, in electrodynamics, a gauge transformation is any transformation on both the scalar and vector potentials of the form
$$
\begin{align*}
\phi' &= \phi -\frac{1}{c} \frac{\partial \chi}{\partial t} \ , \\ 
\mathbf{A}' &= \mathbf{A} + \nabla \chi \ .
\end{align*}
$$
As we mentioned in magnetostatics, we can uniquely specify a given gauge by fixing $\chi$, or equivalently by fixing $\nabla \cdot \mathbf{A}$. This ability to specify the divergence of the vector potential gives us a gauge freedom we can use to simplify various formulas without altering any of the underlying physics. We can fix $\nabla \cdot \mathbf{A}$ to be anything we like so long as we're consistent within a given theory.

In magnetostatics we fixed the gauge by choosing the *Coulomb gauge* $\nabla \cdot \mathbf{A} = 0$, which was a natural choice in that setting. We could still do that in electrodynamics as well if we like. If we did, the differential equations for the potentials could be written
$$
\begin{align*}
\nabla^2 \phi &= -4\pi \rho \ , \\
\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} &= -\frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \nabla \frac{\partial \phi}{\partial t} \ .
\end{align*}
$$
This choice of gauge makes the equation for the scalar potential especially simple. We just recover Poisson's equation, the same equation we saw in electrostatics. In the absence of boundary conditions, the solution to Poisson's equation is just given by
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \frac{\rho(\mathbf{x},t)}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
However, the equation for the vector potential $\mathbf{A}$ is much more complex. It's coupled to the scalar potential $\phi$, which means we need to first solve for $\phi$ before we can solve for $\mathbf{A}$. Assuming we've done so, we can treat $4\pi\mathbf{J}_\ell \equiv \nabla \partial_t \phi$ as a kind of current and solve for $\mathbf{A}$ that way.

While this is physically allowed though it's not intuitively pleasing. For one thing, we'd like the equations for $\phi$ and $\mathbf{A}$ to have the same form if possible. But more importantly, Poisson's equation when $\phi$ is time dependent is *instantaneous*. It suggests that the scalar potential can propagate at infinite speeds, which violates special relativity if we insist $\phi$ have any physical meaning at all.  Thus, in electrodynamics we'd prefer to choose a different gauge that fixes both of these issues.

By staring at the original differential equations for the potentials in the previous section one can identify such a gauge,
$$
\boxed{
\nabla \cdot \mathbf{A} = - \frac{1}{c} \frac{\partial \phi}{\partial t} 
} \ .
$$
This choice of gauge is called the *Lorentz gauge*, because it makes both potentials Lorentz invariant. That is, it ensures that both potentials can only propagate at finite speeds. In this gauge, the differential equations for the potentials can be written
$$
\boxed{
\begin{align*}
&\nabla^2 \phi - \frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = -4\pi \rho \\ 
&\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} = -\frac{4\pi}{c} \mathbf{J}
\end{align*}
} \ .
$$
Differential equations of this form are known as *inhomogeneous wave equations*. As we'll soon see, functions satisfying this kind of partial differential equation necessarily must propagate at a finite speed, namely $c$.

To see what kind of condition the Lorentz gauge places on the gauge potential $\chi$, suppose
$$
\nabla \cdot \mathbf{A}' = - \frac{1}{c} \frac{\partial \phi'}{\partial t} \ ,
$$
where $\mathbf{A}' = \mathbf{A} + \nabla \chi$ and $\phi' = \phi - \partial_t \chi / c$ is some gauge transformation. Plugging in and simplifying, we get
$$
\nabla^2 \chi - \frac{1}{c^2} \frac{\partial^2 \chi}{\partial t^2} = -\nabla \cdot \mathbf{A} - \frac{1}{c} \frac{\partial \phi}{\partial t} \ .
$$
Thus, if we have potentials $\phi$ and $\mathbf{A}$ that don't satisfy the Lorentz gauge, we can always convert them into potentials that do by finding a gauge potential $\chi$ that solves this inhomogeneous wave equation.

## Green's Function

Just as we characterized the solutions to Poisson's equation in electrostatics by first finding its Green's function, we'll want to do the same with the inhomogeneous wave equation. First, we'll want to understand the nature of wave equation problems, which are characterized by both boundary conditions as well as initial conditions.

### Inhomogeneous Wave Equation

Suppose $\phi(\mathbf{x},t)$ is some arbitrary scalar field that happens to satisfy the inhomogeneous wave equation
$$
\nabla^2 \phi - \frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = -4\pi \rho(\mathbf{x},t) \ ,
$$
where $\rho(\mathbf{x},t)$ is some arbitrary source function. Here we assume $\phi(\mathbf{x},t)$ is some arbitrary scalar field, or perhaps one of the components of some other vector (or tensor) field. Such solutions are sometimes called *wave functions* for reasons we'll soon see.

The wave equation is a second order linear partial differential equation. Since it involves time and not just space, the exact solution for a given source will depend both on the initial conditions of $\phi(\mathbf{x},t)$ and its time derivative, as well as any imposed boundary conditions. A PDE subject to both initial conditions and boundary conditions is often called an *initial-boundary value problem*, or *IBVP* for short.

We often denote the wave operator above using the short-hand symbol $\square$, called the *d'Alembertian*. That is, we define
$$
\square \equiv \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \ .
$$
In this notation the inhomogeneous wave equation is then written simply as
$$
\square \phi = -4\pi\rho(\mathbf{x}, t) \ .
$$
As with any linear PDE, the general solution $\phi(\mathbf{x},t)$ to the wave equation can be express as the sum of two solutions, a *particular solution* $\phi_p(\mathbf{x},t)$ that satisfies the inhomogeneous wave equation $\square \phi_p = -4\pi\rho$ free of any boundary conditions, and a *homogeneous solution* $\phi_h(\mathbf{x},t)$ that satisfies the *homogeneous wave equation* $\square \phi_h = 0$ subject to the boundary conditions of the problem. Provided we can find such solutions, we can always then write
$$
\phi(\mathbf{x},t) = \phi_h(\mathbf{x},t) + \phi_p(\mathbf{x},t) \ .
$$
That we can always express the general solution as the sum of a particular and homogeneous solution follows as usual from the principle of superposition, which applies to *any* linear PDE.

We'll discuss the homogeneous solutions in much more depth in later chapters when we study electromagnetic waves. For now, we'll focus on the particular solution, or equivalently its Green's function. The Green's function $G(\mathbf{x}, t)$ is defined to be the solution to the inhomogeneous wave equation for a unit point charge passing through the origin at time $t=0$,
$$
\nabla^2 G - \frac{1}{c^2} \frac{\partial^2 G}{\partial t^2} = -4\pi\delta(\mathbf{x})\delta(t) \ .
$$
Assuming we can find this Green's function, we can obtain the particular solution $\phi_p(\mathbf{x},t)$ via space-time convolution of $G(\mathbf{x}, t)$ with the source $\rho(\mathbf{x},t)$,
$$
\phi_p(\mathbf{x},t) = \int d^3\mathbf{x}' dt' \ \rho(\mathbf{x}', t') G(\mathbf{x}-\mathbf{x}', t-t') \ .
$$
Notice that we are now doing a 4-dimensional convolution over *both* space and time.


### Green's Function

Let's now try to find this Green's function. Suppose $G(\mathbf{x}, t)$ is the solution to the inhomogeneous wave equation
$$
\nabla^2 G - \frac{1}{c^2} \frac{\partial^2 G}{\partial t^2} = -4\pi\delta(\mathbf{x})\delta(t) \ .
$$
As with any Green's function, we'll require that $G(\mathbf{x}, t) \to 0$ as its arguments go to infinity, i.e. $r \to \infty$ and $t \to \infty$.

To find $G(\mathbf{x}, t)$ we'll use the same approach we did to find the Green's function for Poisson's equation in electrostatics. That is, we'll take the Fourier transform of both sides of the wave equation, solve for the Green's function in the frequency domain, and then inverse transform to get the original Green's function in the space-time domain.

By definition, the Fourier transform of a space-time dependent function $G(\mathbf{x}, t)$ is given by
$$
G(\mathbf{k},\omega) \equiv \int d^3\mathbf{x} dt \ G(\mathbf{x},t) e^{-i (\mathbf{k} \cdot \mathbf{x} - \omega t)} \ .
$$
Here the spatial Fourier conjugate $\mathbf{k}$ represents the usual wave vector, and the time Fourier conjugate $\omega$ represents the usual angular frequency. Note that as usual $\mathbf{k}$ has dimensions of inverse length, while $\omega$ has dimensions of inverse time.

Now, we want to Fourier transform the wave equation above. This can be done by multiplying both sides by $e^{-i (\mathbf{k} \cdot \mathbf{x} - \omega t)}$ and integrating over all space and time,
$$
\int d^3\mathbf{x} dt \ \bigg[\nabla^2 G - \frac{1}{c^2} \frac{\partial^2 G}{\partial t^2}\bigg] e^{-i (\mathbf{k} \cdot \mathbf{x} - \omega t)} = -4\pi \int d^3\mathbf{x} dt \ \delta(\mathbf{x})\delta(t) e^{-i (\mathbf{k} \cdot \mathbf{x} - \omega t)} \ .
$$
The integral on the right-hand side is just one following the usual properties of the delta function, while the integral on the left-hand side can be simplified by applying integration by parts twice and noting the boundary terms vanish. All together, we get
$$
\bigg(-|\mathbf{k}|^2 + \frac{\omega^2}{c^2}\bigg) G(\mathbf{k}, \omega) = -4\pi \ .
$$
We thus have
$$
G(\mathbf{k}, \omega) = -\frac{4\pi}{(\omega/c)^2 - |\mathbf{k}|^2} = -\frac{4\pi c^2}{\omega^2 - c^2|\mathbf{k}|^2} \ .
$$
To recover $G(\mathbf{x},t)$ we now need to take the *inverse* Fourier transform of this function, which is defined as the integral
$$
G(\mathbf{x},t) = \int \frac{d^3\mathbf{k}d\omega}{(2\pi)^4} \ G(\mathbf{k},\omega) e^{i(\mathbf{k} \cdot \mathbf{x} - \omega t)} \ .
$$
All that remains now is to perform the integration. We'll find it convenient to integrate over $\omega$ first. We'll thus write
$$
G(\mathbf{x},t) = -\frac{4\pi c^2}{(2\pi)^4} \int d^3\mathbf{k} \  e^{i\mathbf{k} \cdot \mathbf{x}} \int_{-\infty}^\infty d\omega \ \frac{e^{-i\omega t}}{\omega^2 - c^2|\mathbf{k}|^2} \ .
$$
Performing the integration over $\omega$ requires the tools of complex analysis. We showed in the appendix that
$$
\int_{-\infty}^\infty dx \ \frac{e^{-itx}}{x^2 - a^2} = -\frac{\pi}{a} \sin a|t| \ .
$$
Identifying $x \leftrightarrow \omega$ and $a \leftrightarrow c|\mathbf{k}|$, we can see that the solution to the $\omega$ integral is thus
$$
\int_{-\infty}^\infty d\omega \ \frac{e^{-i\omega t}}{\omega^2 - c^2|\mathbf{k}|^2} = -\frac{\pi}{c|\mathbf{k}|} \sin c|\mathbf{k}t| \ .
$$

Plugging this result back into the Green's function and simplifying, we have
$$
G(\mathbf{x},t) = \frac{c}{(2\pi)^2} \int d^3\mathbf{k} \  e^{i\mathbf{k} \cdot \mathbf{x}} \frac{\sin c|\mathbf{k}t|}{|\mathbf{k}|} \ .
$$
All that remains now is the integral over $\mathbf{k}$. We'll orient the axes in $\mathbf{k}$ space so that $\mathbf{x} = r\mathbf{e}_z$ and $\mathbf{k} \cdot \mathbf{x} = kr\cos\theta_k$, where $k = |\mathbf{k}|$. Writing the volume element in spherical coordinates $(k, \theta_k, \varphi_k)$ as $d^3 \mathbf{k} = k^2 \sin\theta_k dk d\theta_k d\varphi_k$ and integrating over $\varphi_k$, we get
$$
G(\mathbf{x},t) = \frac{c}{2\pi} \int_0^\infty k^2 dk \ \frac{\sin ck|t|}{k} \int_0^\pi d\theta_k \  \sin\theta_k e^{ikr\cos\theta_k} \ .
$$
We saw this same $\theta_k$ integral before when calculating the Green's function in electrostatics, where we showed
$$
\int_0^\pi d\theta_k \  \sin\theta_k e^{ikr\cos\theta_k} = \int_{-1}^1 d\mu \ e^{ikr\mu} = 2\frac{\sin kr}{kr} \ .
$$
All that remains now is the integral over $k$. Canceling the factors of $k^2$, we have
$$
G(\mathbf{x},t) = \frac{c}{\pi r} \int_0^\infty dk \ \sin ck|t| \sin kr \ .
$$
This integral can be done by writing the expanding the sines in terms of complex exponentials and exploiting the properties of the delta function. We have
$$
\begin{align*}
G(\mathbf{x},t) &= \frac{c}{\pi r} \int_0^\infty dk \ \frac{1}{2i} \big(e^{ick|t|} - e^{-ick|t|}\big) \frac{1}{2i} \big(e^{ikr} - e^{-ikr}\big) \\
&= -\frac{c}{4\pi r} \int_0^\infty dk \ \big[e^{ik(c|t| + r)} - e^{ik(c|t| - r)} - e^{ik(-c|t| + r)} + e^{ik(-c|t| - r)}\big] \\
&= -\frac{c}{4\pi r} \big[2\pi\delta(c|t| + r) - 2\pi\delta(c|t| - r) - 2\pi\delta(-c|t| + r) + 2\pi\delta(-c|t| - r)\big] \\
&= \frac{c}{2r} \big[\delta(c|t| - r) - \delta(c|t| + r)\big] \\
&= \frac{1}{2r} \big[\delta(|t| - r/c) - \delta(|t| + r/c)\big] \ .
\end{align*}
$$
Now, since $|t| > 0$ and $r > 0$, the second delta function must vanish since we can never have $|t| = -r/c$, which means
$$
G(\mathbf{x},t) = \frac{1}{2r} \delta(|t| - r/c) = \frac{1}{2r} [\delta(t - r/c) + \delta(t + r/c)] \ .
$$
This is called the *symmetrized Green's function* for the wave equation for reasons we'll understand in a moment.
### Retarded and Advanced Solutions

Let's now examine each term in this Green's function more closely. It'll be useful to define
$$
t_\text{ret} \equiv t - \frac{r}{c} \quad , \quad t_\text{adv} \equiv t + \frac{r}{c} \ .
$$
We call $t_\text{ret}$ the *retarded time* and $t_\text{adv}$ the *advanced time* for reasons that'll become clear in a moment. Similarly, we define
$$
\begin{align*}
&G_\text{ret}(\mathbf{x}, t) \equiv \frac{\delta(t - r/c)}{r} = \frac{\delta(t_\text{ret})}{r} \ , \\
&G_\text{adv}(\mathbf{x}, t) \equiv \frac{\delta(t + r/c)}{r} = \frac{\delta(t_\text{adv})}{r} \ ,
\end{align*}
$$
where $G_\text{ret}(\mathbf{x}, t)$ is called the *retarded Green's function* and $G_\text{adv}(\mathbf{x}, t)$ is the *advanced Green's function*. In this notation, we can then rewrite the symmetrized Green's function as the average of the retarded and advanced Green's functions,
$$
G(\mathbf{x}, t) = \frac{1}{2}[G_\text{ret}(\mathbf{x}, t) + G_\text{adv}(\mathbf{x}, t)] \ .
$$
Now, notice that both $G_\text{ret}(\mathbf{x}, t)$ and $G_\text{adv}(\mathbf{x}, t)$ each individually satisfy the inhomogeneous wave equation
$$
\nabla^2 G - \frac{1}{c^2} \frac{\partial^2 G}{\partial t^2} = -4\pi\delta(\mathbf{x})\delta(t) \ .
$$
That is, the retarded and advanced Green's functions are each individually valid Green's function. By the principle of superposition then, any weighted average of the two will also be a valid Green's function to the wave equation. This means that in some sense the retarded and Green's functions are more fundamental than the symmetrized Green's function found before.

Now, let's ask how these retarded and advanced Green's functions behave physically with time. Let's first look at the retarded Green's function
$$
G_\text{ret}(\mathbf{x}, t) = \frac{\delta(t_\text{ret})}{r} = \frac{\delta(t - r/c)}{r} \ .
$$
Notice that this function is only non-zero when $t = r/c$. Since $r/c \geq 0$, this function can only be non-zero when $t \geq 0$. In particular, this means $G_\text{ret}(\mathbf{x}, t) = 0$ when $t < 0$. When $t \geq 0$, the function is only non-zero when $t = r/c$. This is known as a *wavefront*. As time increases, this wavefront propagates outward from the origin as a spherical wave with a constant speed $c$, reaching a radius $r$ at time $t = r/c$ and going to infinity as $t \to \infty$. We can thus think of the retarded Green's function as a spherical wave propagating outward in time from the origin at a constant speed.

FIGURE (show outward spherical wave)

What about the advanced Green's function? In that case, we instead have
$$
G_\text{adv}(\mathbf{x}, t) = \frac{\delta(t_\text{adv})}{r} = \frac{\delta(t + r/c)}{r} \ .
$$
Notice this function is only non-zero when $t = -r/c$. Since $r/c \geq 0$, $G_\text{adv}(\mathbf{x}, t)$ can only be non-zero when $t \leq 0$. Thus, we must have $G_\text{adv}(\mathbf{x}, t) = 0$ when $t > 0$. When $t \leq 0$, the wavefront of this function is only non-zero when $-t = r/c$. At time $t=0$, the wavefront propagates outward to infinity with *decreasing* time, at the same constant speed $c$. To an observer, it would seem like a wave is coming in from infinity and terminating at the origin at time $t=0$. Equivalently, we can think of the advanced Green's function as a spherical wave propagating out from the origin at constant speed, but *backward* in time instead of forward.

FIGURE (show inward spherical wave)

Now, from a physical perspective, only the *retarded* Green's function seems physical. If we want to think of the source charges as being the cause of an EM field, then we want to think of the field as propagating outward from the source as time increases, not propagating inward toward the source as time increases. This is the causality condition we mentioned before, which did not follow from Maxwell's equations alone. For this reason, on physical grounds we reject the advanced Green's function and insist that only the retarded Green's function be valid. Causality is thus an extra condition we impose on the theory, not one that follows from anything we've encountered before in the theory of electromagnetism.

Thus, from here on we will *always* assume that the only valid Green's function for the wave equation is the *retarded* Green's function. That is, we'll set $G(\mathbf{x}, t) \equiv G_\text{ret}(\mathbf{x}, t)$. Shifting the Green's function back to $\mathbf{x}'$ and $t'$, we thus finally have
$$
\boxed{
G(\mathbf{x} - \mathbf{x}', t - t') \equiv \frac{\delta(t' - t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|}
} \ ,
$$
where the retarded time $t_\text{ret}$ is given by
$$
\boxed{
t_\text{ret} \equiv t - \frac{|\mathbf{x} - \mathbf{x}'|}{c}
} \ .
$$
We can now convolve this Green's function with the source function $\rho(\mathbf{x},t)$ to get the particular solution for the wave equation,
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' dt' \ \rho(\mathbf{x}', t') G(\mathbf{x}-\mathbf{x}', t-t') = \int d^3\mathbf{x}' dt' \ \rho(\mathbf{x}', t') \frac{\delta(\mathbf{x} - \mathbf{x}', t' - t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Notice that we can go ahead and integrate with respect to $t'$ to get a volume integral. If we do that we get
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \ \frac{\rho(\mathbf{x}', t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Evidently, the particular solution to the wave equation looks exactly like the one for Poisson's equation, except that now there's a time delay $|\mathbf{x} - \mathbf{x}'|/c$ for the source at $\mathbf{x}'$ to affect the field at $\mathbf{x}$. That is, at time $t'$, the field propagates outward from $\mathbf{x}'$ as a wave traveling at a finite speed $c$. This means any source disturbance will take time to affect the field. Nothing is instantaneous.

## Retarded Potentials

With the particular solution to the inhomogeneous wave equation in hand we can now immediately write down the particular solutions for the scalar and vector potentials in terms of the source distributions, provided we work in the Lorentz gauge.

For the scalar potential $\phi(\mathbf{x},t)$, we have
$$
\boxed{
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \frac{\rho(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|}
} \ .
$$
For the vector potential $\mathbf{A}(\mathbf{x},t)$, each component $A_i$ will satisfy its own inhomogeneous wave equation
$$
\square A_i = -\frac{4\pi}{c} J_i \ .
$$
Identifying the source as $J_i/c$, we can thus express the particular solution for the vector potential in vector form as
$$
\boxed{
\mathbf{A}(\mathbf{x},t) = \frac{1}{c} \int d^3\mathbf{x}' \frac{\mathbf{J}(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} 
} \ .
$$
Notice that now the equipotentials at any given point in time are now a function of the source at a previous time, not the value of the source at the current time. If a source is disturbed, it will take a finite amount of time for this disturbance to propagate out to a given field point and change the equipotentials. This lag time is of course just the time $|\mathbf{x} - \mathbf{x}'|/c$ for the disturbance to propagate from the source point to the field point when moving at the speed of light $c$.

### Coulomb Gauge

Remember that the particular solutions for the potentials above are only true in the Lorentz gauge, since only in the Lorentz gauge do the potentials satisfy the inhomogeneous wave equation. If one chooses another gauge the solutions will look different. For instance, in the Coulomb gauge $\nabla \cdot \mathbf{A} = 0$, the potentials instead satisfy the equations
$$
\begin{align*}

\nabla^2 \phi &= -4\pi\rho \ , \\
\nabla^2 \mathbf{A} - \frac{1}{c^2} \frac{\partial^2 \mathbf{A}}{\partial t^2} &= -\frac{4\pi}{c} \mathbf{J} + \frac{1}{c} \frac{\partial}{\partial t} \nabla\phi \ .

\end{align*}
$$
This means the scalar potential will satisfy Poisson's equation, which means we get what the same solution we got in electrostatics, except that the charge density now depends on the *current* time $t$,
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' \frac{\rho(\mathbf{x}',t)}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Assuming we first solve for $\phi(\mathbf{x},t)$, the vector potential would again solve the inhomogeneous wave equation, except with the usual current density $\mathbf{J}(\mathbf{x},t)$ replaced by the *transverse current density* defined by
$$
\mathbf{J}_T(\mathbf{x},t) \equiv \mathbf{J}(\mathbf{x},t) - \frac{1}{4\pi} \frac{\partial}{\partial t} \nabla\phi(\mathbf{x},t) \ .
$$
This means in the Coulomb gauge the vector potential would have the particular solution
$$
\mathbf{A}(\mathbf{x},t) = \frac{1}{c} \int d^3\mathbf{x}' \frac{\mathbf{J}_T(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
While this is a perfectly valid choice of gauge, its disadvantage is that it seems to suggest the scalar potential depends instantaneously on the charge density, while the vector potential depends on the transverse current density at a time in the past, thus breaking the symmetry between the behavior of the two fields and the intuitive sense that signals should only affect fields at a finite propagation speed. Nevertheless, the fields will still be the same in either choice of gauge, as they must.

### Jefimenko Equations

If we like, we can now use these particular solutions for the potentials to find the particular solutions for the EM fields, known as the *Jefimenko equations*. These equations are the electrodynamic generalization of the Coulomb and Biot-Savart laws from statics. We'll derive these equations in the Lorentz gauge. Since the fields must be the same in any gauge, their forms will still be the same whether we choose the Coulomb gauge or something else.

Let's focus on the E-field first. Using the solutions above, we can express $\mathbf{E}(\mathbf{x},t)$ in terms of the scalar and vector potentials as
$$
\begin{align*}
\mathbf{E}(\mathbf{x},t) &= -\nabla \phi(\mathbf{x},t) - \frac{1}{c} \frac{\partial}{\partial t} \mathbf{A}(\mathbf{x},t) \\
&= -\nabla \int d^3\mathbf{x}' \frac{\rho(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} - \frac{1}{c^2} \frac{\partial}{\partial t} \int d^3\mathbf{x}' \frac{\mathbf{J}(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \\
&= - \int d^3\mathbf{x}' \bigg[\nabla \frac{\rho(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} + \frac{1}{c^2} \frac{\partial_t \mathbf{J}(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|}\bigg] \ .
\end{align*}
$$
We now need to evaluate the gradient in the first term. By the product rule for gradients we have
$$
\nabla \frac{\rho(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} = \frac{\nabla \rho(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} + \rho(\mathbf{x}',t_\text{ret}) \nabla \frac{1}{|\mathbf{x} - \mathbf{x}'|} \ .
$$

Now, recall that $\nabla r = \mathbf{x}/r$, $\nabla 1/r = -\mathbf{x}/r^3$, and $t_\text{ret} = t - |\mathbf{x} - \mathbf{x}'|/c$. This means by the chain rule we have
$$
\begin{align*}
&\nabla \frac{1}{|\mathbf{x} - \mathbf{x}'|} = -\frac{\mathbf{x} - \mathbf{x}'}{|\mathbf{x} - \mathbf{x}'|^3} \ , \\
&\nabla \rho(\mathbf{x}',t_\text{ret}) = -\frac{1}{c} \frac{\mathbf{x}-\mathbf{x}'}{|\mathbf{x}-\mathbf{x}'|} \partial_t \rho(\mathbf{x}',t_\text{ret}) \ .
\end{align*}
$$
Plugging these back into the above integral, we finally get
$$
\boxed{
\mathbf{E}(\mathbf{x},t) = \int d^3\mathbf{x}' \bigg[\rho(\mathbf{x}',t_\text{ret}) \frac{\mathbf{x}-\mathbf{x}'}{|\mathbf{x}-\mathbf{x}'|^3} + \frac{\partial_t \rho(\mathbf{x}',t_\text{ret})}{c} \frac{\mathbf{x}-\mathbf{x}'}{|\mathbf{x}-\mathbf{x}'|^2} - \frac{\partial_t \mathbf{J}(\mathbf{x}',t_\text{ret})}{c^2} \frac{1}{|\mathbf{x}-\mathbf{x}'|} \bigg]
} \ .
$$
Notice when the source distributions are *static* the time derivative terms both vanish, leaving us with Coulomb's law from electrostatics as we'd expect. Provided $|\mathbf{x}-\mathbf{x}'| \ll ct$, we can set $t_\text{ret} \approx t$ and assume the source distributions affect the E-field instantaneously, which is what we implicitly assume in electrostatics.

Indeed, this formula is the electrodynamic generalization of Coulomb's law. It now must also account for the fact that the E-field depends on time-varying charge and current distributions. Also notice the factors of $c$ in the time-dependent terms. This means these terms won't contribute much to the E-field *unless* the charge and current distributions are changing rapidly relative to the speed of light $c$. This is why we can often use Coulomb's law as an approximation for slowly varying charge distributions.

Let's now focus on the B-field. We can express $\mathbf{B}(\mathbf{x},t)$ in terms of the vector potential as
$$
\mathbf{B}(\mathbf{x},t) = \nabla \times \mathbf{A}(\mathbf{x},t) = \nabla \times \frac{1}{c} \int d^3\mathbf{x}' \frac{\mathbf{J}(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Pulling the curl inside the integral and using the product rule for curls, we can write
$$
\begin{align*}
\mathbf{B}(\mathbf{x},t) &= \frac{1}{c} \int d^3\mathbf{x}' \nabla \times \frac{\mathbf{J}(\mathbf{x}',t_\text{ret})}{|\mathbf{x} - \mathbf{x}'|} \\
&= \frac{1}{c} \int d^3\mathbf{x}' \bigg[\nabla \frac{1}{|\mathbf{x} - \mathbf{x}'|} \times \mathbf{J}(\mathbf{x}',t_\text{ret}) + \frac{1}{|\mathbf{x} - \mathbf{x}'|} \nabla \times \mathbf{J}(\mathbf{x}',t_\text{ret})\bigg] \ .
\end{align*}
$$
We again use the fact that $\nabla 1/r = -\mathbf{x}/r^3$, and use the chain rule for curls to write
$$
\nabla \times \mathbf{J}(\mathbf{x}',t_\text{ret}) = \frac{1}{c} \partial_t \mathbf{J}(\mathbf{x}',t_\text{ret}) \times \frac{\mathbf{x}-\mathbf{x}'}{|\mathbf{x}-\mathbf{x}'|} \ .
$$
Plugging this back in to the integral, we finally get
$$
\boxed{
\mathbf{B}(\mathbf{x},t) = \frac{1}{c} \int d^3\mathbf{x}' \bigg[\mathbf{J}(\mathbf{x}',t_\text{ret}) \times \frac{\mathbf{x}-\mathbf{x}'}{|\mathbf{x}-\mathbf{x}'|^3} + \frac{\partial_t \mathbf{J}(\mathbf{x}',t_\text{ret})}{c} \times \frac{\mathbf{x}-\mathbf{x}'}{|\mathbf{x}-\mathbf{x}'|^2} \bigg]
} \ .
$$
Notice that in the static case the time derivative in the second term vanishes, leaving us with the Biot-Savart law from magnetostatics provided $t_\text{ret} \approx t$. Indeed, this is the electrodynamics generalization of the Biot-Savart law, now accounting for the fact that the current density can vary with time. Since the second term contains an extra factor of $c$ in the denominator, it won't appreciably affect the B-field unless the current density is varying rapidly relative to the speed of light.

Both of these equations together are known as the *Jefimenko equations*. Curiously, these equations were evidently never formally written down until the 1960s. Perhaps the main reason for this is that, while these equations are nice to be aware of, they're not particularly useful in practice to calculate the EM fields in electrodynamics. Indeed, it's usually much easier just to calculate the potentials explicitly and then differentiate them to find the fields.

## Lienard-Wiechert Potentials

We'd now like to calculate the potentials for what should be the simplest problem in electrodynamics, the moving point charge. As we'll see, even the potentials and fields for such a simple problem turn out to be surprisingly complicated compared to those for the stationary point charge from electrostatics. However, they contain a powerful implication: Namely, moving charges *radiate*. That is, they emit electromagnetic energy.

### Potentials

To start, suppose we have a point charge $q$ moving along some trajectory $\mathbf{x}_p(t)$ in space with velocity $\mathbf{v}_p(t)$. Then its charge and current distributions are given by
$$
\rho(\mathbf{x}',t') = q \delta(\mathbf{x}' - \mathbf{x}_p(t')) \quad , \quad \mathbf{J}(\mathbf{x}',t') = q \mathbf{v}_p(t) \delta(\mathbf{x}' - \mathbf{x}_p(t')) \ .
$$
Substituting these distributions into the potentials and evaluating the trajectory at the retarded time $t_\text{ret}$, we get
$$
\phi(\mathbf{x},t) = \int d^3\mathbf{x}' q \frac{\delta\big(\mathbf{x}' - \mathbf{x}_p(t_\text{ret}(t'))\big)}{|\mathbf{x} - \mathbf{x}'|} \quad , \quad \mathbf{A}(\mathbf{x},t) = \frac{1}{c} \int d^3\mathbf{x}' q \mathbf{v}_p(t_\text{ret}) \frac{\delta\big(\mathbf{x}' - \mathbf{x}_p(t_\text{ret}(t'))\big)}{|\mathbf{x} - \mathbf{x}'|} \ .
$$
Note that now the retarded time $t_\text{ret}$ will depend implicitly on the source time $t'$ since $\mathbf{x}_p$ depends on $t'$. Since we need to be very careful about this when doing the time integration, we'll explicitly write $t_\text{ret} = t_\text{ret}(t')$. To evaluate these integrals it'll be useful to rewrite the delta functions in the factored form
$$
\delta\big(\mathbf{x}' - \mathbf{x}_p(t_\text{ret}(t'))\big) = \delta(\mathbf{x}' - \mathbf{x}_p(t')) \delta(t' - t_\text{ret}(t')) \ .
$$
This is completely equivalent to the original delta function provided we insert an integral over $t'$ into the potentials,
$$
\begin{align*}
&\phi(\mathbf{x},t) = \int d^3\mathbf{x}' dt' q \frac{\delta(\mathbf{x}' - \mathbf{x}_p(t'))}{|\mathbf{x} - \mathbf{x}'|} \delta(t' - t_\text{ret}(t')) \ , \\
&\mathbf{A}(\mathbf{x},t) = \frac{1}{c} \int d^3\mathbf{x}' dt' q \mathbf{v}_p(t') \frac{\delta(\mathbf{x}' - \mathbf{x}_p(t'))}{|\mathbf{x} - \mathbf{x}'|} \delta(t' - t_\text{ret}(t')) \ .
\end{align*}
$$
Now we can interchange the order of integration and evaluate the volume integrals first. Upon performing the volume integration, the delta function picks out $\mathbf{x}' = \mathbf{x}_s(t')$, leaving us with the following integrals over $t$',
$$
\begin{align*}
&\phi(\mathbf{x},t) = q \int dt' \frac{\delta(t' - t_\text{ret}(t'))}{|\mathbf{x} - \mathbf{x}_p(t')|} \ , \\
&\mathbf{A}(\mathbf{x},t) = \frac{q}{c} \int dt' \mathbf{v}_p(t')\frac{\delta(t' - t_\text{ret}(t'))}{|\mathbf{x} - \mathbf{x}_p(t')|} \ .
\end{align*}
$$
Let's now look closer at the argument $f(t')$ of these remaining delta functions. Writing out $t_\text{ret}(t')$ explicitly, we have
$$
f(t') \equiv t' - t_\text{ret}(t') = t' - \bigg(t - \frac{|\mathbf{x} - \mathbf{x}_p(t')|}{c}\bigg) \ .
$$
Now, we'd like to write the delta function $\delta(f(t'))$ inside the integrals above using the delta function identity
$$
\delta(f(t')) = \sum_i \frac{\delta(t' - t_i)}{\quad \big|\frac{\partial}{\partial t'} f(t')\big|_{t' = t_i}} \ ,
$$
where $t' = t_i$ are the roots of $f(t')$. But first we need to figure out what the roots of $f(t')$ are. In principle, such a function could have no roots, or arbitrarily many arbitrary roots. But *physically*, we want to insist that $f(t') = 0$ only at some *unique* retarded time $t_\text{ret}$ which depends only on $\mathbf{x}$ and $t$, and not on $t'$. It can be proven that under certain regularity conditions on $f(t')$ that such a unique retarded time exists. In that case, $f(t') = 0$ only when $t' = t_\text{ret}$, and we can write
$$
\delta(f(t')) = \frac{\delta(t' - t_\text{ret})}{\quad \frac{\partial}{\partial t'} f(t')\big|_{t' = t_\text{ret}}} \ .
$$
Note that now we're abusing notation a bit by now assuming $t_\text{ret}$ is a fixed value, not a function of $t'$. That is, $t_\text{ret}$ without the argument is now the (assumed to be unique) implicit solution to the equation
$$
t_\text{ret}(t_\text{ret}) = t - \frac{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}{c} \ .
$$
Next, we need to evaluate the derivative in the denominator and set $t' = t_\text{ret}$. We have
$$
\begin{align*}
\frac{\partial}{\partial t'} f(t') \bigg|_{t' = t_\text{ret}} &= \frac{\partial}{\partial t'} \bigg[t' - \bigg(t - \frac{|\mathbf{x} - \mathbf{x}_p(t')|}{c}\bigg)\bigg]_{t' = t_\text{ret}} \\
&= 1 + \frac{1}{c} \frac{\partial}{\partial t'} |\mathbf{x} - \mathbf{x}_p(t')| \bigg|_{t' = t_\text{ret}} \\
&= 1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|} \ .
\end{align*}
$$
Plugging this back into the delta function identity, we have
$$
\delta(t-t_\text{ret}(t')) = \frac{\delta(t' - t_\text{ret})}{1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}} \ .
$$
Plugging this back into the integral and evaluating the delta function, the scalar potential becomes
$$
\begin{align*}
\phi(\mathbf{x},t) &= q \int dt' \frac{\delta(t' - t_\text{ret}(t'))}{|\mathbf{x} - \mathbf{x}_p(t')|} \\
&= q \int dt' \frac{\delta(t' - t_\text{ret})}{1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}} \frac{1}{|\mathbf{x} - \mathbf{x}_p(t')|} \\
&= \frac{q}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|} \bigg(1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}\bigg)^{-1} \ .
\end{align*}
$$

Thus, the scalar potential for a moving point charge $q$ with trajectory $\mathbf{x}_p(t)$ and velocity $\mathbf{v}_p(t)$ is given by
$$
\boxed{
\phi(\mathbf{x},t) = \frac{q}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|} \bigg(1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}\bigg)^{-1} 
} \ .
$$
Notice we easily recover the Coulomb potential when the particle is at rest, as we'd expect. Interestingly, when the charge is moving we now have what appears to be a relativistic correction to the scalar potential given by the factor
$$
\mathcal{R} \equiv \bigg(1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}\bigg)^{-1} \ .
$$
Provided the particle is moving slowly relative to the speed of light, we have $|\mathbf{v}_p| \ll c$, which means $\mathcal{R} \to 1$, in which case we recover a Coulomb-like scalar potential,
$$
\phi(\mathbf{x},t) \approx \frac{q}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|} \ .
$$
In this limit the equipotentials propagate outward from the moving charge as spherical waves moving at the speed of light. However, when $|\mathbf{v}_p| \sim c$ these waves are no longer spherical. In this limit, we have two cases. In the plane normal to the charge's motion, $\mathcal{R} = 1$ at all speeds, which means the field lines in this plane still move outward as circular waves. However, outside this normal plane the equipotentials get *contracted* near the moving charge, with $\mathcal{R} \to \infty$ as $|\mathbf{v}_p| \to c$.

FIGURE (equipotentials of moving point charge)

Similarly, the vector potential for a moving point charge is given by
$$
\boxed{
\mathbf{A}(\mathbf{x},t) = \frac{\mathbf{v}_p(t_\text{ret})}{c} \frac{q}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|} \bigg(1 - \frac{\mathbf{v}_p(t_\text{ret})}{c} \cdot \frac{\mathbf{x} - \mathbf{x}_p(t_\text{ret})}{|\mathbf{x} - \mathbf{x}_p(t_\text{ret})|}\bigg)^{-1} 
} \ .
$$
Notice that when the velocity of the particle is zero the vector potential vanishes, reflecting the fact that only moving point charges can create magnetic fields. Notice the vector potential is directly proportional to the scalar potential above, with
$$
\mathbf{A}(\mathbf{x},t) = \frac{\mathbf{v}_p(t_\text{ret})}{c} \phi(\mathbf{x},t) \ .
$$
These two potentials for a moving point charge are known as the *Lienard-Wiechert potentials*. Together, these potentials are the electrodynamic generalization of the usual Coulomb potential for a stationary point charge. One immediate consequence of the Lienard-Wiechert potentials is that moving charges *radiate*. That is, they emit energy in the form of EM waves. We'll discuss this radiation in more detail later.

Note that more commonly in textbooks, one defines $\mathbf{n}_p \equiv (\mathbf{x} - \mathbf{x}_p)/|\mathbf{x} - \mathbf{x}_p|$ and $\boldsymbol{\beta}_p \equiv \mathbf{v}_p / c$ and writes the Lienard-Wiechert potentials in the equivalent form
$$
\begin{align*}
&\phi(\mathbf{x},t) = \bigg[\frac{q}{(1 - \boldsymbol{\beta}_p \cdot \mathbf{n}_p)|\mathbf{x} - \mathbf{x}_p|}\bigg]_{t_\text{ret}} \ , \\
&\mathbf{A}(\mathbf{x},t) = \bigg[\frac{q\boldsymbol{\beta}_p}{(1 - \boldsymbol{\beta}_p \cdot \mathbf{n}_p)|\mathbf{x} - \mathbf{x}_p|}\bigg]_{t_\text{ret}} \ .
\end{align*}
$$
In this form it's understood that the arguments inside the brackets are evaluated at the retarded time $t_\text{ret}$.

### Fields




- Follow wikipedia article for more here. Seems like the simplest approach.
- There are different retarded times in this problem, one that depends on $t'$ and one that doesn't. Figure this out.




## Conservation Laws

We'll now derive the important conservation laws of electrodynamics. We've already shown, rather required, that the conservation of charge be satisfied in electrodynamics via the continuity equation
$$
\frac{\partial \rho}{\partial t} + \nabla \cdot \mathbf{J} = 0 \ .
$$
We now want to see what the conservation laws look like for the other quantities of interest, namely electromagnetic energy,  momentum, and angular momentum. 

Before exploring these, it'll be helpful to analyze the general form of a local conservation law. Suppose $Q(t)$ is the amount of some scalar quantity of interest enclosed in some given closed region $\mathcal{V}$ of space at a given time $t$. This could be for instance the total charge inside the region, the total energy, or a component of a field's momentum. Now, $Q(t)$ can only change in one of two possible ways, either $Q$ flows out of (or into) the region, or $Q$ gets created (or destroyed) inside the region via some source. In mathematical terms, we can express this conservation law by writing
$$
\frac{dQ}{dt} = -(\text{rate of flow out}) + (\text{rate of creation}) \ .
$$
The minus sign reflects the fact that $Q$ will decrease if it flows out of the region. Now, assuming we can define $Q$ in terms of some density function $\rho_Q(\mathbf{x},t)$, we can write
$$
Q(t) = \int_\mathcal{V} d^3\mathbf{x} \ \rho_Q(\mathbf{x},t) \ .
$$
Similarly, if $Q$ flows out of the volume with some flow velocity $\mathbf{v}_Q(\mathbf{x},t)$, we can express the rate of flow of $Q$ out of the region in terms of a flux density vector $\mathbf{J}_Q(\mathbf{x},t)$ defined by $\mathbf{J}_Q \equiv \rho_Q \mathbf{v}_Q$. Then we have
$$
\text{rate of flow out} = \int_\mathcal{S} \mathbf{J}_Q \cdot d\mathbf{a} \ .
$$
If we now define $R_Q(\mathbf{x},t)$ to be the creation of $Q(t)$ per unit volume inside $\mathcal{V}$, we can write the conservation law above as
$$
\frac{d}{dt} \int_\mathcal{V} d^3\mathbf{x} \ \rho_Q(\mathbf{x},t) = -\int_\mathcal{S} \mathbf{J}_Q \cdot d\mathbf{a} + \int_\mathcal{V} d^3\mathbf{x} \ R_Q(\mathbf{x},t) \ .
$$
Moving the time derivative inside the integral on the left and then using the divergence theorem to rewrite the surface integral on the right as a volume integral, we finally arrive at the following continuity equation for $Q(t)$,
$$
\frac{\partial \rho_Q}{\partial t} + \nabla \cdot \mathbf{J}_Q = R_Q \ .
$$
This continuity equation looks exactly like the one for conservation of charge, except we generalize it by allowing for the possibility that $Q$ can be spontaneously created inside the region due to some source. For charge, we require that charge can never be spontaneously created since there's no way to create charge in classical physics. However, for other quantities like energy or momentum there are indeed sources that can create them.

### Conservation of Energy

Recall from the previous chapter that the potential energy $\mathcal{U}$ stored in an EM field is given by
$$
\mathcal{U} = \frac{1}{8\pi} \int d^3\mathbf{x} \ (\mathbf{E}^2 + \mathbf{B}^2) \ .
$$
The energy density $u(\mathbf{x},t)$ of the EM field is of course
$$
u = \frac{1}{8\pi}(\mathbf{E}^2 + \mathbf{B}^2) \ .
$$
Let's look at the time derivative of the energy density and see what we find. By the product rule, we can write
$$
\frac{\partial u}{\partial t} = \frac{1}{4\pi} \bigg(\mathbf{E} \cdot \frac{\partial \mathbf{E}}{\partial t} + \mathbf{B} \cdot \frac{\partial \mathbf{B}}{\partial t}\bigg) \ .
$$
From Maxwell's equations, we know that
$$
\frac{\partial \mathbf{E}}{\partial t} = c \nabla \times \mathbf{B} - 4\pi \mathbf{J} \quad , \quad \frac{\partial \mathbf{B}}{\partial t} = -c \nabla \times \mathbf{E} \ .
$$
Substituting these back in, we have
$$
\frac{\partial u}{\partial t} = \frac{c}{4\pi} \bigg(\mathbf{E} \cdot (\nabla \times \mathbf{B}) - \mathbf{B} \cdot (\nabla \times \mathbf{E})\bigg) - \mathbf{J} \cdot \mathbf{E} \ .
$$
Now, one can easily show using index notation that the following vector calculus identity holds,
$$
\nabla \cdot (\mathbf{E} \times \mathbf{B}) = -\mathbf{E} \cdot (\nabla \times \mathbf{B}) + \mathbf{B} \cdot (\nabla \times \mathbf{E}) \ .
$$
We thus evidently have the following conservation law for the potential energy stored in an EM field,
$$
\frac{\partial u}{\partial t} + \nabla \cdot \bigg(\frac{c}{4\pi}\mathbf{E} \times \mathbf{B}\bigg) = - \mathbf{J} \cdot \mathbf{E} \ .
$$
Inside the divergence must be the energy flux density, which we call the *Poynting vector* $\mathbf{S}$,
$$
\boxed{
\mathbf{S} \equiv \frac{c}{4\pi}\mathbf{E} \times \mathbf{B}
} \ .
$$
We'll see the Poynting vector several times in this course. For now observe a couple of things. First, since $\mathbf{S} \propto \mathbf{E} \times \mathbf{B}$, the energy will always flow *perpendicular* to the direction of both EM fields in a direction determined by the right-hand rule. Second, notice the Poynting vector has dimensions of energy flux density, which is energy per unit area per unit time, or equivalently power per unit area. This is true in every unit system, which means $\mathbf{S}$ has units $\text{erg}/\text{s} \cdot \text{cm}^2$ in Gaussian units, and units of $\text{W}/\text{m}^2$ in SI units.

The conservation law for electromagnetic potential energy is known as *Poynting's theorem*, given by
$$
\boxed{
\frac{\partial u}{\partial t} + \nabla \cdot \mathbf{S} = - \mathbf{J} \cdot \mathbf{E} 
} \ .
$$
Unlike with conservation of charge, conservation of energy contains a dissipation term on the right-hand side, reflecting the fact that currents can cause fields to dissipate energy. That energy is dissipated from the field rather than created is reflected in the minus sign. We've seen this dissipation term before. It's known as *Ohmic heating*. We saw that when an E-field is applied across a material it induces a current density inside the material which causes energy to dissipate out of the material at a rate $\mathbf{J} \cdot \mathbf{E}$, usually by heating up the material. This is just the generalization of the familiar $I^2R$ rate of energy dissipation of a resistor.

### Conservation of Momentum

Let's now turn our attention to conservation of EM field momentum. It may be surprising that EM fields have momentum. After all, in classical mechanics momentum is defined as mass times velocity. EM fields don't have mass, but they do indeed have momentum. This surprising fact is a direct consequence of the Lorentz force law.

Recall that the force acting on a charge $q$ moving at a velocity $\mathbf{v}$ in an external EM field is given by
$$
\mathbf{F} = q\mathbf{E} + q\frac{\mathbf{v}}{c} \times \mathbf{B} \ .
$$
By extension, the force on an arbitrary charge distribution due to an external EM field is given by
$$
\mathbf{F} = \int d^3\mathbf{x} \ \bigg(\rho\mathbf{E} + \frac{\mathbf{J}}{c} \times \mathbf{B}\bigg) \ .
$$
Now, according to Newton's second law, this force is also the time derivative of the mechanical momentum,
$$
\mathbf{F} = \frac{d\mathbf{P}_\text{mech}}{dt} \ .
$$
This means we have
$$
\frac{d\mathbf{P}_\text{mech}}{dt} = \int d^3\mathbf{x} \bigg(\rho\mathbf{E} + \frac{\mathbf{J}}{c} \times \mathbf{B}\bigg) \ .
$$
We'd now like to more closely examine the integrand, which is just the *force density* $\mathbf{f}$,
$$
\mathbf{f} \equiv \rho \mathbf{E} + \frac{\mathbf{J}}{c} \times \mathbf{B} \ .
$$
If we apply Maxwell's equations to the right-hand side to eliminate the densities, we get
$$
\begin{align*}
\mathbf{f} &= \frac{1}{4\pi} (\nabla \cdot \mathbf{E}) \mathbf{E} + \frac{1}{c} \bigg(\frac{c}{4\pi}\nabla \times \mathbf{B} - \frac{1}{4\pi} \frac{\partial \mathbf{E}}{\partial t} \bigg) \times \mathbf{B} \\
&= \frac{1}{4\pi} \bigg[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \times \mathbf{B}) \times \mathbf{B} - \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \times \mathbf{B} \bigg] \\
&= \frac{1}{4\pi} \bigg[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B}) \mathbf{B} + (\nabla \times \mathbf{B}) \times \mathbf{B} - \frac{1}{c} \frac{\partial \mathbf{E}}{\partial t} \times \mathbf{B} \bigg]
\end{align*}
$$
In the last step we used the fact that $\nabla \cdot \mathbf{B} = 0$ to insert a vanishing term $(\nabla \times \mathbf{B}) \times \mathbf{B}$. This will be useful in a moment. Next, notice we can simplify the cross product $(\partial_t \mathbf{E}) \times \mathbf{B}$ by applying the product rule along with Faraday's law to get
$$
\frac{\partial\mathbf{E}}{\partial t} \times \mathbf{B} = \frac{\partial}{\partial t} (\mathbf{E} \times \mathbf{B}) - \mathbf{E} \times \frac{\partial \mathbf{B}}{\partial t} = \frac{\partial}{\partial t} (\mathbf{E} \times \mathbf{B}) - c (\nabla \times \mathbf{E}) \times \mathbf{E} \ .
$$
Plugging this into $\mathbf{f}$, we now have
$$
\mathbf{f} = \frac{1}{4\pi} \big[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B})\mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B}) - \mathbf{E} \times (\nabla \times \mathbf{E})\big] - \frac{1}{4\pi c} \frac{\partial}{\partial t} (\mathbf{E} \times \mathbf{B}) \ .
$$
Next, we'll move the $\partial_t (\mathbf{E} \times \mathbf{B})$ to the left-hand side to write
$$
\mathbf{f} + \frac{\partial}{\partial t} \bigg(\frac{\mathbf{E} \times \mathbf{B}}{4\pi c}\bigg) = \frac{1}{4\pi} \big[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B})\mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B}) - \mathbf{E} \times (\nabla \times \mathbf{E})\big] \ .
$$
On the left-hand side we now have what appear to be two force density terms, one the mechanical force density $\mathbf{f}$, and the other the partial time derivative of some quantity
$$
\boxed{
\mathbf{g} \equiv \frac{1}{4\pi c} \mathbf{E} \times \mathbf{B}
} \ .
$$
Since $\mathbf{g}$ comes purely from the EM field and its time derivative is some kind of force density, we can interpret $\mathbf{g}$ as being the *momentum density* of the EM field. Indeed, observe that the dimensions of momentum density are clearly momentum per unit volume, or $\text{erg} \cdot \text{s} / \text{cm}^3$ in Gaussian units. The momentum stored in the field is then given by the volume integral
$$
\mathbf{P}_\text{EM} = \int d^3\mathbf{x} \ \mathbf{g} = \frac{1}{4\pi c} \int d^3\mathbf{x} \ (\mathbf{E} \times \mathbf{B}) \ .
$$
Notice that the momentum density $\mathbf{g}$ is evidently proportional to the Poynting vector, with
$$
\mathbf{g} = \frac{\mathbf{S}}{c^2} \ .
$$
In particular, this means the field's momentum will move along with the Poynting vector, perpendicular to the fields.

We now need to address the right-hand side of the continuity equation above,
$$
\mathbf{f} + \frac{\partial \mathbf{g}}{\partial t} = \frac{1}{4\pi} \big[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B})\mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B}) - \mathbf{E} \times (\nabla \times \mathbf{E})\big] \ .
$$
The goal is to extract from the right-hand side the divergence of something that we can identify as a momentum flux density. Since $\mathbf{g}$ is a vector, there will be one flux density vector per component of $\mathbf{g}$, or equivalently a rank-2 tensor for the entire vector, which we'll denote by $\mathbf{T}$.

At this point it makes sense to proceed in index notation. Observe that
$$
\begin{align*}
[(\nabla \cdot \mathbf{E}) \mathbf{E} - \mathbf{E} \times (\nabla \times \mathbf{E})]_i &= E_i \partial_j E_j - \varepsilon_{ijk} \varepsilon_{k\ell m} E_j \partial_\ell E_m \\
&= E_i \partial_j E_j - (\delta_{i\ell} \delta_{jm} - \delta_{im} \delta_{j\ell}) E_j \partial_\ell E_m \\
&= E_i \partial_j E_j - E_j \partial_i E_j + E_j \partial_j E_i \\
&= \partial_j (E_i E_j) - E_j \partial_j E_i - E_j \partial_i E_j + E_j \partial_j E_i \\
&= \partial_j (E_i E_j) - E_k \partial_i E_k \\
&= \partial_j \bigg(E_i E_j - \frac{1}{2} E_k E_k \delta_{ij} \bigg) \ .
\end{align*}
$$
Similarly, we have
$$
[(\nabla \cdot \mathbf{B}) \mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B})]_i = \partial_j \bigg(B_i B_j - \frac{1}{2} B_k B_k \delta_{ij} \bigg) \ .
$$
Putting these together, we have
$$
[(\nabla \cdot \mathbf{E}) \mathbf{E} - \mathbf{E} \times (\nabla \times \mathbf{E}) + (\nabla \cdot \mathbf{B}) \mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B})]_i = \partial_j \bigg[E_i E_j - B_i B_j - \frac{1}{2} (E_k E_k + B_k B_k) \delta_{ij} \bigg] \ .
$$
We've thus managed to reduce the entire right-hand side of the continuity equation above to the divergence of a rank-2 tensor, which we'll denote by $\mathbf{T}$. That $\mathbf{T}$ is in fact a valid tensor follows from the fact that it's a sum of outer products with a scalar multiplied by the identity tensor. In component notation, this tensor is given by
$$
T_{ij} = \frac{1}{4\pi} \bigg[E_i E_j + B_i B_j - \frac{1}{2}(E_k E_k + B_k B_k) \delta_{ij}\bigg] \ .
$$
Notice this tensor is also symmetric since $T_{ij} = T_{ji}$. We call $\mathbf{T}$ the *electromagnetic stress tensor* or the *momentum flux density tensor*. The conservation law for EM field momentum can thus be written in index notation as
$$
\frac{\partial g_i}{\partial t} - \partial_j T_{ij} = -f_i \ .
$$
In abstract notation, we can write the stress tensor as
$$
\boxed{
\mathbf{T} = \frac{1}{4\pi} \bigg[\mathbf{E} \otimes \mathbf{E} + \mathbf{B} \otimes \mathbf{B} - \frac{1}{2}(\mathbf{E} \cdot \mathbf{E} + \mathbf{B} \cdot \mathbf{B}) \mathbf{1}\bigg]
} \ .
$$
Similarly, the conservation law in abstract notation becomes
$$
\boxed{
\frac{\partial\mathbf{g}}{\partial t} - \nabla \cdot \mathbf{T} = -\mathbf{f} 
} \ .
$$
The fact that the divergence of $\mathbf{T}$ carries a minus sign is merely a convention of definition, ensuring that the stress tensor is positive definite and has a positive trace. However, the fact that the creation term $-\mathbf{f}$ has a minus sign reflects the fact that any charges moving through an EM field dissipate momentum from the fields rather than create momentum in the fields.

At this point, it's worth asking what it actually means to say an EM field has momentum. After all the fields contain no mass, and usually momentum is the product of mass with velocity. What we really mean is this: We want to make sure that momentum is conserved in electromagnetism. Since a charge placed in the presence of an EM field will accelerate and thus gain mechanical momentum, this momentum must be coming from somewhere. 

We could argue, correctly, that this momentum comes from this charge's interaction with the charge configurations creating the EM fields. This is completely equivalent to ignoring the charge configurations completely and assigning this missing momentum directly to the fields, which is what we mean. The utility of assigning this momentum to the fields rather than the charge configurations will make more sense when we discuss optics, where we'll see that even light can have momentum. A strong enough laser beam for instance can make a massive object move just like a mechanical contact force can.

### Conservation of Angular Momentum

Finally, we'll consider the remaining quantity that's often conserved in classical mechanics, angular momentum. Let's first consider again a single charged particle moving in the presence of an EM field. It will experience a Lorentz force
$$
\mathbf{F} = q\mathbf{E} + q\frac{\mathbf{v}}{c} \times \mathbf{B} \ .
$$
Now, there will also be a *Lorentz torque* $\mathbf{N}$ on the particle due to the field given by
$$
\mathbf{N} = \mathbf{x} \times \mathbf{F} = q\mathbf{x} \times\mathbf{E} + q \mathbf{x} \times\bigg(\frac{\mathbf{v}}{c} \times \mathbf{B}\bigg) \ .
$$
If we now consider an extended distribution of charge instead of a single point charge, the Lorentz torque becomes an integral
$$
\mathbf{N} = \int d^3 \mathbf{x} \ \bigg[\rho \mathbf{x} \times\mathbf{E} + \mathbf{x} \times\bigg(\frac{\mathbf{J}}{c} \times \mathbf{B}\bigg)\bigg] \ .
$$
The integrand of this torque we'll call the *torque density* $\boldsymbol{\nu}$,
$$
\boldsymbol{\nu} = \mathbf{x} \times \mathbf{f} = \rho \mathbf{x} \times\mathbf{E} + \mathbf{x} \times\bigg(\frac{\mathbf{J}}{c} \times \mathbf{B}\bigg) \ .
$$
From classical mechanics, we also know the torque is the time derivative of the mechanical angular momentum, which means
$$
\frac{d\mathbf{L}_\text{mech}}{dt} = \int d^3\mathbf{x} \ \boldsymbol{\nu} = \int d^3 \mathbf{x} \ \bigg[\rho \mathbf{x} \times\mathbf{E} + \mathbf{x} \times\bigg(\frac{\mathbf{J}}{c} \times \mathbf{B}\bigg)\bigg] \ .
$$
Now, we'll use our result from the previous section to rewrite the force density $\mathbf{f}$ as
$$
\mathbf{f} = \frac{1}{4\pi} \big[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B})\mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B}) - \mathbf{E} \times (\nabla \times \mathbf{E})\big] - \frac{1}{4\pi c} \frac{\partial}{\partial t} (\mathbf{E} \times \mathbf{B}) \ .
$$
If we again move the $\partial_t (\mathbf{E} \times \mathbf{B})$ term to the left-hand side and take the cross-product of $\mathbf{x}$ with each term, we get
$$
\boldsymbol{\nu} + \frac{\partial}{\partial t} \bigg(\frac{\mathbf{x} \times (\mathbf{E} \times \mathbf{B})}{4\pi c}\bigg) = \frac{1}{4\pi} \mathbf{x} \times \big[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B})\mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B}) - \mathbf{E} \times (\nabla \times \mathbf{E})\big] \ .
$$
On the left-hand side we have the angular momentum density along with the time derivative of some vector
$$
\boldsymbol{\mathcal{L}} \equiv \frac{1}{4\pi c} \mathbf{x} \times (\mathbf{E} \times \mathbf{B}) \ .
$$
This time derivative behaves as if it were the torque density of the field itself. We can thus interpret $\boldsymbol{\mathcal{L}}$ to be the *angular momentum density* stored in the field. Notice $\boldsymbol{\mathcal{L}}$ is clearly related to the ordinary momentum density $\mathbf{g}$, with
$$
\boldsymbol{\mathcal{L}} = \mathbf{x} \times \mathbf{g} \ .
$$
As we'd expect $\boldsymbol{\mathcal{L}}$ has the dimensions of angular momentum per unit volume, and its volume integral is the angular momentum stored in the field,
$$
\mathbf{L}_\text{EM} = \int d^3\mathbf{x} \ \boldsymbol{\mathcal{L}} \ .
$$
With this notation, the conservation law for angular momentum now has the form
$$
\boldsymbol{\nu} + \frac{\partial \boldsymbol{\mathcal{L}}}{\partial t} = \frac{1}{4\pi} \mathbf{x} \times \big[(\nabla \cdot \mathbf{E}) \mathbf{E} + (\nabla \cdot \mathbf{B})\mathbf{B} - \mathbf{B} \times (\nabla \times \mathbf{B}) - \mathbf{E} \times (\nabla \times \mathbf{E})\big] \ .
$$
Now, notice that the right-hand side apart from the $\mathbf{x} \times$ part is just the stress tensor $\mathbf{T}$ we derived before,
$$
\mathbf{T} = \frac{1}{4\pi} \bigg[\mathbf{E} \otimes \mathbf{E} + \mathbf{B} \otimes \mathbf{B} - \frac{1}{2}(\mathbf{E} \cdot \mathbf{E} + \mathbf{B} \cdot \mathbf{B}) \mathbf{1}\bigg] \ .
$$
We thus have
$$
\boldsymbol{\nu} + \frac{\partial \boldsymbol{\mathcal{L}}}{\partial t} = \nabla \cdot (\mathbf{x} \times \mathbf{T}) \ ,
$$
where we interpret $-\mathbf{x} \times \mathbf{T}$ as the antisymmetric rank-2 tensor whose components are given by
$$
M_{ij} \equiv T_{ij} x_k - T_{ik} x_j \ .
$$
The tensor $\mathbf{M} \equiv \mathbf{T} \times \mathbf{x}$ then represents the flux density of EM field's angular momentum.

Putting this all together, we've derived the following conservation law for the electromagnetic angular momentum,
$$
\frac{\partial \boldsymbol{\mathcal{L}}}{\partial t} + \nabla \cdot \mathbf{M} = -\boldsymbol{\nu} \ .
$$

- These last steps here are sloppy. The divergence now depends on the $\mathbf{x} \times$ part as well. Be more careful
- Evidently $\mathbf{M}$ is a rank-3 tensor, which should mean $\nabla \cdot \mathbf{M}$ is rank-2? Figure this out


$$
\frac{\partial \mathcal{L}_i}{\partial t} + \partial_i M_{ij} = -\nu_i
$$

$$
\frac{\partial \boldsymbol{\mathcal{L}}}{\partial t} + \nabla \cdot \mathbf{M} = -\boldsymbol{\nu}
$$

$$
\mathbf{N} = \int d^3\mathbf{x} \ \boldsymbol{\nu}(\mathbf{x},t) = \int d^3\mathbf{x} \ \frac{\partial \boldsymbol{\mathcal{L}}_\text{mech}}{\partial t}
$$

$$
\mathbf{M} = \mathbf{T} \times \mathbf{x} \quad \Longleftrightarrow \quad M_{ijk} = T_{ij} x_k - T_{ik} x_j
$$

$$
\boldsymbol{\mathcal{L}} = \mathbf{x} \times \mathbf{g} = \frac{1}{4\pi c} \mathbf{x} \times (\mathbf{E} \times \mathbf{B}) = \frac{1}{c^2} \mathbf{x} \times \mathbf{S}
$$



$$
\begin{align*}
\big[\mathbf{x} \times [(\nabla \cdot \mathbf{E}) \mathbf{E} - \mathbf{E} \times (\nabla \times \mathbf{E})]\big]_a &= \varepsilon_{abi} x_b [(\nabla \cdot \mathbf{E}) \mathbf{E} - \mathbf{E} \times (\nabla \times \mathbf{E})]_i \\
&= \varepsilon_{abi} x_b [E_i \partial_j E_j - \varepsilon_{ijk} \varepsilon_{k\ell m} E_j \partial_\ell E_m] \\
&= \varepsilon_{abi} x_b [E_i \partial_j E_j - (\delta_{i\ell} \delta_{jm} - \delta_{im} \delta_{j\ell}) E_j \partial_\ell E_m] \\
&= \varepsilon_{abi} x_b [E_i \partial_j E_j - E_j \partial_i E_j + E_j \partial_j E_i] \\
&= \varepsilon_{abi} x_b [\partial_j (E_i E_j) - E_j \partial_j E_i - E_j \partial_i E_j + E_j \partial_j E_i] \\
&= \varepsilon_{abi} x_b [\partial_j (E_i E_j) - E_k \partial_i E_k] \\
&= \varepsilon_{abi} x_b \partial_j \bigg(E_i E_j - \frac{1}{2} E_k E_k \delta_{ij} \bigg) \ .
\end{align*}
$$

$$
\partial_j x_b T_{ij} = \delta_{jb} T_{ij} + x_b \partial_j T_{ij} \\
= \bigg(E_i E_b + B_i B_b - \frac{1}{2} (E_k E_k + B_k B_k) \delta_{ib} \bigg) + x_b \partial_j T_{ij}
$$


$$
\partial_j M_{ijk} = \partial_j(T_{ij} x_k - T_{ik} x_j) = x_k \partial_j T_{ij} - T_{ik} \partial_j x_j = x_k \partial_j T_{ij} - 3 T_{ik}
$$




