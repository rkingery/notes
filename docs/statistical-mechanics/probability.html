<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Personal Notes - 27&nbsp; Probability</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../statistical-mechanics/kinetic-theory.html" rel="next">
<link href="../statistical-mechanics/thermodynamics.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Probability</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Personal Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">Classical Mechanics</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/newtonian-mechanics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Newtonian Mechanics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/simple-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Simple Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/reference-frames.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Reference Frames</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/lagrangian-mechanics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Lagrangian Mechanics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/hamiltonian-mechanics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Hamiltonian Mechanics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/central-forces.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Central Forces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/coupled-oscillations.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Coupled Oscillations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/rigid-bodies.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Rigid Bodies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/canonical-transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Canonical Transformations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/integrability-and-chaos.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Integrability and Chaos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../classical-mechanics/continuum-mechanics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Continuum Mechanics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Electrodynamics</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../electrodynamics/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../electrodynamics/electrostatics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Electrostatics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">Circuit Analysis</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/circuit-abstraction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Lumped Circuit Abstraction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/analysis.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Analyzing Circuits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/nonlinear-methods.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Nonlinear Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/digital-abstraction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Digital Abstraction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/amplifiers.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Amplifiers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/first-order-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">First-Order Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/second-order-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Second-Order Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/ac-analysis.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AC Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/op-amps.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Operational Amplifiers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../circuits/energy-power.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Energy and Power</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">Quantum Mechanics</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quantum-mechanics/identical-particles.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Identical Particles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quantum-mechanics/second-quantization.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Second Quantization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Statistical Mechanics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistical-mechanics/thermodynamics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Thermodynamics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistical-mechanics/probability.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Probability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../statistical-mechanics/kinetic-theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Kinetic Theory</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#univariate-probability" id="toc-univariate-probability" class="nav-link active" data-scroll-target="#univariate-probability">Univariate Probability</a>
  <ul class="collapse">
  <li><a href="#probability-measures" id="toc-probability-measures" class="nav-link" data-scroll-target="#probability-measures">Probability Measures</a></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables">Random Variables</a></li>
  <li><a href="#moments-and-cumulants" id="toc-moments-and-cumulants" class="nav-link" data-scroll-target="#moments-and-cumulants">Moments and Cumulants</a></li>
  <li><a href="#change-of-variables" id="toc-change-of-variables" class="nav-link" data-scroll-target="#change-of-variables">Change of Variables</a></li>
  </ul></li>
  <li><a href="#probability-distributions" id="toc-probability-distributions" class="nav-link" data-scroll-target="#probability-distributions">Probability Distributions</a>
  <ul class="collapse">
  <li><a href="#uniform-distribution" id="toc-uniform-distribution" class="nav-link" data-scroll-target="#uniform-distribution">Uniform Distribution</a></li>
  <li><a href="#gaussian-distribution" id="toc-gaussian-distribution" class="nav-link" data-scroll-target="#gaussian-distribution">Gaussian Distribution</a></li>
  <li><a href="#poisson-distribution" id="toc-poisson-distribution" class="nav-link" data-scroll-target="#poisson-distribution">Poisson Distribution</a></li>
  </ul></li>
  <li><a href="#multivariate-probability" id="toc-multivariate-probability" class="nav-link" data-scroll-target="#multivariate-probability">Multivariate Probability</a>
  <ul class="collapse">
  <li><a href="#random-vectors" id="toc-random-vectors" class="nav-link" data-scroll-target="#random-vectors">Random Vectors</a></li>
  <li><a href="#joint-moments-and-cumulants" id="toc-joint-moments-and-cumulants" class="nav-link" data-scroll-target="#joint-moments-and-cumulants">Joint Moments and Cumulants</a></li>
  <li><a href="#conditional-and-marginal-probability" id="toc-conditional-and-marginal-probability" class="nav-link" data-scroll-target="#conditional-and-marginal-probability">Conditional and Marginal Probability</a></li>
  <li><a href="#the-multivariate-gaussian-distribution" id="toc-the-multivariate-gaussian-distribution" class="nav-link" data-scroll-target="#the-multivariate-gaussian-distribution">The Multivariate Gaussian Distribution</a></li>
  </ul></li>
  <li><a href="#asymptotic-analysis" id="toc-asymptotic-analysis" class="nav-link" data-scroll-target="#asymptotic-analysis">Asymptotic Analysis</a>
  <ul class="collapse">
  <li><a href="#the-central-limit-theorem" id="toc-the-central-limit-theorem" class="nav-link" data-scroll-target="#the-central-limit-theorem">The Central Limit Theorem</a></li>
  <li><a href="#the-saddlepoint-approximation" id="toc-the-saddlepoint-approximation" class="nav-link" data-scroll-target="#the-saddlepoint-approximation">The Saddlepoint Approximation</a></li>
  </ul></li>
  <li><a href="#information-theory" id="toc-information-theory" class="nav-link" data-scroll-target="#information-theory">Information Theory</a>
  <ul class="collapse">
  <li><a href="#information-and-entropy" id="toc-information-and-entropy" class="nav-link" data-scroll-target="#information-and-entropy">Information and Entropy</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Probability</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The laws of thermodynamics are based on observations of <em>macroscopic</em> bodies. They encapsulate the coarse-grained properties of a body like its temperature, pressure, or volume. On the other hand, matter is fundamentally composed of atoms and molecules whose motions are governed by the more fundamental laws of mechanics. In principle, we could describe macroscopic bodies the same way, by examining the motions of every particle inside the body. But this is largely impractical as we’ll see later. A more useful approach is to look at which states large numbers of particles are <em>likely</em> to be in and use that information to make macroscopic predictions. Statistical mechanics is an inherently probabilistic description of a system. As such, it requires an understanding of the mathematical theory of probability, which we’ll discuss here.</p>
<section id="univariate-probability" class="level2">
<h2 class="anchored" data-anchor-id="univariate-probability">Univariate Probability</h2>
<section id="probability-measures" class="level3">
<h3 class="anchored" data-anchor-id="probability-measures">Probability Measures</h3>
<p>At its root, probability is based on the study of <em>events</em> or <em>random variables</em>. Suppose <span class="math inline">\(\mathcal{S}\)</span> is the set of all possible outcomes of an experiment, called the <strong>sample space</strong>. Any subset <span class="math inline">\(E \subset \mathcal{S}\)</span> is called an <strong>event</strong>. A <strong>probability measure</strong> is a set function <span class="math inline">\(\mathbb{Pr}(E)\)</span> that maps events to numerical values between zero and one. It’s meant to formalize the concept of <em>chance</em>. If an event has probability one, we think of that event as being 100% certain to occur. If it has probability zero, we think of the event as having 0% chance to occur. Any values in between mean we’re uncertain whether the event will occur.</p>
<p>Formally, a probability measure on a sample space <span class="math inline">\(\mathcal{S}\)</span> satisfies the following properties:</p>
<ul>
<li>Positivity: For any event <span class="math inline">\(E \subset \mathcal{S}\)</span>, <span class="math inline">\(\mathbb{Pr}(E) \geq 0\)</span>.</li>
<li>Additivity: If <span class="math inline">\(A \subset \mathcal{S}\)</span> and <span class="math inline">\(B \subset \mathcal{S}\)</span> are distinct, disjoint events, then <span class="math inline">\(\mathbb{Pr}(A \text{ or } B) = \mathbb{Pr}(A) + \mathbb{Pr}(B)\)</span>.</li>
<li>Normalization: For the entire sample space <span class="math inline">\(\mathcal{S}\)</span>, <span class="math inline">\(\mathbb{Pr}(\mathcal{S}) = 1\)</span>.</li>
</ul>
<p>While this is a nice formal definition of probability, it’s not practically useful for saying how we should design a probability measure in a practical setting. There are two common approaches for designing probability measures:</p>
<ol type="1">
<li><p>The <strong>objective</strong> approach: In this approach, we imagine running a bunch of experiments and counting the frequency of occurrences of an event. That is, if we run a large number <span class="math inline">\(N\)</span> of experiments and observe an event <span class="math inline">\(A\)</span> occurs exactly <span class="math inline">\(N_A\)</span> times, then we define <span class="math inline">\(\mathbb{Pr}(A)\)</span> by the ratio <span class="math display">\[
\boxed{\mathbb{Pr}(A) \equiv \lim_{N \rightarrow \infty} \frac{N_A}{N}} \ .
\]</span> The fact that the ratio <span class="math inline">\(\frac{N_A}{N}\)</span> stabilizes to a fixed value as <span class="math inline">\(N\)</span> gets infinitely large follows from the <strong>law of large numbers</strong>, which we’ll take as a kind of experimental fact.</p></li>
<li><p>The <strong>subjective</strong> approach: In this approach we take a more theoretical view by looking for a “maximally random” probability distribution that matches what we know to be true. That is, we seek to find the <em>maximum entropy</em> probability distribution that satisfies some given set of known constraints. We’ll talk more about the principle of maximum entropy when we get to information theory. The subjective approach is the one used most heavily in statistical mechanics.</p></li>
</ol>
</section>
<section id="random-variables" class="level3">
<h3 class="anchored" data-anchor-id="random-variables">Random Variables</h3>
<p>A <strong>random variable</strong> is a variable <span class="math inline">\(X\)</span> that doesn’t take on a fixed value, but rather a <em>random</em> value determined by a <strong>probability distribution</strong>. If <span class="math inline">\(E \subset \mathcal{S}\)</span> is an event, a random variable encodes that event using a numerical variable <span class="math inline">\(X=X(E)\)</span>. More formally, <span class="math inline">\(X(E)\)</span> is a function that maps events to a numerical value. It could be a real number, a complex number, or a real or complex vector, for example.</p>
<p>A one-dimensional random variable is a mapping <span class="math inline">\(X(E)\)</span> from events <span class="math inline">\(E \subset \mathcal{S}\)</span> into the real numbers <span class="math inline">\(\mathbb{R}\)</span>. In this case, we can define a <strong>cumulative distribution function</strong> or <strong>CDF</strong> as the function <span class="math display">\[
\boxed{P(x) \equiv \mathbb{Pr}\big(\{X \leq x\}\big)} \ .
\]</span> That is, the CDF is the probability that the random variable <span class="math inline">\(X \leq x\)</span>, where <span class="math inline">\(x\)</span> is just some real number. To obey the laws of probability, the CDF must satisfy the following properties:</p>
<ul>
<li>It’s an increasing function of <span class="math inline">\(x\)</span>. That is, if <span class="math inline">\(x_1 \leq x_2\)</span> then <span class="math inline">\(P(x_1) \leq P(x_2)\)</span>.</li>
<li>It has probability zero at <span class="math inline">\(x=-\infty\)</span>. That is, <span class="math inline">\(\lim_{x \rightarrow -\infty} P(x) = 0\)</span>.</li>
<li>It has probability one at <span class="math inline">\(x=\infty\)</span>. That is, <span class="math inline">\(\lim_{x \rightarrow \infty} P(x) = 1\)</span>.</li>
</ul>
<p>If the set of all values <span class="math inline">\(X\)</span> can take on are discrete, we say <span class="math inline">\(X\)</span> is a <strong>discrete random variable</strong>. In this case, we can define a <strong>probability mass function</strong> or <strong>PMF</strong> as the function <span class="math display">\[
\boxed{p(x) \equiv \mathbb{Pr}(X = n)} \ .
\]</span> By the laws of probability the PMF must satisfy the following properties:</p>
<ul>
<li>It’s between zero and one: <span class="math inline">\(0 \leq p(n) \leq 1\)</span>.</li>
<li>It sums to one over all values, i.e.&nbsp;<span class="math inline">\(\sum_{n \in \mathcal{S}} \ p(n) = 1\)</span>.</li>
<li>It’s related to the CDF by <span class="math inline">\(P(x) = \sum_{n \leq x} \ p(n)\)</span>.</li>
</ul>
<p>If the set of all values it can take on are continuous, we say <span class="math inline">\(X\)</span> is a <strong>continuous random variable</strong>. In this case, we can define a <strong>probability density function</strong> or <strong>PDF</strong> by the function <span class="math display">\[
\boxed{p(x) = \mathbb{Pr}(x \leq X \leq x + dx)} \ .
\]</span> By the laws of probability, the PDF must satisfy the following properties:</p>
<ul>
<li>It’s a positive function <span class="math inline">\(p(x) \geq 0\)</span>.</li>
<li>It integrates to one over all values, <span class="math inline">\(\int_\mathcal{S} p(x) dx = 1\)</span>.</li>
<li>It’s related to the CDF by differentiation, <span class="math inline">\(p(x) = \frac{d}{dx} P(x)\)</span>.</li>
</ul>
<p>Note that in physics the PDF has <em>units</em>. If <span class="math inline">\(x\)</span> has units of <span class="math inline">\([x]\)</span>, then evidently <span class="math inline">\(p(x)\)</span> must have units of <span class="math inline">\(\frac{1}{[x]}\)</span>. This is why we think of the PDF as a <em>density</em>. It’s a probability per unit <span class="math inline">\(x\)</span>.</p>
<p>By convention, if <span class="math inline">\(S \subset \mathbb{R}\)</span> we’ll assume <span class="math inline">\(p(x) = 0\)</span> on values of <span class="math inline">\(x\)</span> outside of <span class="math inline">\(S\)</span>. This means we can treat the PMF as a PDF by using delta functions to indicate where each discrete value of <span class="math inline">\(x\)</span> has non-zero <span class="math inline">\(p(x)\)</span>. That is, <span class="math display">\[
p(k) = p(x) \delta (x - k).
\]</span> For this reason, we’ll generally state results in the form of a continuous random variable.</p>
<p>Where there’s no risk of confusion, we’ll frequently abuse notation by using the lower case letter <span class="math inline">\(x\)</span> for both the random variable as well as the value it can take on.</p>
</section>
<section id="moments-and-cumulants" class="level3">
<h3 class="anchored" data-anchor-id="moments-and-cumulants">Moments and Cumulants</h3>
<p>For both discrete and continuous random variables we can define the <em>expected value</em> of a random function <span class="math inline">\(F(x)\)</span> by summing or integrating the function, weighted by the PMF or PDF. In the continuous case, we’d define the <strong>expected value</strong> <span class="math inline">\(\langle F(x) \rangle\)</span> by <span class="math display">\[
\boxed{\langle F(x) \rangle \equiv \int_{\mathbb{R}} dx \ F(x) p(x)} \ .
\]</span> Some of the functions <span class="math inline">\(F(x)\)</span> have special names:</p>
<ul>
<li>We call <span class="math inline">\(\mu \equiv \langle x \rangle\)</span> the <strong>mean</strong> of <span class="math inline">\(x\)</span>.</li>
<li>We call <span class="math inline">\(\mu_n \equiv \langle x^n \rangle\)</span> the <strong>n<sup>th</sup> moment</strong> of <span class="math inline">\(x\)</span>.</li>
<li>We call <span class="math inline">\(\langle e^{-ikx} \rangle\)</span> the <strong>characteristic function</strong> or <strong>CF</strong> of <span class="math inline">\(x\)</span>.</li>
</ul>
<p>Note the characteristic function is just the Fourier transform of the PDF since <span class="math display">\[
\langle e^{-ikx} \rangle = \tilde p(k) = \int_{\mathbb{R}} dx \ e^{-ikx} p(x).
\]</span> This means we can always go from the CF back to the PDF by taking the inverse Fourier transform, <span class="math display">\[
p(x) = \int_{\mathbb{R}} \frac{dx}{2\pi} \ e^{ikx} \tilde p(k).
\]</span> If we Taylor expand the CF, we can also evidently write <span class="math display">\[
\langle e^{-ikx} \rangle = \bigg\langle \sum_{n=0}^\infty \frac{(-ik)^n}{n!} x^n \bigg\rangle = \sum_{n=0}^\infty \frac{(-ik)^n}{n!} \mu_n.
\]</span> This means that the CF can be used to generate moments for a given distribution. Once we have a closed form for the CF, just expand it into a series and pick off each <span class="math inline">\(\mu_n = \langle x^n \rangle\)</span> term by term.</p>
<p>We can translate the CF to any other point <span class="math inline">\(x_0\)</span> by observing that <span class="math display">\[
\langle e^{-ik(x-x_0)} \rangle = \int_{\mathbb{R}} dx \ e^{-ik(x-x_0)} p(x) = e^{ikx_0} \tilde p(k).
\]</span> More useful to us in practice is not the characteristic function, but its <em>logarithm</em>, called the <strong>cumulant function</strong>. We’ll assume we can expand <span class="math inline">\(\log \tilde p(k)\)</span> as a Taylor Series about <span class="math inline">\(k=0\)</span> weighted by some coefficients <span class="math inline">\(\kappa_n\)</span>. By expanding out the MGF inside the log, we can evidently then write <span class="math display">\[
\log \tilde p(k) = \sum_{n=1}^\infty \frac{(-ik)^n}{n!} \kappa_n = \log\bigg(1 + \sum_{n=1}^\infty \frac{(-ik)^n}{n!} \mu_n \bigg).
\]</span> The sum inside the log is of the form <span class="math inline">\(1+\varepsilon\)</span>. We can thus expand the log in powers of <span class="math inline">\(\varepsilon\)</span> to get <span class="math display">\[
\log\big(1 + \varepsilon \big) = 1 + \varepsilon - \frac{1}{2} \varepsilon^2 + \frac{1}{3} \varepsilon^3 - \frac{1}{4} \varepsilon^4 + \cdots
\]</span> Using this expansion and matching term-by-term to the original expansion of <span class="math inline">\(\log \tilde p(k)\)</span> we can find expressions for the coefficients <span class="math inline">\(\kappa_n\)</span> in terms of the moments <span class="math inline">\(\mu_n\)</span>. These coefficients are called the <strong>n<sup>th</sup> cumulants</strong> of <span class="math inline">\(x\)</span>. The <em>first cumulant</em> turns out to be the <em>mean</em> of <span class="math inline">\(x\)</span>, <span class="math display">\[
\kappa_1 = \langle x \rangle.
\]</span> Intuitively, the <em>mean</em> <span class="math inline">\(\mu\)</span> of a distribution represents its center of mass or average value. To see why, suppose <span class="math inline">\(x\)</span> is a discrete random variable taking on values <span class="math inline">\(1, 2, \cdots, n\)</span> each with probability <span class="math inline">\(p(x) = \frac{1}{n}\)</span>. Then we’d have <span class="math display">\[
\mu = \sum_{x=1}^n x p(x) = \frac{1}{n} \sum_{x=1}^n x = \overline x.
\]</span> That is, the mean is just the unweighted average <span class="math inline">\(\overline x\)</span> from elementary math. In general each <span class="math inline">\(x\)</span> will be <em>weighted</em> by its probability <span class="math inline">\(p(x)\)</span> giving a <em>weighted average</em>.</p>
<p>The <em>second cumulant</em> is evidently given by <span class="math display">\[
\kappa_2 = \langle x^2 \rangle - \langle x \rangle^2.
\]</span> This is called the <strong>variance</strong> of <span class="math inline">\(x\)</span>, usually denoted <span class="math inline">\(\sigma^2\)</span>. By rearranging the right-hand side a little bit we can also write the variance in the form <span class="math display">\[
\boxed{\sigma^2 \equiv \langle x^2 \rangle - \langle x \rangle^2 = \langle (x-\mu) \rangle^2} \ .
\]</span> This gives an intuitive interpretation of the variance. It’s the <em>mean squared</em> difference from the mean. It’s a squared measure of the spread of the distribution. For this reason we often prefer to take its square root to get a measure of the spread in units of <span class="math inline">\(x\)</span> itself. This is called the <strong>standard deviation</strong>, <span class="math inline">\(\sigma \equiv \sqrt{\sigma^2}\)</span>.</p>
<p>The third cumulant is evidently given by <span class="math display">\[
\kappa_3 = \langle x^3 \rangle - 3\langle x^2 \rangle\langle x \rangle + 2 \langle x \rangle^3.
\]</span> It’s not obvious what this represents, but it turns out to represent the <strong>skewness</strong> of <span class="math inline">\(x\)</span>. That is, the tendency for the distribution to <em>skew</em> left or right by some amount. A distribution symmetric about its mean will have zero skew since all of the odd moments will vanish, hence <span class="math inline">\(\kappa_3 = 0\)</span>. Strictly speaking, the skewness is often normalized by dividing by <span class="math inline">\(\sigma^3\)</span>.</p>
<p>There’s a useful graphical trick that can be used to quickly find the relationship between cumulants and moments. The idea is to represent the n<sup>th</sup> cumulant <span class="math inline">\(\kappa_n\)</span> is a <em>bag</em> of <span class="math inline">\(n\)</span> points. Then we can get the n<sup>th</sup> <em>moment</em> by summing over all possible ways of distributing <span class="math inline">\(n\)</span> points among all possible bags <span class="math inline">\(1, 2, \cdots, n\)</span>. It’s easiest to show this by example. Here’s how to get the first few moments from the first few cumulants:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./resources/image-20230406152112380.png" class="img-fluid figure-img" width="450"></p>
</figure>
</div>
</section>
<section id="change-of-variables" class="level3">
<h3 class="anchored" data-anchor-id="change-of-variables">Change of Variables</h3>
<p>Occasionally we’ll want to make a change of variables from one random variable to another. To do that we need to figure out how the probabilities change under the change of variables. Suppose <span class="math inline">\(F=F(X)\)</span> is a random function of a random variable <span class="math inline">\(X\)</span>. If <span class="math inline">\(f = F(x)\)</span>, then probability that <span class="math inline">\(X \in [x, x+dx]\)</span> must be the same as the probability that <span class="math inline">\(F \in [f, f+df]\)</span>. Provided <span class="math inline">\(F(x)\)</span> is single-valued, that means we’d have <span class="math display">\[
p_F(f) df = p_X(x) dx.
\]</span> Dividing both sides by <span class="math inline">\(df\)</span> and noting that probabilities have to be positive, we have <span class="math display">\[
p_F(f) = p_X(x) \bigg|\frac{dx}{df}\bigg|.
\]</span> In general <span class="math inline">\(F(x)\)</span> need not be single-valued. This means we have to sum over all possible values <span class="math inline">\(x_i\)</span> such that <span class="math inline">\(f=F(x_i)\)</span>. In this case, we’d instead have <span class="math display">\[
\boxed{p_F(f) = \sum p_X(x_i) \bigg|\bigg(\frac{dx}{df}\bigg)_{x=x_i}\bigg|} \ .
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./resources/image-20230402072420947.png" class="img-fluid figure-img" width="350"></p>
</figure>
</div>
<hr>
<p><strong>Example: The Laplace Distribution</strong></p>
<p>Suppose <span class="math inline">\(p(x) \propto e^{-\lambda |x|}\)</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is some scale parameter.</p>
<ol type="1">
<li><p>Find the normalization constant <span class="math inline">\(\mathcal{N}\)</span> such that <span class="math inline">\(p(x) = \mathcal{N} e^{-\lambda |x|}\)</span>.</p>
<p>The PDF must integrate to one from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>. We have <span class="math display">\[
1 = \int_{-\infty}^\infty dx \ p(x) = \int_{-\infty}^\infty dx \  \mathcal{N} e^{-\lambda |x|} = \frac{2\mathcal{N}}{\lambda}.
\]</span> Thus, <span class="math inline">\(\mathcal{N} = \frac{\lambda}{2}\)</span>, so we finally just have <span class="math inline">\(p(x) = \frac{\lambda}{2} e^{-\lambda x}\)</span>. This is called the <em>Laplace distribution</em>.</p></li>
<li><p>Suppose <span class="math inline">\(f = x^2\)</span>. Find the new PDF <span class="math inline">\(p_F(f)\)</span> using a change of variables.</p>
<p>This transformation is multi-valued, with <span class="math inline">\(x = \pm \sqrt{f}\)</span>. Using the change of variables, we have <span class="math display">\[
\begin{align*}
p_F(f) &amp;= p(\sqrt{f}) \bigg|\frac{d}{df} \sqrt{f}\bigg| + p(-\sqrt{f}) \bigg|-\frac{d}{df} \sqrt{f}\bigg| \\
&amp;= \frac{\lambda}{2} e^{-\lambda \sqrt{f}} \bigg(\frac{1}{2\sqrt{f}} +  \frac{1}{2\sqrt{f}} \bigg) \\
&amp;= \frac{\lambda}{2\sqrt{f}} e^{-\lambda \sqrt{f}}. \\
\end{align*}
\]</span> Note that this new PDF is only defined when <span class="math inline">\(f \geq 0\)</span> due to the square root.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./resources/image-20230402072235128.png" class="img-fluid figure-img" width="500"></p>
</figure>
</div>
</section>
</section>
<section id="probability-distributions" class="level2">
<h2 class="anchored" data-anchor-id="probability-distributions">Probability Distributions</h2>
<p>So far I’ve used the term <em>distribution</em> rather loosely. More formally, given a random variable <span class="math inline">\(x\)</span>, a <strong>probability distribution</strong> is a specific functional form for <span class="math inline">\(p(x)\)</span>. We’d write <span class="math inline">\(x \sim p(x)\)</span> to indicate that <span class="math inline">\(x\)</span> is distributed as <span class="math inline">\(p(x)\)</span>. Usually <span class="math inline">\(p(x)\)</span> will be <em>parametric</em>, meaning it will have external parameters that tune the shape of the distribution. Here are some common univariate distributions we’ll see in statistical mechanics:</p>
<section id="uniform-distribution" class="level3">
<h3 class="anchored" data-anchor-id="uniform-distribution">Uniform Distribution</h3>
<p>The uniform distribution is perhaps the simplest distribution of all, with <span class="math inline">\(p(x) = const\)</span> on some set <span class="math inline">\(x \in \mathcal{S}\)</span>. As a shorthand we might write <span class="math inline">\(x \sim U(\mathcal{S})\)</span> to say <span class="math inline">\(x\)</span> is uniform on the set <span class="math inline">\(\mathcal{S}\)</span>. In the discrete case, <span class="math inline">\(x\)</span> takes on some number of values <span class="math inline">\(N\)</span> each with equal probability, for example if <span class="math inline">\(\mathcal{S} = \{1, 2, \cdots, n\}\)</span> we have <span class="math display">\[
\boxed{p(x) = \frac{1}{N}} \ .
\]</span> We can easily calculate the moments of a uniform random variable directly. In the discrete case, we’d have <span class="math display">\[
\langle x^n \rangle = \sum_{x=1}^N \ x^n \ p(x) = \frac{1}{N} (1^n + 2^n + \cdots + N^n).
\]</span> This is just an arithmetic sum in powers of <span class="math inline">\(n\)</span>. For example, the first two moments are <span class="math display">\[
\begin{align*}
\langle x \rangle &amp;= \frac{(N+1)}{2}, \\
\langle x^2 \rangle &amp;= \frac{(N+1)(2N+1)}{6}. \\
\end{align*}
\]</span> Using these we can directly calculate the mean and variance, which are <span class="math display">\[
\begin{align*}
\mu &amp;= \langle x \rangle = \frac{(N+1)}{2}, \\
\sigma^2 &amp;= \langle x^2 \rangle - \langle x \rangle^2 = \frac{N^2-1}{12}. \\
\end{align*}
\]</span> The characteristic function is just given by a geometric series in powers of <span class="math inline">\(e^{-ik}\)</span>, <span class="math display">\[
\tilde p(k) = \sum_{x=1}^N p(x) e^{-ikx} = \sum_{x=1}^N \frac{1}{N} \big(e^{-ik}\big)^n = \frac{e^{-ik}-e^{-ik(N+1)}}{1-e^{-ik}}.
\]</span> We could in principle calculate the cumulant function <span class="math inline">\(\log \tilde p(k)\)</span> as well, though it’s clearly not going to be as useful for finding the cumulants here. We’ll see later that the uniform distribution is the <em>maximum entropy</em> distribution when the only known constraints are that <span class="math inline">\(x\)</span> lies in some set <span class="math inline">\(S\)</span>.</p>
</section>
<section id="gaussian-distribution" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-distribution">Gaussian Distribution</h3>
<p>The Gaussian distribution is one of the most fundamental distributions in physics. In statistical mechanics, for example, it describes the velocity of gases in a box. Suppose <span class="math inline">\(x\)</span> is a continuous random variable defined on the whole real line. We say it’s Gaussian distributed if its PDF is given by <span class="math display">\[
\boxed{p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\bigg(-\frac{(x-\mu)^2}{2\sigma^2}\bigg)} \ .
\]</span> The Gaussian distribution has two parameters, a real number <span class="math inline">\(\mu\)</span> and a positive number <span class="math inline">\(\sigma^2\)</span>. As a shorthand we’ll sometimes say <span class="math inline">\(x\)</span> is Gaussian distributed by writing <span class="math inline">\(x \sim \mathcal{N}(\mu, \sigma^2)\)</span>. The PDF’s curve is the distinctive bell-shaped curve that falls off exponentially fast symmetrically around <span class="math inline">\(\mu\)</span>. For practical purposes, almost all of the probability mass lies in the range <span class="math inline">\(-3\sigma \leq x \leq 3\sigma\)</span>.</p>
<p>We can calculate the characteristic function by taking the Fourier transform of <span class="math inline">\(p(x)\)</span>. By using a couple of changes of variables and completing the square inside the exponent, we have, <span class="math display">\[
\begin{align*}
\tilde p(k) &amp;= \int_\mathbb{R} dx \ p(x) e^{-ikx} \\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \int_\mathbb{R} dx \ e^{-\frac{(x-\mu)^2}{2\sigma^2}-ikx} \\
&amp;= \frac{e^{-ik\mu}}{\sqrt{2\pi\sigma^2}} \int_\mathbb{R} dy \ e^{-\frac{y^2}{2\sigma^2}-ikx} \quad &amp;y \equiv&amp; \ x - \mu \\
&amp;= e^{-ik\mu-\frac{1}{2}k^2 \sigma^2} \int_\mathbb{R} \frac{dz}{\sqrt{2\pi\sigma^2}} \ e^{-\frac{z^2}{2\sigma^2}} \quad &amp;z \equiv&amp; \ y + ik\sigma^2 \\
&amp;= e^{-ik\mu-\frac{1}{2}k^2 \sigma^2}. \\
\end{align*}
\]</span> Notice the characteristic function is also itself a Gaussian, just with an imaginary shift. Taking the log immediately gives the cumulant function, which is just the exponent terms, <span class="math display">\[
\log \tilde p(k) = -ik\mu-\frac{1}{2}k^2 \sigma^2.
\]</span> Notice only the first two powers of <span class="math inline">\(k\)</span> appear in the cumulant. This means <span class="math inline">\(\kappa_1 = \mu\)</span>, <span class="math inline">\(\kappa_2 = \sigma^2\)</span>, and all higher cumulants are zero. Evidently, the parameter <span class="math inline">\(\mu\)</span> is just the <em>mean</em> and the parameter <span class="math inline">\(\sigma^2\)</span> is just the <em>variance</em>.</p>
<p>If we like, we can use the graphical trick to read off the moments as well. In this case, there can’t be any bags with more than two points, which greatly simplifies terms. The first few moments are, <span class="math display">\[
\begin{align*}
\langle x \rangle &amp;= \kappa_1 = \mu, \\
\langle x^2 \rangle &amp;= \kappa_2 + \kappa_1^2 = \sigma^2 + \mu^2, \\
\langle x^3 \rangle &amp;= 3 \kappa_2 \kappa_1 + \kappa_1^3 = 3\sigma^2 \mu + \mu^3, \\
\langle x^4 \rangle &amp;= 3 \kappa_2^2 + 6 \kappa_2 \kappa_1^2 + \kappa_1^4 = 3\sigma^4 + 6 \sigma^2 \mu^2 + \mu^4.
\end{align*}
\]</span> ### Binomial Distribution</p>
<p>Suppose we have a binary random variable <span class="math inline">\(x = 0, 1\)</span> that takes on the value one with a fixed probability <span class="math inline">\(p\)</span>. We can express its PMF simply as <span class="math display">\[
p(x) = p^x (1-p)^{1-x}.
\]</span> We’d call such an <span class="math inline">\(x\)</span> a <em>Bernoulli random variable</em>. A common example would be flipping a coin, where heads occurs with a fixed probability <span class="math inline">\(p=0.5\)</span>. Now suppose we allow the binary outcome to repeat <span class="math inline">\(n\)</span> times independently. It turns out that the <em>sum</em> of those <span class="math inline">\(n\)</span> outcomes is distributed in a similar way, except now <span class="math inline">\(x\)</span> can take on any value in the range <span class="math inline">\(x = 0, 1, \cdots, n\)</span>. Accounting for normalization, we have <span class="math display">\[
\boxed{p(x) = \binom{n}{x} p^x (1-p)^{n-x}} \ .
\]</span> We’d say <span class="math inline">\(x\)</span> is <em>binomially distributed</em> with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(n\)</span>, sometimes written as <span class="math inline">\(x \sim \text{Bin}(n,p)\)</span>. The normalization constant is the <em>binomial coefficient</em>, <span class="math display">\[
\binom{n}{x} \equiv \frac{n!}{x!(n-x)!}.
\]</span> The binomial coefficient represents the number of ways to choose <span class="math inline">\(x\)</span> points from a total of <span class="math inline">\(n\)</span> points, assuming the order the points are chosen is irrelevant.</p>
<p>The characteristic function of the binomial distribution can be found using the binomial theorem, <span class="math display">\[
\begin{align*}
\tilde p(k) &amp;= \sum_{x=0}^n \binom{n}{x} p^x (1-p)^{n-x} e^{-ikx} \\
&amp;= \sum_{x=0}^n \binom{n}{x} \big(p e^{-ik}\big)^x (1-p)^{n-x} \\
&amp;= \big(pe^{-ik} + (1-p) \big)^n.
\end{align*}
\]</span> Notice the parameter <span class="math inline">\(n\)</span> appears only in the exponent. This means the cumulant function is just <span class="math inline">\(n\)</span> times the cumulant function for the Bernoulli distribution, <span class="math display">\[
\log \tilde p_n(k) = n \log \tilde p_1(k).
\]</span> In particular, the cumulants are all proportional to <span class="math inline">\(n\)</span> times the Bernoulli cumulants. Expanding out <span class="math inline">\(\log \tilde p_1(k)\)</span>, we have <span class="math display">\[
\begin{align*}
\log \tilde p_1(k) &amp;= \log\big(pe^{-ik} + (1-p)\big) \\
&amp;= \log\big(1 + p(e^{-ik}-1)\big) \\
&amp;= p(e^{-ik}-1) - \frac{1}{2} p^2(e^{-ik}-1)^2 + \cdots \\
&amp;= p\bigg(-ik + \frac{(-ik)^2}{2} + \cdots\bigg) - \frac{1}{2} p^2 \bigg(-ik + \frac{(-ik)^2}{2} + \cdots\bigg)^2 + \cdots \\
&amp;= -ik p + \frac{(-ik)^2}{2} p(1-p) + \cdots
\end{align*}
\]</span> Thus, the mean and variance of the binomial distribution are just <span class="math display">\[
\mu = np, \quad \sigma^2 = np(1-p).
\]</span> These distributions can be straight-forwardly generalized to situations with more than binomial outcomes. The Bernoulli distribution generalizes to the <em>categorical distribution</em>, where <span class="math inline">\(x\)</span> is allowed to be one of <span class="math inline">\(k\)</span> categories, each with a fixed probability <span class="math inline">\(p_j\)</span>. Clearly those probabilities must sum to one. If the categories are <span class="math inline">\(x = 1, 2, \cdots, k\)</span> the PMF would be given by <span class="math display">\[
p(x) = p_1^{\delta_{1x}} p_2^{\delta_{2x}} \cdots p_k^{\delta_{kx}}, \quad x = 1, 2, \cdots, k.
\]</span> The binomial distribution generalizes to the <em>multinomial distribution</em>, where <span class="math inline">\(x\)</span> is now a <em>vector</em> whose j<sup>th</sup> component is the number of times category <span class="math inline">\(j\)</span> occurred in <span class="math inline">\(n\)</span> total trials. Each <span class="math inline">\(x_j = 0, 1, \cdots, n_j\)</span> is essentially its own binomial distribution, except we require <span class="math inline">\(n = n_1 + n_2 + \cdots n_k\)</span>. The PMF is given by <span class="math display">\[
p(x) = \frac{n!}{n_1!n_2!\cdots n_k!} p_1^{n_1} p_2^{n_2} \cdots p_2^{n_2}.
\]</span> The coefficient <span class="math inline">\(\frac{n!}{n_1!n_2!\cdots n_k!}\)</span> is called a <em>multinomial coefficient</em>. It’s a count of the number of ways to distribute <span class="math inline">\(n\)</span> points into <span class="math inline">\(k\)</span> bins such that each bin <span class="math inline">\(j\)</span> contains exactly <span class="math inline">\(n_j\)</span> points. We might say <span class="math inline">\(x\)</span> is multinomially distributed by writing <span class="math inline">\(x \sim \text{Multinomial}(n_1,n_2,\cdots,n_k; p_1, p_2, \cdots, p_k)\)</span>.</p>
</section>
<section id="poisson-distribution" class="level3">
<h3 class="anchored" data-anchor-id="poisson-distribution">Poisson Distribution</h3>
<p>Suppose we’re interested in answering the following question: What is the probability that <span class="math inline">\(x\)</span> events occur inside a time interval <span class="math inline">\([0,T]\)</span> provided each event is independent of the others, and that the probability of any one event occurring in an infinitesimal interval <span class="math inline">\([0,dt]\)</span> is a constant <span class="math inline">\(\alpha dt\)</span>. To figure out what <span class="math inline">\(p(x)\)</span> is, let’s imagine subdividing <span class="math inline">\([0,T]\)</span> up into <span class="math inline">\(N = \frac{T}{dt}\)</span> subintervals. In each subinterval, any single event is a Bernoulli random variable that either occurs with probability <span class="math inline">\(\alpha dt\)</span> or doesn’t occur with probability <span class="math inline">\((1-\alpha)dt\)</span>. If we assume each subinterval is independent of the others, the characteristic function of <span class="math inline">\(p(x)\)</span> is just <span class="math display">\[
\begin{align*}
\tilde p(k) &amp;= (\tilde p_1(k))^N \\
&amp;= \big(\alpha e^{-ik}dt + (1-\alpha)dt\big)^N \\
&amp;= \big(1 + \alpha dt(e^{-ik}-1)\big)^{T/dt} \\
&amp;\approx e^{\alpha T(e^{-ik} - 1)}. \\
\end{align*}
\]</span> The last equality becomes exact when <span class="math inline">\(dt\)</span> is infinitesimal. Let’s define <span class="math inline">\(\lambda \equiv \alpha T\)</span> as a dimensionless rate parameter. Then we can write the characteristic function as <span class="math display">\[
\tilde p(k) = e^{\lambda(e^{-ik} - 1)}.
\]</span> To get the sought after PDF <span class="math inline">\(p(x)\)</span> we just need to take the inverse Fourier transform of <span class="math inline">\(\tilde p(k)\)</span>. We have <span class="math display">\[
\begin{align*}
p(x) &amp;= \int_{\mathbb{R}} \frac{dk}{2\pi} \ \tilde p(k) e^{ikx} \\
&amp;= \int_{\mathbb{R}} \frac{dk}{2\pi} \ e^{\lambda (e^{-ik}-1)}e^{ikx} \\
&amp;= e^{-\lambda} \int_{\mathbb{R}} \frac{dk}{2\pi} \  e^{ikx} \sum_{n=0}^\infty \frac{(\lambda e^{-ik})^n}{n!} \\
&amp;= e^{-\lambda} \sum_{n=0}^\infty \frac{\lambda^n}{n!} \int_{\mathbb{R}} \frac{dk}{2\pi} e^{-ik(x-n)} \\
&amp;= e^{-\lambda} \sum_{n=0}^\infty \frac{\lambda^n}{n!} \delta(x-n). \\
\end{align*}
\]</span> The delta function forces <span class="math inline">\(x\)</span> to be a positive integer for <span class="math inline">\(p(x)\)</span> to be non-zero. That is, <span class="math inline">\(p(x)\)</span> is actually a PMF <span class="math display">\[
\boxed{p(x) = e^{-\lambda} \frac{\lambda^x}{x!}} \ .
\]</span> This is called the <em>Poisson distribution</em>. The Poisson distribution is used to model <em>counts</em> of events, where each event is allowed to occur independently with some fixed rate <span class="math inline">\(\lambda = \alpha T\)</span>. We can denote that <span class="math inline">\(x\)</span> is Poisson distributed by writing <span class="math inline">\(x \sim \text{Poisson}(\lambda)\)</span>.</p>
<p>This distribution has the unusual property that all its cumulants are equal. Indeed, observe we have <span class="math display">\[
\log \tilde p(k) = \lambda (e^{-ik} - 1) = \sum_{n=1}^\infty \frac{(-ik)^n}{n!} \lambda.
\]</span> That is, all the cumulants are just <span class="math inline">\(\lambda\)</span>. In particular, <span class="math inline">\(\mu = \sigma^2 = \lambda\)</span>.</p>
</section>
</section>
<section id="multivariate-probability" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-probability">Multivariate Probability</h2>
<section id="random-vectors" class="level3">
<h3 class="anchored" data-anchor-id="random-vectors">Random Vectors</h3>
<p>Let’s now look at the situation where we have <span class="math inline">\(N\)</span> random variables <span class="math inline">\(X_1, X_2, \cdots, X_N\)</span>. The vector of all such random variables is called a <strong>random vector</strong>, i.e.&nbsp;a vector <span class="math inline">\(\mathbf{X} = (X_1, X_2, \cdots, X_N)\)</span>. To each random vector we can assign a <strong>joint CDF</strong> of the form <span class="math display">\[
P(\mathbf{x}) \equiv \mathbb{Pr}(\{\mathbf{X} \leq \mathbf{x}\}) = \mathbb{Pr}(\{X_1 \leq x_1, X_2 \leq x_2, \cdots, X_N \leq x_N\}).
\]</span> The CDF must be an increasing function in each <span class="math inline">\(x_i\)</span>, go to <span class="math inline">\(0\)</span> as all <span class="math inline">\(x_i \rightarrow -\infty\)</span>, and go to <span class="math inline">\(1\)</span> as all the <span class="math inline">\(x_i \rightarrow \infty\)</span>.</p>
<p>If <span class="math inline">\(\mathbf{X}\)</span> is discrete, we can assign a <strong>joint PMF</strong> to each value in the support <span class="math inline">\(S \in \mathbb{R}^N\)</span> by defining <span class="math display">\[
\boxed{p(\mathbf{x}) \equiv \mathbb{Pr}(\{\mathbf{X} = \mathbf{x}\})} \ .
\]</span> The joint PMF is a valid probability, meaning it must satisfy <span class="math inline">\(0 \leq p(\mathbf{n}) \leq 1\)</span> and <span class="math inline">\(\sum_{\mathbf{n} \in S} p(\mathbf{n}) = 1\)</span>.</p>
<p>Similarly, if <span class="math inline">\(\mathbf{X}\)</span> is continuous, we can assign a <strong>joint PDF</strong> to each value in <span class="math inline">\(S \in \mathbb{R}^N\)</span> by defining <span class="math display">\[
p(\mathbf{x}) d^Nx \equiv \mathbb{Pr}(\{x_1 \leq X_1 \leq x_1 + dx_1, x_2 \leq X_2 \leq x_2 + dx_2, \cdots, x_N \leq X_N \leq x_N + dx_N\}),
\]</span> where <span class="math inline">\(d^N x = dx_1dx_2\cdots dx_N\)</span> is the <span class="math inline">\(N\)</span>-dimensional <em>volume element</em>. The joint PMF must satisfy both <span class="math inline">\(p(\mathbf{x}) \geq 0\)</span>, and <span class="math inline">\(\int_S d^N x \ p(\mathbf{x}) = 1\)</span>. Clearly the joint PMF is just a special case of the joint PDF, since we can always just use delta functions to express a PMF as a PDF.</p>
<p>As with ordinary random variables, we’ll frequently abuse notation by using <span class="math inline">\(\mathbf{x}\)</span> for both the random vector itself as well as its value where there’s no risk of confusion.</p>
</section>
<section id="joint-moments-and-cumulants" class="level3">
<h3 class="anchored" data-anchor-id="joint-moments-and-cumulants">Joint Moments and Cumulants</h3>
<p>For any function <span class="math inline">\(F(\mathbf{x})\)</span> of a random vector <span class="math inline">\(\mathbf{x}\)</span> we can define its expectation value as <span class="math display">\[
\boxed{\langle \mathbf{x} \rangle \equiv \int_S d^N x \ F(\mathbf{x}) p(\mathbf{x})} \ .
\]</span> For both discrete and continuous random vectors we can define the <strong>joint characteristic function</strong> <span class="math display">\[
\boxed{\tilde p(\mathbf{k}) \equiv \langle e^{-i\mathbf{k} \cdot \mathbf{x}} \rangle \equiv \int_{\mathbb{R}^N} d^Nx \ p(\mathbf{x}) e^{-i\mathbf{k} \cdot \mathbf{x}}} \ .
\]</span> By taking the logarithm of the joint CF, we can also define the <strong>joint cumulant function</strong> <span class="math inline">\(\log \tilde p(\mathbf{k})\)</span>. From these two functions we can extract the joint moments and cumulants. The <span class="math inline">\(n_1, n_2, \cdots, n_N\)</span> <strong>joint moment</strong> <span class="math inline">\(\mu_{n_1,n_2,\cdots,n_N} \equiv \langle x_1^{n_1} x_2^{n_2} \cdots x_N^{n_N} \rangle\)</span> of <span class="math inline">\(\mathbf{X}\)</span> is given by taking partial derivatives of the joint characteristic function, <span class="math display">\[
\langle x_1^{n_1} x_2^{n_2} \cdots x_N^{n_N} \rangle \equiv \frac{\partial^{n_1}}{\partial (-i k_1)^{n_1}} \frac{\partial^{n_2}}{\partial (-i k_2)^{n_2}} \cdots \frac{\partial^{n_N}}{\partial (-i k_N)^{n_N}} \tilde p(k_1, k_2, \cdots, k_N) \bigg |_{k_1=k_2=\cdots=k_N=0}.
\]</span> Similarly, the <span class="math inline">\(n_1, n_2, \cdots, n_N\)</span> <strong>joint cumulant</strong> <span class="math inline">\(\kappa_{n_1,n_2,\cdots,n_N}\)</span> is given by taking partial derivatives of the joint cumulant function, <span class="math display">\[
\kappa_{n_1,n_2,\cdots,n_N} \equiv \frac{\partial^{n_1}}{\partial (-i k_1)^{n_1}} \frac{\partial^{n_2}}{\partial (-i k_2)^{n_2}} \cdots \frac{\partial^{n_N}}{\partial (-i k_N)^{n_N}} \log \tilde p(k_1, k_2, \cdots, k_N) \bigg |_{k_1=k_2=\cdots=k_N=0}.
\]</span> The sum <span class="math inline">\(n \equiv n_1 + n_2 + \cdots n_N\)</span> determines the order of the moment or cumulant. Of particular interest are the first and second cumulants. The first cumulants are the means <span class="math inline">\(\mu_i \equiv \langle x_i \rangle\)</span>. We can think of these together by putting them all into a <strong>mean vector</strong> <span class="math inline">\(\boldsymbol{\mu} \equiv \langle \mathbf{x} \rangle \equiv \big(\mu_1, \mu_2, \cdots, \mu_N\big)\)</span>. The second cumulants are the <em>covariances</em>, <span class="math inline">\(\sigma_{ij} \equiv \kappa_{ij}\)</span>. We can put all these into an <span class="math inline">\(N \times N\)</span> matrix to get the <strong>covariance matrix</strong> <span class="math inline">\(\mathbf{\Sigma} \equiv \big(\sigma_{ij}\big)_{i,j=1,\cdots,N}\)</span>. The diagonal entries of the covariance matrix correspond to the usual <em>variances</em> <span class="math inline">\(\sigma_i^2 = \sigma_{ii}\)</span>. The off diagonal terms are the <em>covariances</em>, capturing the dependence or <em>correlation</em> between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p>
<p>We can use the same graphical trick to express joint cumulants in terms of joint moments. The only difference is we need to label each point by its index and bag them appropriately. We can use this to show that the covariance <span class="math inline">\(\sigma_{ij} \equiv \kappa_{ij}\)</span> can be written as <span class="math display">\[
\sigma_{ij} = \langle x_i x_j \rangle - \langle x_i \rangle \langle x_j \rangle = \langle (x_i - \mu_i)(x_j - \mu_j) \rangle.
\]</span> In matrix notation, the entire covariance matrix can be expressed using moments as <span class="math display">\[
\boxed{\mathbf{\Sigma} = \langle \mathbf{x}\mathbf{x}^\top \rangle - \langle \mathbf{x} \rangle \langle \mathbf{x} \rangle^\top = \langle (\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^\top \rangle} \ .
\]</span> This implies the covariance matrix must in fact be a positive semi-definite matrix. That is, <span class="math display">\[
\mathbf{\Sigma} = \mathbf{\Sigma}^\top, \quad \mathbf{v}^\top\mathbf{\Sigma}\mathbf{v} \geq 0 \quad \forall \mathbf{v} \neq \mathbf{0}.
\]</span></p>
</section>
<section id="conditional-and-marginal-probability" class="level3">
<h3 class="anchored" data-anchor-id="conditional-and-marginal-probability">Conditional and Marginal Probability</h3>
<p>We can get smaller joint probabilities by “summing out” the random variables we don’t need. These are called <strong>marginal probabilities</strong> or <strong>unconditional probabilities</strong>. For example, if we have two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the marginal PDF <span class="math inline">\(p(y)\)</span> is given by integrating <span class="math inline">\(x\)</span> out of the joint PDF <span class="math inline">\(p(x,y)\)</span>, <span class="math display">\[
\boxed{p(y) \equiv \int_\mathbb{R} dx \ p(x,y)} \ .
\]</span> If we have <span class="math inline">\(N\)</span> random variables <span class="math inline">\(x_1,x_2,\cdots, x_s, x_{s+1},\cdots,x_N\)</span> and integrate out the last <span class="math inline">\(N-s\)</span> variables <span class="math inline">\(x_s, x_{s+1},\cdots,x_N\)</span>, then we get the marginal PDF <span class="math inline">\(p(x_1,x_2,\cdots, x_s)\)</span>, <span class="math display">\[
p(x_1,x_2,\cdots, x_s) \equiv \int_{\mathbb{R}^{N-s}} dx_s, dx_{s+1},dx_N \ p(x_1,x_2,\cdots, x_s, x_s, x_{s+1},\cdots,x_N).
\]</span> Similarly, we can define the <strong>conditional probabilities</strong>, which allow for random variables to depend on the outcome of other random variables directly. For example, for two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with joint PDF <span class="math inline">\(p(x,y)\)</span>, we can define the <em>conditional probability</em> of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> as <span class="math display">\[
\boxed{p(y|x) \equiv \frac{p(x,y)}{p(x)}} \ .
\]</span> We can think of <span class="math inline">\(p(x,y)\)</span> as a kind of <em>prior distribution</em> and <span class="math inline">\(p(x)\)</span> as a kind of normalization constant. Notice we can similarly write <span class="math inline">\(p(x,y) = p(x|y) p(y)\)</span>. If we plug this into the formula for <span class="math inline">\(p(y|x)\)</span> we get the well-known <strong>Bayes’ Rule</strong>, which says that <span class="math display">\[
p(y|x) = \frac{p(x|y)p(y)}{p(x)}.
\]</span> If we have <span class="math inline">\(N\)</span> random variables <span class="math inline">\(x_1,x_2,\cdots, x_s, x_{s+1},\cdots,x_N\)</span> and want to condition the first <span class="math inline">\(s\)</span> variables on the last <span class="math inline">\(N-s\)</span> variables, we’d similarly write <span class="math display">\[
p(x_1,x_2,\cdots, x_s) \equiv \frac{p(x_1,x_2,\cdots, x_s, x_{s+1},\cdots,x_N)}{p(x_{s+1},\cdots,x_N)}.
\]</span> We haven’t proven it, but it’s not hard to show that the marginal and conditional probabilities are indeed valid probabilities and PDFs.</p>
<p>When conditioning a random variable <span class="math inline">\(y\)</span> on another random variable <span class="math inline">\(x\)</span> gives no information about <span class="math inline">\(y\)</span>, we say that <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are <strong>independent</strong>, sometimes written <span class="math inline">\(x \perp y\)</span>. If <span class="math inline">\(x\)</span> gives no information about <span class="math inline">\(y\)</span>, that means we must have <span class="math inline">\(p(y|x) = p(y)\)</span>, which is equivalent to saying the joint PDF factors, <span class="math inline">\(p(x,y) = p(x) p(y)\)</span>. Clearly, if <span class="math inline">\(x\)</span> gives no information about <span class="math inline">\(y\)</span>, then <span class="math inline">\(y\)</span> gives no information about <span class="math inline">\(x\)</span> either. Independence is symmetric.</p>
<p>More generally, we say <span class="math inline">\(N\)</span> random variables <span class="math inline">\(x_1,x_2,\cdots,x_N\)</span> are mutually independent provided <span class="math display">\[
\boxed{p(x_1,x_2,\cdots,x_N) = p_1(x_1) p_2(x_2) \cdots p_N(x_N)} \ .
\]</span> In the special case where all <span class="math inline">\(N\)</span> variables also happen to come from the same distribution <span class="math inline">\(p(x)\)</span> we say they’re <strong>independent identically distributed</strong> or <strong>IID</strong>. In this simple case we just have <span class="math display">\[
p(x_1,x_2,\cdots,x_N) = \big(p(x)\big)^N.
\]</span> Independent random variables have the property that their <em>mixed cumulants</em> will always be zero. This is equivalent to saying that the joint expectation of any product of random variables value factors, <span class="math display">\[
\langle F_1(x_1) F_2(x_2) \cdots F_N(x_N) \rangle = \langle F_1(x_1) \rangle \langle F_2(x_2) \rangle \cdots \langle F_N(x_N) \rangle.
\]</span></p>
</section>
<section id="the-multivariate-gaussian-distribution" class="level3">
<h3 class="anchored" data-anchor-id="the-multivariate-gaussian-distribution">The Multivariate Gaussian Distribution</h3>
<p>While there are many joint probability distributions, the most important one to be aware of is the <strong>multivariate Gaussian distribution</strong>. Suppose <span class="math inline">\(\mathbf{x} = (x_1, x_2, \cdots, x_N)\)</span> are independent, with each <span class="math inline">\(x_i\)</span> Gaussian distributed with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma_i^2\)</span>. Then it’s easy to show their joint PDF is given by <span class="math display">\[
p(x_1, x_2, \cdots, x_N) = \bigg(\frac{1}{(2\pi)^N\sigma_1^2\sigma_2^2\cdots\sigma_N^2}\bigg)^{1/2} \exp\bigg(-\frac{1}{2} \sum_{i=1}^N \frac{(x_i-\mu_i)^2}{\sigma_i^2} \bigg).
\]</span> But what if <span class="math inline">\(\mathbf{x}\)</span> is <em>not</em> independent? All we have to do in that case is make a change of basis. Notice that joint PDF above is just the diagonalized form for the following joint PDF in vector form, <span class="math display">\[
\boxed{p(\mathbf{x}) = \bigg(\frac{1}{(2\pi)^N \det(\mathbf{\Sigma})}\bigg)^{1/2} \exp\bigg(-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}) \bigg)} \ .
\]</span> By making a change of basis or rotating <span class="math inline">\(\mathbf{\Sigma}\)</span>, this vectorized PDF gives the most general form of the Gaussian distribution for <span class="math inline">\(N\)</span> variables. This is the <em>multivariate Gaussian distribution</em>, denoted <span class="math inline">\(\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu},\mathbf{\Sigma})\)</span>.</p>
<p>Using the same diagonalization trick, it’s just as easy to show that the joint characteristic function is <span class="math display">\[
\boxed{\tilde p(\mathbf{k}) = \exp(-i\mathbf{k} \cdot \boldsymbol{\mu} - \frac{1}{2} \mathbf{k}^\top \mathbf{\Sigma} \mathbf{k})} \ ,
\]</span> and the joint cumulant is just <span class="math inline">\(\log \tilde p(\mathbf{k}) = -i\mathbf{k} \cdot \boldsymbol{\mu} - \frac{1}{2} \mathbf{k}^\top \mathbf{\Sigma} \mathbf{k}\)</span>. This again implies that only the first and second joint cumulants are non-zero for the multivariate Gaussian. All higher-order terms vanish. For this reason, multivariate Gaussian random variables satisfy a special condition known as <em>Wick’s Theorem</em>.</p>
<p><strong>Wick’s Theorem:</strong> Suppose <span class="math inline">\(\mathbf{x} = (x_1, x_2, \cdots, x_N)\)</span> is a Gaussian random vector with mean <span class="math inline">\(\boldsymbol{\mu} = \mathbf{0}\)</span>. Then the <span class="math inline">\(n\)</span><sup>th</sup> joint moments are given by <span class="math display">\[
\boxed{
\langle x_1^{n_1} x_2^{n_2} \cdots x_N^{n_N} \rangle =
\begin{cases}
0 &amp; n = \text{odd} \\
\text{sum of all pairwise contractions} &amp; n = \text{even} \\
\end{cases}
} \ .
\]</span> For example, suppose we wanted to calculate <span class="math inline">\(\langle x_1^2 x_2 x_3 \rangle\)</span>. In this case, the possible pairwise contractions are</p>
<ul>
<li><span class="math inline">\(x_1 x_1\)</span> and <span class="math inline">\(x_2 x_3\)</span> , which gives a term <span class="math inline">\(\sigma_{11} \sigma_{23}\)</span>,</li>
<li><span class="math inline">\(x_1 x_2\)</span> and <span class="math inline">\(x_1 x_3\)</span> , which gives a term <span class="math inline">\(\sigma_{12} \sigma_{13}\)</span>,</li>
<li><span class="math inline">\(x_1 x_3\)</span> and <span class="math inline">\(x_1 x_2\)</span> , which gives a term <span class="math inline">\(\sigma_{13} \sigma_{12}\)</span>.</li>
</ul>
<p>Summing each of these pairwise contractions together, we just have <span class="math display">\[
\langle x_1^2 x_2 x_3 \rangle = \sigma_{11} \sigma_{23} + 2 \sigma_{12} \sigma_{13}.
\]</span></p>
</section>
</section>
<section id="asymptotic-analysis" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-analysis">Asymptotic Analysis</h2>
<p>In this section we’ll focus on important results that apply for <em>large</em> numbers of random variables <span class="math inline">\(N \gg 1\)</span>.</p>
<section id="the-central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="the-central-limit-theorem">The Central Limit Theorem</h3>
<p>It turns out that the <em>sum</em> of random variables will often by approximately Gaussian distributed provided some minor regularity assumptions are met. This important result is called the <em>central limit theorem</em>.</p>
<p><strong>Central Limit Theorem:</strong> Suppose <span class="math inline">\(x = \sum_{i=1}^N x_i\)</span> is a sum of <span class="math inline">\(N\)</span> IID random variables with finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Then when <span class="math inline">\(N \gg 1\)</span> the probability density satisfies <span class="math display">\[
\boxed{p\bigg(\frac{x-N\mu}{\sqrt{N\sigma^2}}\bigg) \approx \frac{1}{\sqrt{2 \pi}} \exp\bigg(-\frac{1}{2}\bigg(\frac{x-N\mu}{\sqrt{N\sigma^2}}\bigg)^2\bigg)} \ .
\]</span> <strong>Proof:</strong> Suppose each <span class="math inline">\(x_i\)</span> is IID with distribution <span class="math inline">\(p_1(x_1)\)</span>. The characteristic function for <span class="math inline">\(p(x)\)</span> must then be <span class="math display">\[
\tilde p(k) = \langle e^{-i kx} \rangle = \langle e^{-i k\sum_{i=1}^N x_i} \rangle = \prod_{i=1}^N \langle e^{-i k x_i} \rangle = \tilde p(k_1, k_2, \cdots, k_N) \bigg |_{k_1=k_2=\cdots=k_N=k}.
\]</span> If we take the cumulant function <span class="math inline">\(\log \tilde p(k)\)</span> and expand it out directly, we have <span class="math display">\[
\log \tilde p(k) = \sum_{n=1}^\infty \frac{(-ik)^n}{n!} \kappa_n(x) = -ik \kappa_1(x) + \frac{(-ik)^2}{2} \kappa_2(x) + \cdots
\]</span> Expanding out the cumulant function <span class="math inline">\(\log \tilde p(k_1, k_2, \cdots, k_N)\)</span> and setting all <span class="math inline">\(k_i=k\)</span>, we have <span class="math display">\[
\begin{align*}
\log \tilde p(k_1, k_2, \cdots, k_N) &amp;= \sum_{n=0}^\infty \sum_{\sum n_j=n} \frac{(-ik_1)^{n_1} (-ik_2)^{n_2} \cdots (-ik_N)^{n_N}}{n!} \kappa_{n_1 n_2 \cdots n_N} \bigg |_{k_1=k_2=\cdots=k_N=k} \\
&amp;= \sum_{n=0}^\infty \sum_{\sum n_j=n} \frac{(-ik)^n}{n!} \kappa_{n_1 n_2 \cdots n_N} \\
&amp;= (-ik) \sum_{\sum n_j=1} \kappa_{n_1 n_2 \cdots n_N} + \frac{(-ik)^2}{2} \sum_{\sum n_j=2} \kappa_{n_1 n_2 \cdots n_N} + \cdots
\end{align*}
\]</span> Equating the two equations, we thus have <span class="math display">\[
\kappa_n(x) = \sum_{\sum n_j=n} \kappa_{n_1 n_2 \cdots n_N}.
\]</span> That is, the <span class="math inline">\(n\)</span><sup>th</sup> cumulant of the sum is the sum of all the joint <span class="math inline">\(n\)</span><sup>th</sup> cumulants. Now, suppose all the <span class="math inline">\(x_i\)</span> are independent. Then their joint PDF must factor as <span class="math display">\[
p(x_1,x_2,\cdots,x_N) = p_1(x_1) p_2(x_2) \cdots p_N(x_N).
\]</span> Moreover, since their mixed cumulants must be zero, the cumulants of the sum further reduce to <span class="math display">\[
\kappa_n(x) = \sum_{i=1}^N \kappa_n(x_i),
\]</span> Now suppose all the <span class="math inline">\(x_i\)</span> are identically distributed with the same PDF <span class="math inline">\(p_1(x_i)\)</span>. Then we further have just <span class="math display">\[
\kappa_n = N \kappa_{n,i}.
\]</span> Define another random variable <span class="math inline">\(y\)</span> by re-centering and rescaling <span class="math inline">\(x\)</span> as <span class="math display">\[
z \equiv \frac{x - N\mu}{\sqrt{N\sigma^2}}.
\]</span> Then the cumulants of <span class="math inline">\(z\)</span> are given by <span class="math display">\[
\begin{align*}
\kappa_1(z) &amp;= 0, \\
\kappa_2(z) &amp;= 1, \\
\kappa_n(z) &amp;= \frac{N\kappa_n(x_1)}{(N\sigma^2)^{n/2}} = O\big(N^{1-n/2}\big). \\
\end{align*}
\]</span> The higher order cumulants of <span class="math inline">\(z\)</span> evidently go to zero when <span class="math inline">\(N \gg 1\)</span>. But we already know the only distribution whose higher moments are zero is the Gaussian distribution. Thus, we’ve shown <span class="math display">\[
p(z) \approx \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}. \qquad \text{Q.E.D.}
\]</span> Note the central limit theorem is also true for non-IID random variables, provided the higher cumulants decay as <span class="math inline">\(\kappa_n(x) = O(N^{n/2})\)</span>.</p>
<p>In the proof of the CLT we implicitly assumed that the cumulants were all finite. What if that weren’t the case? This will happen if the PDF of each <span class="math inline">\(x_i\)</span> is heavy-tailed. Heavy-tailed distributions are commonly used to model <em>rare events</em>. It turns out then that the sum won’t in general converge to a Gaussian. In fact, if it does converge, it’ll converge to a <em>Levy distribution</em>, a general class of heavy-tailed distributions.</p>
</section>
<section id="the-saddlepoint-approximation" class="level3">
<h3 class="anchored" data-anchor-id="the-saddlepoint-approximation">The Saddlepoint Approximation</h3>
<p>In the section on thermodynamics, we saw both <em>intensive</em> variables and <em>extensive</em> variables. The intensive variables are ones that don’t depend on particle number at all, i.e.&nbsp;they’re <span class="math inline">\(O(1)\)</span> functions of <span class="math inline">\(N\)</span>. The extensive variables are linear in particle number, i.e.&nbsp;they’re <span class="math inline">\(O(N)\)</span>. In principle we could imagine other functional dependences on <span class="math inline">\(N\)</span> as well. For example, a variable could be <em>polynomial</em> in <span class="math inline">\(N\)</span>, i.e.&nbsp;<span class="math inline">\(O(N^p)\)</span> for some <span class="math inline">\(p\)</span>. More importantly, a variable can be <em>exponential</em> in <span class="math inline">\(N\)</span>, i.e.&nbsp;<span class="math inline">\(O(e^{N\phi})\)</span> for some <span class="math inline">\(\phi\)</span>. For example, the <em>volume</em> of a gas would be a variable that can scale exponentially with <span class="math inline">\(N\)</span>, since it often goes like <span class="math inline">\(V^N\)</span>.</p>
<p>When <span class="math inline">\(N \gg 1\)</span>, the sum of many exponential variables can be well approximated by the maximum term. Suppose we have <span class="math inline">\(n\)</span> non-negative variables <span class="math inline">\(x_1,x_2,\cdots,x_n\)</span> of the form <span class="math inline">\(x_i \sim e^{N\phi_i}\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span>. Then their sum <span class="math inline">\(S = \sum x_i\)</span> satisfies <span class="math display">\[
S \sim x_{max} = \max_{i=1,\cdots,n} x_i, \quad \text{when} \quad N \rightarrow \infty.
\]</span> When each <span class="math inline">\(x_i = A_i e^{N\phi_i}\)</span>, this just says <span class="math inline">\(S \approx A_{max} e^{N\phi_{max}}\)</span> when <span class="math inline">\(N \gg 1\)</span>. To see why this fact is true, note that since each <span class="math inline">\(x_i \geq 0\)</span>, we must have <span class="math inline">\(x_{max} \leq S \leq nx_{max}\)</span>. Since the logarithm is monotonic, if we take the log of each term and divide by <span class="math inline">\(N\)</span>, we have <span class="math display">\[
\frac{\log x_{max}}{N} \leq \frac{\log S}{N} \leq \frac{\log x_{max}}{N} + \frac{\log n}{N}.
\]</span> If we take <span class="math inline">\(N \rightarrow \infty\)</span> while holding <span class="math inline">\(n\)</span> fixed, then the term <span class="math inline">\(\frac{\log n}{N} \rightarrow 0\)</span>, which gives <span class="math display">\[
\frac{\log S}{N} \sim \frac{\log x_{max}}{N} = \phi_i.
\]</span></p>
<p>More useful for our purposes will be the <em>continuous</em> analog of this result, the <em>saddlepoint approximation</em>.</p>
<p><strong>Saddlepoint Approximation:</strong> Suppose we have a function of the form <span class="math inline">\(f(x) = e^{N\phi(x)}\)</span> where <span class="math inline">\(\phi(x)\)</span> grows polynomially. Then we have <span class="math display">\[
\boxed{S = \int_\mathbb{R} dx \ e^{N\phi(x)} \sim \sqrt{\frac{2\pi}{N|\phi''(x_{max})|}} e^{N\phi_{max}}} \ , \quad \text{as} \ \ N \rightarrow \infty.
\]</span> <strong>Proof:</strong> To see why this is true let’s first Taylor expand <span class="math inline">\(\phi(x)\)</span> around its global maximum <span class="math inline">\(x_{max}\)</span>. Since <span class="math inline">\(\phi'(x_{max}) = 0\)</span> and <span class="math inline">\(\phi''(x_{max}) \leq 0\)</span>, we have <span class="math display">\[
\phi(x) = \phi(x_{max}) - \frac{1}{2} |\phi''(x_{max})| (x-x_{max})^2 + O\big((x-x_{max})^3\big).
\]</span> Plugging this into the integral and simplifying then gives <span class="math display">\[
\begin{align*}
S &amp;= \int_\mathbb{R} dx \ \exp\bigg(N\phi_{max} - \frac{N}{2} |\phi''(x_{max})| (x-x_{max})^2 + \frac{N}{6} |\phi'''(x_{max})| (x-x_{max})^3 + \cdots\bigg) \\
&amp;= \int_\mathbb{R} dx \ \exp\bigg(N\phi_{max} - \frac{N}{2} |\phi''(x_{max})| (x-x_{max})^2\bigg) \exp\bigg(\frac{N}{6} |\phi'''(x_{max})| (x-x_{max})^3 + \cdots\bigg) \\
&amp;= e^{N\phi_{max}} \int_\mathbb{R} dx \ \exp\bigg(- \frac{N}{2} |\phi''(x_{max})| (x-x_{max})^2\bigg) \bigg(1 + \frac{N}{6}|\phi'''(x_{max})|(x-x_{max})^3 + \cdots\bigg) \\
&amp;= e^{N\phi_{max}} \sqrt{\frac{2\pi}{N|\phi''(x_{max})|}} \bigg(1 + O\bigg(\frac{1}{N}\bigg)\bigg).
\end{align*}
\]</span> The last line follows from the fact that the integral with <span class="math inline">\((x-x_{max})^3\)</span> vanishes since it’s an odd function, which means the next term <span class="math inline">\((x-x_{max})^4\)</span> has to be considered, which integrates to order <span class="math inline">\(O\big(N^{-3/2}\big)\)</span>.</p>
<p>Now, let’s again look at <span class="math inline">\(\frac{\log S}{N}\)</span> as <span class="math inline">\(N \rightarrow \infty\)</span>. We have <span class="math display">\[
\frac{\log S}{N} = \phi_{max} - \frac{1}{2N} \log \frac{N|\phi''(x_{max})|}{2\pi} + O\bigg(\frac{1}{N^2}\bigg).
\]</span> We can see that as <span class="math inline">\(N \rightarrow \infty\)</span>, <span class="math inline">\(\frac{\log S}{N} \rightarrow \phi_{max}\)</span> with a correction of order <span class="math inline">\(O\big(\frac{\log N}{N}\big)\)</span>. <span class="math inline">\(\text{Q.E.D.}\)</span></p>
<p>It’s interesting to observe that only the global maximum appears in this approximation. What if <span class="math inline">\(\phi(x)\)</span> had some other local maximum <span class="math inline">\(\phi(x_{max}')\)</span>? Strictly speaking we’d have to do the same approximation scheme about each of the maxima one-by-one. However, due to the presence of the exponential, if <span class="math inline">\(\phi(x_{max}') &lt; \phi(x_{max})\)</span>, then for large <span class="math inline">\(N\)</span> we’d have <span class="math display">\[
e^{N\phi(x_{max}')} \ll e^{N\phi(x_{max})}.
\]</span> In the limit where <span class="math inline">\(N \rightarrow \infty\)</span>, the correction term <span class="math inline">\(e^{-N\big(\phi(x_{max})-\phi(x_{max}')\big)} \rightarrow 0\)</span>. In this sense, we can indeed neglect the other local maxima as long as they’re less than <span class="math inline">\(\phi(x_{max})\)</span> and <span class="math inline">\(N \gg 1\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./resources/image-20230406152315205.png" class="img-fluid figure-img" width="400"></p>
</figure>
</div>
<p>By far the most useful corollary to this result for our purposes is the <em>Stirling Approximation</em>.</p>
<p><strong>Stirling Approximation:</strong> As <span class="math inline">\(N \rightarrow \infty\)</span>, we have <span class="math display">\[
\boxed{N! \sim N^N e^{-N} \sqrt{2\pi N}} \ .
\]</span> <strong>Proof:</strong> Observe by induction that we can write <span class="math inline">\(N!\)</span> as the following integral, <span class="math display">\[
N! = \int_0^\infty dx \ x^N e^{-x} = \int_0^\infty dx \ \exp\bigg(N\bigg(\log x - \frac{x}{N}\bigg)\bigg).
\]</span> Take <span class="math inline">\(\phi(x) = \log x - \frac{x}{N}\)</span>. This function is maximized when <span class="math inline">\(x_{max} = N\)</span>, where <span class="math inline">\(\phi_{max} = \log N - 1\)</span>. At this point we have <span class="math inline">\(\phi''(x_{max}) = - \frac{1}{N^2}\)</span>. Plugging all this into the saddlepoint approximation, we have <span class="math display">\[
\begin{align*}
N! &amp;= e^{N(\log N - 1)} \sqrt{\frac{2\pi}{N|-N^{-2}|}} \bigg(1 + O\bigg(\frac{1}{N}\bigg)\bigg) \\
&amp;= N^N e^{-N} \sqrt{2\pi N} \bigg(1 + O\bigg(\frac{1}{N}\bigg)\bigg). \quad \text{Q.E.D.}
\end{align*}
\]</span> Usually we’ll be more interested in <span class="math inline">\(\log N!\)</span> rather than <span class="math inline">\(N!\)</span> itself. In that case we just have <span class="math display">\[
\log N! = N \log N - N + \frac{1}{2} \log 2\pi N + O\bigg(\frac{1}{N}\bigg).
\]</span> We’ll typically imagine <span class="math inline">\(N\)</span> to be really big, like <span class="math inline">\(N \sim 10^{23}\)</span>. In that case we can neglect the sublinear terms and safely write <span class="math display">\[
\boxed{\log N! \approx N\log N - N} \ .
\]</span> This will usually be the form of Stirling’s approximation that we use in practice.</p>
</section>
</section>
<section id="information-theory" class="level2">
<h2 class="anchored" data-anchor-id="information-theory">Information Theory</h2>
<section id="information-and-entropy" class="level3">
<h3 class="anchored" data-anchor-id="information-and-entropy">Information and Entropy</h3>
<p>We can think about probabilities in a completely different sense by thinking about the <em>information content</em> contained in a system and how uncertain we are about what that information content is. Suppose we wanted to transmit a message containing <span class="math inline">\(N\)</span> characters, where each character is sampled from some alphabet <span class="math inline">\(\Sigma\)</span> containing <span class="math inline">\(M\)</span> total characters. We’d like to ask the following question: How many bits of information does a <em>typical</em> message of <span class="math inline">\(N\)</span> characters from this alphabet contain?</p>
<p>Suppose we had no information at all about how often any one particular character <span class="math inline">\(x_m \in \Sigma\)</span> occurs in a message. In this case, we’d have to assume that all messages of length <span class="math inline">\(N\)</span> are typical. Since there are <span class="math inline">\(M^N\)</span> possible messages of length <span class="math inline">\(N\)</span>, we’d say there are <span class="math inline">\(g = M^N\)</span> typical messages. Since <span class="math inline">\(g\)</span> contains <span class="math inline">\(\log_2 g\)</span> bits of information, this means a typical message would contain <span class="math inline">\(\log g = N \log_2 M\)</span> bits of information.</p>
<p>Suppose now that we had an estimate of the frequency <span class="math inline">\(p_m\)</span> that each character <span class="math inline">\(x_m \in \Sigma\)</span> occurs in a message. That is, in a message of length <span class="math inline">\(N\)</span>, we expect each character <span class="math inline">\(x_m\)</span> to occur <span class="math inline">\(N_m \approx Np_m\)</span> total times, or to be more precise <span class="math inline">\(N_m = Np_m + O\big(\sqrt{N}\big)\)</span> since each character is a Bernoulli random variable, hence a message of length <span class="math inline">\(N\)</span> is a binomial random variable. In this case, the number of <em>typical</em> messages is just the number of ways of placing <span class="math inline">\(N\)</span> random characters into <span class="math inline">\(M\)</span> bins of sizes <span class="math inline">\(N_1, N_2, \cdots, N_M\)</span>, which is <span class="math display">\[
g = \frac{N!}{\prod_{m=1}^M N_m!}.
\]</span> The total number of bits contained in a typical message would then be <span class="math inline">\(\log_2 g\)</span>. If we assume the message length <span class="math inline">\(N\)</span> is large compared to the alphabet size <span class="math inline">\(M\)</span>, then we can apply the Stirling approximation to each term containing a factorial. Using the fact <span class="math inline">\(N_m = Np_m\)</span> and <span class="math inline">\(\sum N_m = N\)</span>, we have <span class="math display">\[
\begin{align*}
\log_2 g &amp;= \log_2 N! - \sum_{m=1}^M \log_2 N_m! \\
&amp;\approx \big(N\log_2 N - N\big) - \sum_{m=1}^M \big(N_m \log_2 N_m - N_m \big) \\
&amp;\approx N \log_2 N - \sum N_m \log_2 N_m \\
&amp;\approx - N \sum_{m=1}^M p_m \log_2 p_m.
\end{align*}
\]</span> The term <span class="math inline">\(-\sum p_m \log_2 p_m\)</span> is just a function of the underlying probability distribution of characters, not of the message length <span class="math inline">\(N\)</span> itself. It captures our uncertainty or surprise in what message we’d receive. We call this term the <strong>information entropy</strong> or <strong>Shannon entropy</strong>. Since the choice of base for the logarithm merely adds a constant to this sum, in physics we more typically use the <em>natural logarithm</em> instead of the base-2 logarithm, which expresses entropy in <em>nats</em> instead of <em>bits</em>. In this form, the information entropy can be defined as <span class="math display">\[
\boxed{S \equiv -\sum_{m=1}^M p_m \log p_m} \ .
\]</span> We’ve thus answered the question sought: a typical message of length <span class="math inline">\(N\)</span> contains about <span class="math inline">\(\log_2 g \approx NS\)</span> bits of information, up to an additive constant that depends on the base of logarithm. Notice that if we <em>knew</em> exactly which message to expect, that would mean <span class="math inline">\(g = 1\)</span>, which means <span class="math inline">\(S = 0\)</span>. Since we already know the most number of messages possible is <span class="math inline">\(g=M^N\)</span>, the information entropy must evidently satisfy <span class="math display">\[
0 \leq S \leq N \log M.
\]</span> In thermodynamics, the information entropy corresponds to the <strong>mixing entropy</strong> up to a factor of Boltzmann’s constant <span class="math inline">\(k_B\)</span>. One implication of this is that while information entropy is dimensionless, thermodynamic entropy has units, namely units of <span class="math inline">\(k_B\)</span>, which is energy per degree.</p>
<p>The terms <span class="math inline">\(I_m \equiv -\log p_m\)</span> capture the <strong>information content</strong> contained in any one particular character <span class="math inline">\(x_m\)</span>. If <span class="math inline">\(p_m \approx 0\)</span> then <span class="math inline">\(I_m \approx \infty\)</span>, meaning <span class="math inline">\(x_m\)</span> contains an infinite number of bits of new information relative to what we already know. If <span class="math inline">\(p_m \approx 1\)</span> then <span class="math inline">\(I_m \approx 0\)</span>, meaning <span class="math inline">\(x_m\)</span> contains no new bits of information. We can thus also think of the entropy as the <em>expected information content</em> of a message, since <span class="math display">\[
S = -\sum_{m=1}^M p_m \log p_m = -\langle \log p \rangle = \langle I \rangle.
\]</span> While information theory was built around the idea of transmitting messages, there’s nothing inherently limiting these ideas to messages alone. We can apply the concept of entropy as defined to <em>any</em> discrete probability distribution, where each <span class="math inline">\(x_m\)</span> corresponds to some value taken on by a random variable.</p>
<p>What about continuous distributions though? We can try to extend entropy to these as well, but we have to be careful. Since density functions needn’t be positive, the entropy will no longer in general be positive either, meaning it doesn’t make sense to think about it as a direct measure of information content. Nevertheless, we could <em>define</em> the information entropy of a continuous distribution as <span class="math display">\[
\boxed{S \equiv -\int_{\mathbb{R}} dx \ p(x) \log p(x)} \ .
\]</span> From a physical perspective, a more troublesome problem with this definition is that in general <span class="math inline">\(dx\)</span> will have units, which means <span class="math inline">\(p(x)\)</span> will have units as well. But we can’t have a function with units inside a logarithm. The <em>right</em> way to deal with this will be to convert <span class="math inline">\(dx\)</span> to some kind of dimensionless measure so that <span class="math inline">\(p(x)\)</span> will also be dimensionless. For example, in statistical mechanics we’ll usually be looking at distributions over phase space, where the integration measure is <span class="math inline">\(d\Gamma \propto d^3 x d^3 p\)</span>. In this case, we’d need to divide by a constant that has units of <span class="math inline">\([xp]\)</span>. We’ll see from quantum mechanics that the correct constant is in fact Planck’s constant <span class="math inline">\(h\)</span>. That is, the right integration measure is <span class="math display">\[
d\Gamma = \frac{d^3 x d^3 p}{h^3}.
\]</span> ### The Principle of Maximum Entropy</p>
<p>We can use the idea of information entropy to finally answer the question regarding what the correct way is to define probabilities subjectively or theoretically. The idea is to use the <em>principle of maximum entropy</em>.</p>
<p><strong>Principle of Maximum Entropy:</strong> The unbiased assignment of probability is the one that maximizes the information entropy subject to known constraints. More formally, assign a probability distribution <span class="math inline">\(p(x)\)</span> that maximizes the constrained problem <span class="math display">\[
\boxed{
\begin{align*}
&amp;\max_{p(x)} S[p(x)] =  \max_{p(x)} \bigg(- \sum_{x \in \mathcal{S}} p(x) \log p(x) \bigg) \\
&amp;\text{subject to} \ \sum_{x \in \mathcal{S}} p(x) = 1 \ \text{and} \ g(x) = 0 \\
\end{align*}
} \ .
\]</span> where <span class="math inline">\(g(x) = 0\)</span> is any set of known constraints on the probability distribution. The first constraint that probabilities sum to one will always be there so that <span class="math inline">\(p(x)\)</span> yields a valid probability function.</p>
<p>This is in essence just a generalization of the <em>principle of indifference</em>. If we don’t have any information to go on, we should assume all outcomes have equal probability. The principle of maximum entropy extends this idea to general distributions where we might know some information, like what its mean or variance is, or what range it’s bounded to.</p>
<p>Using Lagrange multipliers, the principle of maximum entropy is equivalent to maximizing the Lagrange multiplier function <span class="math display">\[
L(p(x),\lambda) \equiv - \sum_{x \in \mathcal{S}} p(x) \log p(x) - \alpha \bigg(\sum_{x \in \mathcal{S}} p(x) - 1\bigg) - \beta \cdot g(x).
\]</span> subject to <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p><strong>Example:</strong> Let’s formally prove the principle of indifference using the principle of maximum entropy. That is, in the absence of no known information, the unbiased probabilities to assign are the ones where each outcome has an equal probability to occur. Suppose the random variable is discrete with <span class="math inline">\(n\)</span> outcomes of probabilities <span class="math inline">\(p_1, p_2, \cdots, p_n\)</span>. In this case, the problem to solve is <span class="math display">\[
\begin{align*}
&amp;\max_{p} \bigg(- \sum_{i=1}^n p_i \log p_i \bigg) \\
&amp;\text{subject to} \ \sum_{i=1}^n p_i = 1.
\end{align*}
\]</span> This is equivalent to maximizing the Lagrange multiplier function <span class="math display">\[
L(p,\alpha) \equiv - \sum_{i=1}^n p_i \log p_i - \alpha \bigg(\sum_{i=1}^n p_i - 1\bigg).
\]</span> Differentiating with respect to each <span class="math inline">\(p_j\)</span> and <span class="math inline">\(\alpha\)</span> and setting the derivatives to zero, we have <span class="math display">\[
\begin{align*}
\frac{\partial L}{\partial p_j} &amp;= -\log p_j - 1 - \alpha \equiv 0\\
\frac{\partial L}{\partial \alpha} &amp;= \sum_{i=1}^n p_i - 1 \equiv 0\\
\end{align*}
\]</span> Solving this system of equations implies that <span class="math display">\[
\sum_{j=1}^n p_j = \sum_{j=1}^n e^{-(1+\alpha)} = 1 \quad \Longrightarrow \quad e^{1+\alpha} = n.
\]</span> Thus, the maximum entropy probabilities are just <span class="math inline">\(p_j = \frac{1}{n}\)</span> for all <span class="math inline">\(j\)</span>, as expected.</p>
<p>The same method can be used in the continuous case as well by replacing the sums <span class="math inline">\(\sum_{i=1}^n p_i\)</span> by integrals <span class="math inline">\(\int_a^b dx \ p(x)\)</span>. The main subtlety to be aware of in the continuous case is that we’re no longer maximizing a function of <span class="math inline">\(n\)</span> probabilities, but a <em>functional</em> of the form <span class="math inline">\(S[p(x)]\)</span> over all possible functions <span class="math inline">\(p(x)\)</span>. These can be solved for <span class="math inline">\(p(x)\)</span> by finding the choice of <span class="math inline">\(p(x)\)</span> that extremizes the functional <span class="math inline">\(S[p(x)]\)</span>. In that case, the maximum entropy probabilities will turn out to be <span class="math inline">\(p(x) = \frac{1}{b-a}\)</span> as expected.</p>
<p><strong>Example:</strong> Here’s an interesting example involving continuous distributions. Suppose we knew that a random variable <span class="math inline">\(x\)</span> on the real line had a given mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What is the maximum entropy distribution <span class="math inline">\(p(x)\)</span> such that these two cumulants are known? The problem to solve is now <span class="math display">\[
\begin{align*}
&amp;\max_{p(x)} \bigg(- \int_\mathbb{R} dx \ p(x) \log p(x) \bigg) \\
&amp;\text{subject to} \ \int_\mathbb{R} dx \ p(x) = 1, \\
&amp;\text{and} \int_\mathbb{R} dx \ x p(x) = \mu, \ \ \int_\mathbb{R} dx \ x^2 p(x) - \mu^2 = \sigma^2. \\
\end{align*}
\]</span> The Lagrange multiplier function is then <span class="math display">\[
\begin{align*}
L(p(x),\alpha,\beta,\gamma) = -&amp;\bigg(\int_\mathbb{R} dx \ p(x) \log p(x) \bigg) - \alpha\bigg(\int_\mathbb{R} dx \ p(x) - 1\bigg) \\
- \beta&amp;\bigg(\int_\mathbb{R} dx \ x p(x) - \mu\bigg) - \gamma \bigg(\int_\mathbb{R} dx \ x^2 p(x) - \mu^2 - \sigma^2\bigg).
\end{align*}
\]</span> To maximize this function, consider a functional perturbation <span class="math inline">\(p + \delta p\)</span>. Notice every term is linear in <span class="math inline">\(p\)</span> except the first term, which is <span class="math inline">\(p \log p\)</span>. In that term, we have <span class="math display">\[
\begin{align*}
(p + \delta p) \log (p + \delta p) &amp;= (p + \delta p) \log(1 + \frac{\delta p}{p}) \log p \\
&amp;= (p + \delta p) \bigg(1 + \frac{\delta p}{p}\bigg) \log p \\
&amp;= p \log p + \delta p (\log p + 1) + O(\delta p^2).
\end{align*}
\]</span> If we ignore terms of order higher than <span class="math inline">\(\delta p\)</span>, then solving for <span class="math inline">\(\delta L\)</span> and setting it to zero gives <span class="math display">\[
\begin{align*}
\delta L &amp;= L(p + \delta p,\alpha,\beta,\gamma) - L(p,\alpha,\beta,\gamma) \\
&amp;= -\int_\mathbb{R} dx \delta p \ \bigg[\log p + 1 + \alpha + \beta x + \gamma(x^2 - 2\mu x) \bigg] \equiv 0.
\end{align*}
\]</span> Since this must be true for any perturbation <span class="math inline">\(\delta p\)</span>, the integrand must be zero, <span class="math display">\[
\log p + 1 + \alpha + \beta x + \gamma(x^2 - 2\mu x) = 0.
\]</span> Solving then for <span class="math inline">\(p(x)\)</span> we have <span class="math display">\[
p(x) = \exp\big(-1 - \alpha - \beta x - \gamma(x^2 - 2\mu x)\big).
\]</span> The exponent is just a quadratic function of <span class="math inline">\(x\)</span>, hence we can rewrite <span class="math inline">\(p(x)\)</span> in terms of new constants as <span class="math display">\[
p(x) = \mathcal{N} \exp\bigg(-\frac{(x-a\mu)^2}{2b\sigma^2}\bigg).
\]</span> Since this has the form of a Gaussian, integrating over the real line gives a normalization constant of the form <span class="math inline">\(\mathcal{N} = (2\pi b \sigma^2)^{-1/2}\)</span>. Similarly, by shift invariance, integrating the mean function requires that <span class="math inline">\(a=1\)</span>. Last, the variance constraint requires that <span class="math inline">\(b=1\)</span>. We’ve thus shown that the continuous probability distribution whose mean and variance are known must be a Gaussian distribution.</p>
<p><strong>Example:</strong> Let’s do one more example that’s very relevant to statistical mechanics. Suppose we have a discrete random variable <span class="math inline">\(x\)</span> that can take on a possibly countably infinite number of values. Suppose we know that some positive function <span class="math inline">\(E(x) \geq 0\)</span> of <span class="math inline">\(x\)</span> has expectation <span class="math inline">\(\langle E(x) \rangle = E\)</span>. Then the problem to solve is <span class="math display">\[
\begin{align*}
&amp;\max_p \bigg(- \sum_{i=0}^\infty p_i \log p_i \bigg) \\
&amp;\text{subject to} \ \sum_{i=0}^\infty p_i = 1, \ \text{and} \ \sum_{i=0}^\infty p_i E_i = E. \\
\end{align*}
\]</span> The Lagrange multiplier function is then given by <span class="math display">\[
L(p,\alpha) \equiv - \sum_{i=0}^\infty p_i \log p_i - \alpha \bigg(\sum_{i=0}^\infty p_i - 1\bigg) - \beta \bigg(\sum_{i=0}^\infty p_i E_i - E\bigg).
\]</span> Differentiating with respect to each <span class="math inline">\(p_j\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\beta\)</span> and setting all the derivatives to zero, we have <span class="math display">\[
\begin{align*}
\frac{\partial L}{\partial p_j} &amp;= -\log p_j - 1 - \alpha - \beta E_i \equiv 0, \\
\frac{\partial L}{\partial \alpha} &amp;= \sum_{i=0}^\infty p_i - 1 \equiv 0, \\
\frac{\partial L}{\partial \alpha} &amp;= \sum_{i=0}^\infty p_i E_i - E \equiv 0. \\
\end{align*}
\]</span> Together, these imply that the probabilities must have the form <span class="math display">\[
p_j = e^{-(1+\alpha)} e^{-\beta E_j}.
\]</span> Again, the factor <span class="math inline">\(e^{-(1+\alpha)}\)</span> is just a normalization constant. If we redefine it to be <span class="math inline">\(\frac{1}{Z}\)</span>, then we finally have <span class="math display">\[
p_j = \frac{1}{Z} e^{-\beta E_j},
\]</span> where by normalization <span class="math inline">\(Z\)</span> must satisfy the relation <span class="math display">\[
Z = \sum_{i=0}^\infty e^{-\beta E_i}.
\]</span> More generally, we could imagine <span class="math inline">\(\mathbf{E}(x)\)</span> being a vector-valued function, in which case the same results apply just be replacing the scalars <span class="math inline">\(\beta E_i\)</span> with vectors <span class="math inline">\(\boldsymbol{\beta} \cdot \mathbf{E}_i\)</span>. We’ll see later that the normalization constant <span class="math inline">\(Z\)</span> is very important to statistical mechanics. It’s called the <strong>partition function</strong>. In that case, <span class="math inline">\(E_j\)</span> represents the energy of the system in the <span class="math inline">\(j\)</span><sup>th</sup> state and <span class="math inline">\(E\)</span> represents the average internal energy of the system, i.e.&nbsp;the energy that satisfies the first law of thermodynamics.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../statistical-mechanics/thermodynamics.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Thermodynamics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../statistical-mechanics/kinetic-theory.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Kinetic Theory</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>