[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Notes",
    "section": "",
    "text": "Preface\nThis page contains notes I’ve taken over time for several different subjects of interest. Currently these subjects include\n\nClassical Mechanics\nElectrodynamics\nCircuit Analysis\nQuantum Mechanics\nStatistical Mechanics\n\nFeel free to use whatever you find helpful."
  },
  {
    "objectID": "classical-mechanics/newtonian-mechanics.html#point-particles",
    "href": "classical-mechanics/newtonian-mechanics.html#point-particles",
    "title": "Newtonian Mechanics",
    "section": "Point Particles",
    "text": "Point Particles\nIn nature, an object is made of matter. It can be composed of many different molecules arranged in intricate and complicated ways. Further, each molecule is itself made of atoms, and each atom is itself made up of subatomic particles. Trying to model the motion of an object would be extremely cumbersome if we insisted on modeling the dynamics of each subatomic particle.\nInstead, it’s convenient to make abstractions. The most convenient abstraction to make is that we can describe the global behavior of an object as if it were a point object with no width. It can’t spin or deform. It’s one indivisible thing. We call these point particles.\nWe’ll think of a point particle as following some trajectory in the 3-dimensional Euclidean space \\(\\mathbb{R}^{3}\\). The trajectory or position is a time-dependent vector\n\\[\n\\mathbf{x}(t) = x(t)\\mathbf{e}_x + y(t)\\mathbf{e}_y + z(t)\\mathbf{e}_z.\n\\] A moving particle also has associated to it a velocity vector given by\n\\[\n\\mathbf{v} = \\mathbf{\\dot x} = \\frac{d\\mathbf{x}}{dt}.\n\\] Perhaps the most fundamental goal of classical mechanics is to find these two vectors as a function of time. In the Newtonian formulation, if we want to find a particle’s trajectory, we start with the particle’s acceleration vector \\[\n\\mathbf{a} = \\mathbf{\\dot v} = \\mathbf{\\ddot x} = \\frac{d^2\\mathbf{x}}{dt^2},\n\\] and match it with the force vector \\(\\mathbf{F}\\) via Newton’s Second Law to get a second-order differential equation for \\(\\mathbf{x}(t)\\)."
  },
  {
    "objectID": "classical-mechanics/newtonian-mechanics.html#newtons-laws",
    "href": "classical-mechanics/newtonian-mechanics.html#newtons-laws",
    "title": "Newtonian Mechanics",
    "section": "Newton’s Laws",
    "text": "Newton’s Laws\nNewton’s Laws efficiently encapsulate the fundamental physics of classical mechanics. They’re stated below specifically for a point particle, or body, but can be extended to more complex systems as well.\n\nA body remains at rest, or in motion at a constant speed in a straight line, unless acted upon by a force. That is,\n\\[\n\\mathbf{F} = \\mathbf{0} \\Rightarrow \\mathbf{v}=const.\n\\]\nWhen a body is acted upon by a force, the time rate of change of its acceleration is proportional to the force. That is, \\[\n\\mathbf{F} = m \\mathbf{a}.\n\\]\nIf two bodies exert forces on each other, these forces have the same magnitude but opposite directions. That is, \\[\n\\mathbf{F}_{12} = \\mathbf{F}_{21}.\n\\]\n\n\n\n\n\n\nForces are vectors, which means they obey the superposition principle, and can be analyzed in components. Position, velocity, and acceleration are vectors as well. The proportionality constant between \\(\\mathbf{F}\\) and \\(\\mathbf{a}\\) is called the mass \\(m\\). Loosely speaking, the mass of an object is a measure of its inertia or resistance to motion.\nThe functional form of the forces themselves depend on the particular type of forces applied. Some common forces are:\n\nGravitational Force: \\(\\mathbf{F} = -\\frac{GMm}{r^2} \\mathbf{e}_r\\)\nCoulomb Force: \\(\\mathbf{F} = k_e \\frac{Qq}{r^2} \\mathbf{e}_r\\)\nHarmonic Oscillator: \\(\\mathbf{F} = -k\\mathbf{x}\\)\nLorentz Force: \\(\\mathbf{F} = q\\mathbf{E} + \\frac{q}{c}\\mathbf{v} \\times \\mathbf{B}\\)\nThrust: \\(\\mathbf{F} = - |\\mathbf{v}_{ex}| \\dot m \\mathbf{e}_v\\)\n\n\n\n\n\nNormal Forces: \\(\\mathbf{F} = \\mathbf{N}\\)\nTension Forces: \\(\\mathbf{F} = \\mathbf{T}\\)\nFrictional Forces: \\(\\mathbf{F} = -\\mu |\\mathbf{N}| \\mathbf{e}_v\\)\nDrag Forces: \\(\\mathbf{F} = -f(\\mathbf{v}) \\mathbf{e}_v \\approx -a\\mathbf{v} -b|\\mathbf{v}|^2\\mathbf{e}_v\\)\nCentrifugal Forces: \\(\\mathbf{F} = m\\boldsymbol{\\omega} \\times (\\mathbf{x} \\times \\boldsymbol{\\omega})\\)\nCoriolis Forces: \\(\\mathbf{F} = 2m \\mathbf{v} \\times \\boldsymbol{\\omega}\\)\nBuoyant Forces: \\(\\mathbf{F} = - \\rho_{liq} V_{sub} \\mathbf{g}\\)"
  },
  {
    "objectID": "classical-mechanics/newtonian-mechanics.html#conservation-laws",
    "href": "classical-mechanics/newtonian-mechanics.html#conservation-laws",
    "title": "Newtonian Mechanics",
    "section": "Conservation Laws",
    "text": "Conservation Laws\nA quantity Q is said to be conserved if its time derivative is zero, \\(\\dot Q = 0\\). That is, Q is conserved it it’s constant in time.\n\nMomentum\nFor an object moving at velocity \\(\\mathbf{v}\\), define its linear momentum \\(\\mathbf{p}\\) by\n\\[\n\\mathbf{p} = m \\mathbf{v}.\n\\] If the mass \\(m\\) is constant, we evidently have \\[\n\\mathbf{F} = \\mathbf{\\dot p}.\n\\] If \\(\\mathbf{F} = \\mathbf{0}\\), then \\(\\mathbf{p}=const\\), hence momentum is conserved if there are no forces applied. This is the conservation of momentum.\n\n\nAngular Momentum\nDefine the angular momentum \\(\\mathbf{L}\\) of an object by \\[\n\\mathbf{L} = \\mathbf{x} \\times \\mathbf{p}.\n\\] Similarly, define the torque or moment \\(\\mathbf{N}\\) by \\[\\mathbf{N} = \\mathbf{x} \\times \\mathbf{F}.\\]\nNote both angular momentum and torque depend on the choice of coordinate system used since the position vector \\(\\mathbf{x}\\) depends on choice of origin. Now, observe that \\[\n\\mathbf{\\dot L} = \\mathbf{\\dot x} \\times \\mathbf{p} + \\mathbf{x} \\times \\mathbf{\\dot p} = m \\mathbf{v} \\times \\mathbf{v} + \\mathbf{x} \\times \\mathbf{F} = \\mathbf{N}.\n\\] Thus, \\(\\mathbf{N} = \\mathbf{\\dot L}\\). If \\(\\mathbf{N} = \\mathbf{0}\\), then \\(\\mathbf{L}=const\\), hence angular momentum must be conserved if there are no torques applied. This is the conservation of angular momentum.\n\n\nWork and Energy\nDefine the work done on an object as it moves along a path \\(\\gamma\\) from \\(A\\) to \\(B\\) by\n\\[\nW = \\int_A^B \\mathbf{F} \\cdot d\\mathbf{x}.\n\\]\n\n\n\n\n\nIn general, work depends on the path taken to get from \\(A\\) to \\(B\\), hence it isn’t a unique property of the system.\nObserve that \\[\ndW = \\mathbf{F} \\cdot d\\mathbf{x} = \\mathbf{F} \\cdot \\mathbf{v} dt = d\\bigg(\\frac{1}{2}m\\mathbf{v}^2 \\bigg).\n\\] Define the kinetic energy of the system by \\(T = \\frac{1}{2} m \\mathbf{v}^2\\). Then we evidently have \\(dW=dT\\). That is, the work done on the system to get from \\(A\\) to \\(B\\) via \\(\\gamma\\) is just the change in kinetic energy between \\(A\\) and \\(B\\), \\[\nW = \\Delta T = T_B - T_A.\n\\] When the work done is independent of the path taken it’s a state function of the kinetic energy. In this case, the force \\(\\mathbf{F}\\) is said to be conservative.\nBy the Helmholtz theorem, the following conditions are all equivalent:\n\n\\(\\mathbf{F}\\) is conservative,\n\\(W\\) is path-independent,\n\\(\\nabla \\times \\mathbf{F} = \\mathbf{0}\\),\nThere is a scalar potential \\(V=V(\\mathbf{x})\\) such that \\(\\mathbf{F} = -\\nabla V\\).\n\nThe scalar potential \\(V\\) is called the potential energy of the system. Evidently, if \\(\\mathbf{F}\\) is conservative, we have \\[\nW = \\int_A^B \\mathbf{F} \\cdot d\\mathbf{x} = -\\int_A^B \\nabla V \\cdot d\\mathbf{x} = -\\int_A^B dV = V_A - V_B = -\\Delta V = \\Delta T.\n\\] That is, \\(\\Delta T + \\Delta V = 0\\). Define the total mechanical energy \\(E\\) of the system by \\[\nE = T + V.\n\\] Then \\(\\Delta E = \\Delta (T + V) = 0\\). That is, energy is conserved when the forces on the system are conservative. This is the conservation of energy.\nEnergy isn’t generally conserved if the forces aren’t conservative. Examples of non-conservative forces include any force that’s a function of velocity. These include dissipative forces like friction or drag, as well as magnetic forces."
  },
  {
    "objectID": "classical-mechanics/newtonian-mechanics.html#using-newtons-laws",
    "href": "classical-mechanics/newtonian-mechanics.html#using-newtons-laws",
    "title": "Newtonian Mechanics",
    "section": "Using Newton’s Laws",
    "text": "Using Newton’s Laws\nThe primary goal of mechanics is to understand how systems evolve with time. To understand a particle’s given trajectory in Newtonian Mechanics, we need to\n\nWrite down all the forces acting on the particle,\nUse \\(\\mathbf{F} = m \\mathbf{a}\\) to set up the equations of motion,\nSolve the equations of motion for the trajectory \\(\\mathbf{x}(t)\\), either analytically or (usually) numerically.\n\nHere are some examples.\n\n\nExample: Projectile motion\nSuppose a cannon is launched from the origin at an angle \\(\\theta\\) above the ground with initial velocity \\(\\mathbf{v}_0\\).\n\n\n\n\n\n\nWrite down the equations of motion. Assume drag is negligible.\nThe forces are \\(\\mathbf{F} = \\mathbf{g} = -g\\mathbf{e}_y\\). Then, \\[\n\\mathbf{a} = \\ddot x\\mathbf{e}_x + \\ddot y\\mathbf{e}_y = -g\\mathbf{e}_y \\quad \\Longrightarrow \\quad   \\ddot x = 0, \\ \\ \\ddot y = -mg.\n\\]\nFind the trajectory \\(\\mathbf{x}(t) = x(t)\\mathbf{e}_x + y(t)\\mathbf{e}_y\\).\nIntegrating each element twice gives \\[\n\\begin{align*}\nx(t) &= x_0 + v_{0x}t = v_0t \\cos \\theta, \\\\\ny(t) &= y_0 + v_{0y}t - \\frac{1}{2} gt^2 = v_{0}t\\sin \\theta - \\frac{1}{2} gt^2.\n\\end{align*}\n\\]\nFind the range, i.e. the value \\(R=x(T)\\) when the cannon hits the ground. Which launch angle maximizes the range?\nFirst, we need to find the time \\(T\\) when \\(y(T) = 0\\). Setting \\[\ny(T) = 0 = v_0 T \\sin \\theta - \\frac{1}{2} gT^2 \\Longrightarrow T = 0, \\frac{2v_0 \\sin \\theta}{g}.\n\\] The \\(T=0\\) case is trivial. Plugging the other one in to \\(x(T)\\) finally gives the range, \\[\nR = x(T) = v_0T \\cos \\theta = \\frac{2v_0^2 \\sin \\theta}{g} \\cos \\theta = \\frac{v_0^2 \\sin 2\\theta}{g}.\n\\] Note that the range is maximized when \\(\\sin 2 \\theta = 1\\), which is when the launch angle is \\(\\theta = 45^\\circ\\).\nFind the shape of the motion \\(y = y(x)\\).\nWe need to eliminate \\(t\\) in both equations and solve for \\(y=y(x(t))\\). Solving \\(x(t)\\) for \\(t\\) gives, \\[\nx = v_0 t\\cos \\theta \\Longrightarrow t = \\frac{x}{v_0 \\cos \\theta}.\n\\] Plugging this into \\(y\\) then gives \\[\ny = v_{0}\\frac{x}{v_0 \\cos \\theta}\\sin \\theta - \\frac{1}{2} g\\bigg(\\frac{x}{v_0 \\cos \\theta}\\bigg)^2 =  \\tan \\theta \\cdot x - \\frac{g}{2v_0^2 \\cos^2 \\theta} x^2.\n\\] This is a downward sloping parabola with vertex at \\(\\big(\\frac{v_0^2 \\sin 2\\theta}{2g}, \\frac{v_0^2 \\sin^2 \\theta}{g}\\big)\\).\nFind any conserved quantities.\n\nMomentum: Since \\(\\mathbf{F} \\neq \\mathbf{0}\\), momentum isn’t conserved. However, \\(p_x\\) is conserved.\nAngular Momentum: Since \\(\\mathbf{N} = \\mathbf{x} \\times \\mathbf{F} = \\mathbf{x} \\times m\\mathbf{g} \\neq 0\\), angular momentum is not conserved.\nEnergy: Since \\(V=mgy\\), the force \\(\\mathbf{F}\\) is conservative, hence energy is conserved.\n\n\n\n\n\nExample: Block sliding on a ramp with friction\nA block of mass \\(m\\) is sliding down a ramp inclined from the horizontal at an angle \\(\\theta\\). Assume the system has a coefficient of friction \\(\\mu\\), and that the block starts from rest at the top of the ramp.\n\n\n\n\n\n\nWrite down the equations of motion.\nChoose a coordinate system such that \\(x\\) is pointing downwards parallel to the ramp and \\(y\\) is pointing outwards perpendicular to the ramp. There are three forces acting, gravity, the normal force, and the frictional force, so \\[\n\\mathbf{F} = \\mathbf{N} + m\\mathbf{g} - \\mu \\mathbf{N} \\mathbf{e}_v = N\\mathbf{e}_y + mg(\\sin\\theta\\mathbf{e}_x - \\cos\\theta\\mathbf{e}_y) - \\mu N \\mathbf{e}_x.\n\\] Resolving into components, we have \\[\n\\begin{align*}\nm \\ddot x &= mg\\sin\\theta - \\mu N, \\\\\nm \\ddot y &= N - mg\\cos\\theta = 0.\n\\end{align*}\n\\] The second equation follows from the assumption that the block is constrained to stay on the ramp.\nFind the trajectory \\(\\mathbf{x}(t) = x(t)\\mathbf{e}_x + y(t)\\mathbf{e}_y\\).\nTo solve, we need to eliminate the normal force \\(N\\). Using the EOM for \\(\\ddot y\\), we get \\(N = mg\\cos\\theta\\). Plugging this into the equation for \\(\\ddot x\\) then gives\n\\[\n\\begin{align*}\n\\ddot x &= g(\\sin\\theta - \\mu\\cos\\theta) = const, \\\\\n\\ddot y &= 0.\n\\end{align*}\n\\] Suppose the block starts at the top of the ramp, which we’ll call the origin. Then integrating, we get,\n\\[\n\\begin{align*}\nx(t) &= v_0 t + \\frac{1}{2}g(\\sin\\theta - \\mu\\cos\\theta)t^2, \\\\\ny(t) &= 0.\n\\end{align*}\n\\] Notice \\(x(t)\\) is just the equation of an object falling under a modified gravity \\[\n\\mathbf{g}'=-g(\\sin\\theta - \\mu\\cos\\theta)\\mathbf{e}_x.\n\\]\nFind the angle \\(\\theta\\) at which the block will start sliding.\nThe block will move if \\(\\ddot x \\geq 0\\), i.e. when \\(\\mu \\leq \\tan\\theta\\). It will start moving at the angle when \\(\\tan\\theta=\\mu\\) exactly, i.e. when \\[\n\\theta = \\arctan\\mu.\n\\]\nFind any conserved quantities.\n\nMomentum: Since \\(\\mathbf{F} \\neq \\mathbf{0}\\), momentum is not conserved. However, \\(p_y\\) is conserved.\nAngular momentum: Since \\(\\mathbf{N} = \\mathbf{x} \\times \\mathbf{F} \\neq \\mathbf{0}\\), angular momentum is not conserved.\nEnergy: Since friction is present, \\(\\mathbf{F}\\) is a dissipative force, hence it’s not conservative, and energy is not conserved.\n\nFind the rate of energy dissipation as the block slides down the ramp.\nFriction dissipates as a heat \\(Q\\). If the block slides a distance \\(L\\), this means \\(E(0) = E(L) + Q\\). Since the block starts from rest, \\(E(0) = 0\\). At \\(x=L\\), the work done is \\[\nW = \\int_0^L F_x dx = \\int_0^L mg(\\sin\\theta - \\mu\\cos\\theta)dx = mgL(\\sin\\theta - \\mu\\cos\\theta) = T(L) - 0 = T(L),\n\\] so the energy when the block gets to the bottom is \\[\nE(L) = T(L) + V(L,0) = mgL(\\sin\\theta - \\mu\\cos\\theta) - mgL\\sin\\theta = -\\mu mgL\\cos\\theta.\n\\] Finally, using this to solve for \\(Q\\), the heat dissipated over the entire trajectory, we get \\[\nQ = E(0) - E(L) = \\mu mgL\\cos\\theta.\n\\] The most important sanity check here is to notice there’s no heat dissipation if there is no friction."
  },
  {
    "objectID": "classical-mechanics/newtonian-mechanics.html#curvilinear-coordinates",
    "href": "classical-mechanics/newtonian-mechanics.html#curvilinear-coordinates",
    "title": "Newtonian Mechanics",
    "section": "Curvilinear Coordinates",
    "text": "Curvilinear Coordinates\nFor many problems, it’s more convenient to take advantage of the underlying symmetry by using special coordinate systems. Other than rectangular coordinates \\((x,y,z)\\), the most common coordinate systems worth being familiar with are polar coordinates \\((r,\\varphi)\\), cylindrical coordinates \\((\\rho,\\varphi,z)\\), and spherical coordinates \\((r,\\theta,\\varphi)\\).\n\nPolar Coordinates\nFor problems with circular symmetry it’s convenient to use polar coordinates \\((r,\\varphi)\\), defined by\n\\[\n\\begin{align*}\nx &=  r\\cos\\varphi, \\\\\ny &=  r\\sin\\varphi. \\\\\n\\end{align*}\n\\] where \\(r \\geq 0\\) and \\(0 \\leq \\varphi \\leq 2\\pi\\). We can assign basis vectors to polar coordinates \\(\\mathbf{e}_r, \\mathbf{e}_\\varphi\\) to each point as usual.\n\n\n\n\n\nThe thing to keep in mind is that these curvilinear basis vectors are now functions of position,\n\\[\n\\begin{align*}\n\\mathbf{e}_r &= \\mathbf{e}_r(r, \\varphi), \\\\\n\\mathbf{e}_\\varphi &= \\mathbf{e}_\\varphi(r, \\varphi).\n\\end{align*}\n\\] We can figure out how these basis vectors change by taking their differentials, which follow from the figure above,\n\\[\n\\begin{align*}\nd\\mathbf{e}_r &= \\mathbf{e}_\\varphi d\\varphi, \\\\\nd\\mathbf{e}_\\varphi &= -\\mathbf{e}_r d\\varphi.\n\\end{align*}\n\\] Using these differential forms, we can conclude that the motion vectors change as follows,\n\\[\n\\begin{align*}\n\\mathbf{x} &= r\\mathbf{e}_r, \\\\\n\\mathbf{v} &= \\dot r \\mathbf{e}_r + r\\dot \\varphi \\mathbf{e}_\\varphi, \\\\\n\\mathbf{a} &= (\\ddot r - r\\dot \\varphi^2)\\mathbf{e}_r + (2\\dot r \\dot \\varphi + r\\ddot \\varphi)\\mathbf{e}_\\varphi.\n\\end{align*}\n\\]\n\n\nExample: Circular orbits\nSuppose an object moves in a circular orbit of radius \\(r\\) at a constant angular velocity \\(\\omega\\) due to a central force \\(\\mathbf{F} = -F\\mathbf{e}_r\\).\n\n\n\n\n\n\nFind the equations of motion. Since the object moves at constant \\(\\omega\\), we have \\(\\dot \\varphi = \\omega = const\\). Using the polar equations for velocity and acceleration, we have \\[\n\\begin{align*}\n\\mathbf{v} &= \\dot r \\mathbf{e}_r, \\\\\n\\mathbf{a} &= -r\\omega^2\\mathbf{e}_r + r \\dot \\omega\\mathbf{e}_\\varphi = - \\frac{F}{m}\\mathbf{e}_r.\n\\end{align*}\n\\] Note we can re-write these equations to get \\(F = m\\omega^2 r\\).\nFind the period \\(\\tau\\) of the orbit.\nWe want the time it takes for \\(\\Delta \\varphi = 2\\pi\\). Since \\(\\Delta \\varphi = \\omega\\tau\\), solving for \\(\\tau\\) gives \\[\\tau = \\frac{2\\pi}{\\omega}.\\]\nSuppose the central force is the gravitational force, \\(F = \\frac{GMm}{r^2}\\). Find the angular velocity, the period, and the orbital velocity as a function of \\(G, M, r\\).\nWe have \\[\nF = \\frac{GMm}{r^2} = m\\omega^2 r \\ \\Longrightarrow \\ \\omega = \\sqrt{\\frac{GM}{r^3}} \\ \\Longrightarrow \\ \\tau = \\frac{2\\pi}{\\sqrt{GM}} r^{3/2}.\n\\] This is just a special case of Kepler’s third law, \\(\\tau^2 \\propto r^3\\). The orbital velocity is given by \\[\nv = r\\omega = \\sqrt{\\frac{GM}{r}}.\n\\]\n\n\n\n\nExample: Simple pendulum\nConsider the problem of the simple pendulum, where a mass \\(m\\) swings on a massless string of length \\(\\ell\\) under the force of gravity. The string is fixed at one point. Assume no damping is present.\n\n\n\n\n\n\nFind the equations of motion from the forces directly.\nThere are two forces in this problem, gravity and the tension in the string, \\[\n\\mathbf{F} = \\mathbf{T} + m\\mathbf{g} = -T\\mathbf{e}_r + mg(\\cos\\theta \\mathbf{e}_r - \\sin\\theta\\mathbf{e}_\\theta).\n\\] Dividing by \\(m\\) and setting equal to the polar form of \\(\\mathbf{a}\\), we have \\[\n\\mathbf{a} = (-T+mg\\cos\\theta)\\mathbf{e}_r - mg\\sin\\theta\\mathbf{e}_\\theta = -m\\ell^2 \\dot \\theta^2 \\mathbf{e}_r + m\\ell^2 \\ddot \\theta \\mathbf{e}_\\theta.\n\\] This gives two equations of motion, one for the tension and one for the angular acceleration, \\[\n\\begin{align*}\nT &= m\\ell^2 \\dot\\theta^2 + mg\\cos\\theta, \\\\\n\\ddot \\theta &= -\\frac{g}{\\ell} \\sin\\theta. \\\\\n\\end{align*}\n\\]\nFind the equations of motion again, but this time using torques.\nRecall \\(\\mathbf{N} = I \\boldsymbol{\\dot \\omega}\\), where \\(I\\) is the scalar moment of inertia and \\(\\boldsymbol{\\omega}\\) is the angular velocity vector. In this case, \\(I=m\\ell^2\\) and \\(\\boldsymbol{\\dot \\omega} = \\ddot \\theta \\mathbf{e}_z\\). Then we have \\[\nI \\boldsymbol{\\dot \\omega} = m\\ell^2 \\ddot \\theta \\mathbf{e}_z \\equiv \\ell\\mathbf{e}_r \\times m\\mathbf{g} = -mg\\ell\\sin\\theta \\mathbf{e}_z = \\mathbf{N},\n\\] which can be solve to get \\(\\ddot \\theta = -\\frac{g}{\\ell}\\sin\\theta\\). Notice how in this approach we don’t need to worry about the tension at all.\nSuppose \\(\\theta\\) is small. Write down the equations of motion, solve them, and find the period.\nWhen \\(\\theta \\ll 1\\) the small angle approximation applies, \\(\\sin\\theta \\approx \\theta\\). In this case, the equation of motion reduces to \\[\\ddot \\theta = -\\frac{g}{\\ell} \\theta,\\] which is just simple harmonic oscillation with angular frequency \\(\\omega = \\sqrt{\\frac{g}{\\ell}}\\). The solution to SHO is \\[\n\\theta(t) = A\\sin(\\omega t + \\phi),\n\\] where \\(A\\) is some amplitude and \\(\\phi\\) is some phase determined by the initial conditions. Finally, solving for the period, we have \\[\n\\tau = \\frac{2\\pi}{\\omega} = 2\\pi\\sqrt{\\frac{\\ell}{g}}.\n\\]\n\n\n\n\n\nCylindrical Coordinates\nCylindrical coordinates extend polar coodinates by adding in the z-axis from the rectangular system,\n\\[\n\\begin{align*}\nx &=  r\\cos\\varphi, \\\\\ny &=  r\\sin\\varphi, \\\\\nz &= z.\n\\end{align*}\n\\] The basis vectors are \\(\\mathbf{e}_r, \\mathbf{e}_\\varphi, \\mathbf{e}_z\\). Their differential forms are just \\[\n\\begin{align*}\nd\\mathbf{e}_r &= \\mathbf{e}_\\varphi d\\varphi, \\\\\nd\\mathbf{e}_\\varphi &= -\\mathbf{e}_r d\\varphi \\\\\nd\\mathbf{e}_z &= 0.\n\\end{align*}\n\\] The motion vectors in cylindrical coordinates are thus given by, \\[\n\\begin{align*}\n\\mathbf{x} &= r\\mathbf{e}_r, \\\\\n\\mathbf{v} &= \\dot r \\mathbf{e}_r + r\\dot \\varphi \\mathbf{e}_\\varphi + \\dot z \\mathbf{e}_z , \\\\\n\\mathbf{a} &= (\\ddot r - r\\dot \\varphi^2)\\mathbf{e}_r + (2\\dot r \\dot \\varphi + r\\ddot \\varphi)\\mathbf{e}_\\varphi + \\ddot z \\mathbf{e}_z.\n\\end{align*}\n\\]\n\n\nSpherical Coordinates\nSpherical coordinates extend polar coordinates in a slightly different way. The radius \\(r\\) is now 3-dimensional, and there are two angles, a polar angle \\(0 \\leq \\theta \\leq \\pi\\) and an azimuthal angle \\(0 \\leq \\varphi \\leq 2\\pi\\). The conversion to rectangular coordinates is given by, \\[\n\\begin{align*}\nx &=  r\\sin\\theta\\cos\\varphi, \\\\\ny &=  r\\sin\\theta\\sin\\varphi, \\\\\nz &= r\\cos\\theta. \\\\\n\\end{align*}\n\\] The basis vectors are \\(\\mathbf{e}_r, \\mathbf{e}_\\theta, \\mathbf{e}_\\varphi\\). Deriving the differential forms of these is a good bit more complex. Here they are, \\[\n\\begin{aligned}\nd\\mathbf{e}_r &= \\dot\\theta \\sin\\varphi d\\mathbf{e}_\\theta + \\dot\\varphi d\\mathbf{e}_\\varphi, \\\\\nd\\mathbf{e}_\\theta &= - \\dot\\theta \\sin\\varphi d\\mathbf{e}_r - \\dot\\theta \\cos\\varphi d\\mathbf{e}_\\varphi, \\\\\nd\\mathbf{e}_\\varphi &= - \\dot\\varphi \\mathbf{e}_r + \\dot\\theta \\cos\\varphi \\mathbf{e}_\\theta. \\\\\n\\end{aligned}\n\\] These can then be used to get the motion vectors in spherical coordinates, \\[\n\\begin{align*}\n\\mathbf{r} &= r \\mathbf{e}_r, \\\\\n\\mathbf{v} &= \\dot{r} \\mathbf{e}_r + r \\dot\\theta \\sin\\varphi \\mathbf{e}_{\\theta} + r \\dot\\varphi \\mathbf{e}_{\\varphi}, \\\\\n\\mathbf{a} &= (\\ddot{r} - r \\dot{\\theta}^2 \\sin^2\\varphi - r \\dot{\\varphi}^2) \\mathbf{e}_r \\\\\n&\\quad + (r \\ddot\\theta \\sin\\varphi + 2 \\dot{r} \\dot\\theta \\sin\\varphi + 2 r \\dot\\theta \\dot\\varphi \\cos\\varphi) \\mathbf{e}_{\\theta} \\\\\n&\\quad + (r \\ddot\\varphi + 2 \\dot{r} \\dot\\varphi - r \\dot{\\theta}^2 \\sin\\varphi \\cos\\varphi) \\mathbf{e}_{\\varphi}. \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "classical-mechanics/newtonian-mechanics.html#many-particle-systems",
    "href": "classical-mechanics/newtonian-mechanics.html#many-particle-systems",
    "title": "Newtonian Mechanics",
    "section": "Many-Particle Systems",
    "text": "Many-Particle Systems\nThus far we’ve worked with single-particle systems. Let’s now consider a system of \\(N\\) particles with positions \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N\\) respectively. We can use the principle of superposition to extend the laws derived above for single particles.\nFor \\(N\\)-particle systems it’s convenient to characterize the system’s position using the center of mass vector \\(\\mathbf{R}\\), \\[\n\\mathbf{R} \\equiv \\frac{1}{M}\\sum_{i=1}^N m_i \\mathbf{x}_i,\n\\] where \\(M\\) is just the total mass of the system, \\(M \\equiv \\sum m_i\\). The center of mass is just the mass-weighted average of all the particle position vectors.\nSuppose an external force \\(\\mathbf{F}^{ext}\\) is acting on the system, and suppose each particle \\(i\\) imparts a force \\(\\mathbf{F}_{ij}\\) on particle \\(j \\neq i\\). Here’s what this would look like for \\(N=3\\) particles.\n\n\n\n\n\nBy superposition, the total force acting on the entire system is thus, \\[\n\\mathbf{F} = \\mathbf{F}^{ext} + \\sum_{i \\neq j} \\mathbf{F}_{ij} = \\sum m_i \\mathbf{a}_i = M\\mathbf{\\ddot R}.\n\\] Now, by Newton’s third law, \\(\\mathbf{F}_{ij} = -\\mathbf{F}_{ji}\\). This means all the internal forces cancel in pairs, so we have \\[\n\\mathbf{F}^{ext} = M\\mathbf{\\ddot R}.\n\\] That is, the system as a whole moves as if it were a point mass \\(M\\) with an external force \\(\\mathbf{F}^{ext}\\) acting on its center of mass \\(\\mathbf{R}\\).\nIf the total momentum is defined as \\(\\mathbf{P} = M \\mathbf{\\dot R}\\), this expression then says \\(\\mathbf{F}^{ext} = \\mathbf{\\dot P}\\). Thus, if no external forces act on the system, then its total linear momentum \\(\\mathbf{P}\\) is conserved.\nLet’s now consider the total torques on the system. Suppose the system experiences an external torque \\(\\mathbf{N}^{ext}\\), and that each particle \\(i\\) exerts a torque \\(\\mathbf{N}_{ij}\\) on particle \\(j\\). Then by superposition, the total torque on the system is \\[\n\\mathbf{N} = \\mathbf{N}^{ext} + \\sum_{i \\neq j} \\mathbf{N}_{ij} = \\mathbf{N}^{ext} + \\sum_{i \\neq j} \\mathbf{x}_i \\times \\mathbf{F}_{ij},\n\\] Again, we can use the fact that each \\(\\mathbf{F}_{ij} = -\\mathbf{F}_{ji}\\). If we do this, we can re-write the total torque as\n\\[\n\\mathbf{N} = \\mathbf{N}^{ext} + \\sum_{i<j} (\\mathbf{x}_{i}-\\mathbf{x}_{j}) \\times \\mathbf{F}_{ij} = \\mathbf{N}^{ext}.\n\\] Now, if we further assume that each internal force acts centrally, i.e. \\(\\mathbf{F}_{ij} = \\mathbf{F}_{ij}(\\mathbf{x}_{i}-\\mathbf{x}_{j})\\), then the internal cross products all vanish, and we just get \\(\\mathbf{N} = \\mathbf{N}^{ext}\\). That is, if all the internal forces are central, then the total torque on the system is just the external torque.\nIf the total angular momentum on the system is defined as \\(\\mathbf{L} = \\mathbf{R} \\times \\mathbf{P}\\), this expression says \\(\\mathbf{\\dot L} = \\mathbf{N}^{ext}\\). Thus, if no external torques act on the system, then its total angular momentum \\(\\mathbf{L}\\) is conserved.\nIt’s insightful to separate each particle’s motion vectors explicitly into a center of mass component and a relative component,\n\\[\n\\begin{align*}\n\\mathbf{x}_i &= \\mathbf{R} + \\boldsymbol{\\mathscr{r}}_i, \\\\\n\\mathbf{v}_i &= \\mathbf{V} + \\boldsymbol{\\mathscr{v}}_i. \\\\\n\\end{align*}\n\\]\n\n\n\n\n\nLet’s re-write the total angular momentum \\(\\mathbf{L}\\) in terms of these vectors,\n\\[\n\\begin{align*}\n\\mathbf{L} &= \\sum \\mathbf{x}_i \\times \\mathbf{p}_i = \\sum (\\mathbf{R} + \\boldsymbol{\\mathscr{r}}_i) \\times m_i(\\mathbf{V} + \\boldsymbol{\\mathscr{v}}_i) \\\\\n&= M\\mathbf{R} \\times \\mathbf{V} + \\sum m_i \\boldsymbol{\\mathscr{r}}_i \\times \\boldsymbol{\\mathscr{v}}_i + \\mathbf{R} \\times \\bigg(\\sum m_i \\boldsymbol{\\mathscr{v}}_i \\bigg) + \\bigg(\\sum m_i \\boldsymbol{\\mathscr{r}}_i \\bigg)\\times \\mathbf{V} \\\\\n&= \\mathbf{R} \\times \\mathbf{P} + \\sum m_i \\boldsymbol{\\mathscr{r}}_i \\times \\boldsymbol{\\mathscr{v}}_i \\\\\n&\\equiv \\mathbf{L}^{orb} + \\mathbf{L}^{spin}. \\\\\n\\end{align*}\n\\] We’ve thus been able to separate the angular momentum into two components, an orbital angular momentum \\(\\mathbf{L}^{orb} = \\mathbf{R} \\times \\mathbf{P}\\), and a spin angular momentum \\(\\mathbf{L}^{spin} = \\sum m_i \\boldsymbol{\\mathscr{r}}_i \\times \\boldsymbol{\\mathscr{v}}_i\\). The orbital angular momentum describes how the center of mass of the object tends to rotate about some external point. The spin angular momentum describes how the system itself tends to rotate about its center of mass.\n\n\n\n\n\nLast, let’s look at the total energies of the system. For a system with \\(N\\) particles, the potential energy will be a function of all the position vectors, \\(V = V(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N)\\). It won’t generally simplify. But the kinetic energy we can simplify. Writing it in terms of its relative and center of mass velocities, we have\n\\[\n\\begin{align*}\nT &= \\frac{1}{2}\\sum m_i \\mathbf{v}_i^2 = \\sum m_i (\\mathbf{V} + \\boldsymbol{\\mathscr{v}}_i)^2 \\\\\n&= \\frac{1}{2}M\\mathbf{V}^2 + \\frac{1}{2}\\sum m_i\\boldsymbol{\\mathscr{v}}_i^2 + \\mathbf{V} \\cdot \\bigg(\\sum m_i \\boldsymbol{\\mathscr{v}}_i\\bigg) \\\\\n&= \\frac{1}{2}M\\mathbf{V}^2 + \\frac{1}{2}\\sum m_i\\boldsymbol{\\mathscr{v}}_i^2 \\\\\n&= T^{CM} + T^{rel}.\n\\end{align*}\n\\] Thus, the kinetic energy separates into a sum of the kinetic energy on the center of mass \\(T^{CM} = \\frac{1}{2}M\\mathbf{V}^2\\), and the kinetic energy of the relative components \\(T^{rel} = \\frac{1}{2}\\sum m_i\\boldsymbol{\\mathscr{v}}_i^2\\). Evidently, the total energy is \\[\nE = T + V = T^{CM} + T^{rel} + V(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N).\n\\] It’s conserved provided the external force \\(\\mathbf{F}^{ext}\\) is conservative.\n\n\nExample: Rockets\nSuppose a rocket of mass \\(m_0=m(t) + m_{ex}(t)\\) is moving through free space with no external forces acting on it. It’s expelling fuel for trust at some constant speed \\(v_{ex}\\) with respect to the rocket.\n\n\n\n\n\n\nWhat is the force of thrust on the rocket?\nNo external forces are present, so \\(\\mathbf{F}^{ext} = \\mathbf{0}\\). The internal forces are the thrust of the rocket, and the force of the exhaust. In the frame of the rocket they cancel out, \\(\\mathbf{F}_{th} = \\mathbf{F}_{ex}\\), so we have \\[\n\\mathbf{F}_{th} = -\\mathbf{F}_{ex} = -\\mathbf{\\dot p}_{ex} = -\\frac{d}{dt}(m_{ex} \\mathbf{v}_{ex}) = -\\dot m_{ex} \\mathbf{v}_{ex}.\n\\] Now, since \\(m_0 = m + m_{ex}\\), \\(\\dot m = -\\dot m_{ex}\\), and \\(\\mathbf{v}_{ex} = -v_{ex}\\mathbf{e}_v\\), we have \\[\n\\mathbf{F}_{th} = -\\dot m v_{ex} \\mathbf{e}_v.\n\\]\nFind the velocity \\(\\mathbf{v}(t)\\) of the rocket.\nUsing the fact that \\(\\mathbf{F}_{th} = m\\mathbf{a}\\), we have \\(-\\dot m v_{ex} = m \\dot v\\), a first-order differential equation in \\(v(t)\\), \\[\n\\dot v + v_{ex} \\frac{\\dot m}{m} = 0.\n\\] Integrating both sides and solving for \\(v(t)\\), we get \\[\nv(t) = v_0 - v_{ex} \\int_{m_0}^m \\frac{dm}{m} = -v_{ex} \\log \\frac{m(t)}{m_0}.\n\\] Or, expressing in the form of the well-known rocket equation, \\[\n\\Delta v = v_{ex} \\log\\frac{m_0}{m(t)}.\n\\]\nFind the position \\(\\mathbf{x}(t)\\) of the rocket, assuming fuel is expelled form the rocket at a constant rate.\nAssume \\(\\dot m = -k = const\\). Since there are no external forces, the rocket must be traveling along some line. Suppose without loss of generality then that \\(\\mathbf{x}(t) = z(t)\\mathbf{e}_z\\). Then we have, \\[\n\\begin{align*}\nz(t) &= \\int_0^t v(t) dt = v_{ex} \\int_0^t dt \\log\\frac{m_0}{m_0-kt} \\\\\n&= v_{ex} \\bigg[t - \\bigg(\\frac{m_0-kt}{k} \\bigg) \\log \\bigg(\\frac{m_0}{m_0-kt} \\bigg) \\bigg] \\\\\n&= v_{ex} t - \\frac{v_{ex}}{k}(m_0 - kt)\\log\\bigg(\\frac{m_0}{m_0-kt} \\bigg).\n\\end{align*}\n\\]"
  },
  {
    "objectID": "classical-mechanics/simple-systems.html#independent-forces",
    "href": "classical-mechanics/simple-systems.html#independent-forces",
    "title": "Simple Systems",
    "section": "Independent Forces",
    "text": "Independent Forces\nThe first and simplest case we’ll consider are forces that don’t depend on position or velocity, \\[\nm \\mathbf{a} = \\mathbf{F}_0(t).\n\\] We can solve these systems directly by integrating both sides, i.e. reducing to quadrature. We have,\n\\[\n\\begin{align*}\n\\mathbf{a}(t) &= \\frac{1}{m}\\mathbf{F}_0(t), \\\\\n\\mathbf{v}(t) &= \\mathbf{v}_0 + \\frac{1}{m}\\int_0^t dt'\\mathbf{F}_0(t'), \\\\\n\\mathbf{x}(t) &= \\mathbf{x}_0 + \\mathbf{v}_0t + \\frac{1}{m}\\int_0^t dt' \\int_0^{t'} dt''\\mathbf{F}_0(t''). \\\\\n\\end{align*}\n\\] The simplest of these cases are when there are no forces at all, and when the forces are constant. If there are no forces at all acting on the system, \\(\\mathbf{F}_0 = \\mathbf{0}\\), in which case the equations of motion reduce to\n\\[\n\\begin{align*}\n\\mathbf{a}(t) &= 0, \\\\\n\\mathbf{v}(t) &= \\mathbf{v}_0, \\\\\n\\mathbf{x}(t) &= \\mathbf{x}_0 + \\mathbf{v}_0t. \\\\\n\\end{align*}\n\\] This is just a statement of Newton’s First Law. If no forces act on a particle, it continues linearly along its path at constant velocity. The next simplest case is when \\(\\mathbf{F}_0=const\\). In this case, the equations of motion become\n\\[\n\\begin{align*}\n\\mathbf{a}(t) &= \\frac{1}{m}\\mathbf{F}_0, \\\\\n\\mathbf{v}(t) &= \\mathbf{v}_0 + \\frac{1}{m}\\mathbf{F}_0 t, \\\\\n\\mathbf{x}(t) &= \\mathbf{x}_0 + \\mathbf{v}_0t + \\frac{1}{2m^2}\\mathbf{F}_0^2. \\\\\n\\end{align*}\n\\] This case includes the gravitional force near the surface of the Earth, in which case \\(\\mathbf{F}_0=m\\mathbf{g}\\). It also includes the problem of an electric charge placed close to a large conducting sheet with a uniform electric field, where \\(\\mathbf{F}_0=q\\mathbf{E}_0\\).\nIn these problems, the motion will always be along a parabolic arc. The parabola will slope toward the force if the force is attractive, and away from the force if it’s repulsive.\n\n\nExample: Free-fall near Earth\nSuppose an object of mass \\(m\\) is falling freely near the Earth’s surface. In this case, \\(\\mathbf{F}_0 = m\\mathbf{g}\\), so\n\\[\n\\begin{align*}\n\\mathbf{a}(t) &= \\mathbf{g}, \\\\\n\\mathbf{v}(t) &= \\mathbf{v}_0 - \\mathbf{g}t, \\\\\n\\mathbf{x}(t) &= \\mathbf{x}_0 + \\mathbf{v}_0t + \\frac{1}{2}\\mathbf{g}t^2.\n\\end{align*}\n\\] The motion in this case will always lie in the plane spanned by \\(\\mathbf{v}_0\\) and \\(\\mathbf{g}\\). This means without loss of generality we can assume motion lies in the xy-plane with \\(\\mathbf{g} = -g\\mathbf{e}_y\\). Then \\(y\\) can be solved as a function of \\(x\\) to give \\[\ny(x) = v_{0}\\frac{x}{v_0 \\cos \\theta}\\sin \\theta - \\frac{1}{2} g\\bigg(\\frac{x}{v_0 \\cos \\theta}\\bigg)^2,\n\\] which is of course a downward-sloping parabola centered at the vertex \\(\\big(\\frac{v_0^2 \\sin 2\\theta}{2g}, \\frac{v_0^2 \\sin^2 \\theta}{g}\\big)\\)."
  },
  {
    "objectID": "classical-mechanics/simple-systems.html#drag-forces",
    "href": "classical-mechanics/simple-systems.html#drag-forces",
    "title": "Simple Systems",
    "section": "Drag Forces",
    "text": "Drag Forces\nThe next type of forces we’ll consider are those which are functions of velocity,\n\\[\nm\\mathbf{a} = \\mathbf{F}(\\mathbf{v}).\n\\] In the 1-dimensional case, this reduces to,\n\\[\nma = F(v).\n\\] Provided \\(v\\) is small, we can approximate \\(F(v)\\) by its first few terms. I’ll write it as,\n\\[\nF(v) \\approx a - bv - cv^2.\n\\] Typically, drag forces shouldn’t apply a force when the particle is at rest, which means \\(a=0\\). The remaining two terms cover two distinct regimes of drag:\n\nLinear or viscous drag: \\(F(v) = -bv\\), where \\(b > 0\\).\nQuadratic or air drag: \\(F(v) = -cv^2\\), where \\(c > 0\\).\n\n\nLinear Drag\nIt’s convenient to analyze these two distinct cases separately. Let’s first look at linear drag. In that case \\(c=0\\), and we end up with the linear differential equation \\[\nm \\ddot x + b \\dot x = 0.\n\\] To solve this equation, re-write it in terms of \\(v = \\dot x\\),\n\\[\n\\frac{dv}{dt} = -\\frac{b}{m} v.\n\\] Integrating both sides, we get\n\\[\nv(t) = v_0 e^{-\\frac{b}{m} t}.\n\\] For \\(x(t)\\) just integrate both sides again to get\n\\[\nx(t) = x_0 + \\int_0^t v_0 e^{-\\frac{b}{m} t'} dt' = x_0 + \\frac{mv_0}{b}\\big(1 - e^{-\\frac{b}{m} t}\\big).\n\\] Evidently, such forces cause a moving particle to slowly come to rest, since \\(v \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). The position where the particle comes to rest is evidently \\(x_f = x_0 + \\frac{mv_0}{b}\\). The \\(\\frac{1}{e}\\) decay time is \\(\\tau = \\frac{m}{b}\\). This suggests that \\(b\\) functions as a sort of drag coefficient, since a large \\(b\\) causes the system to dissipate faster.\n\n\n\n\n\nLinear drag is frequently used to model objects moving through a viscous medium at low speeds. Suppose a spherical object of radius \\(R\\) is moving slowly in a viscous medium with viscosity \\(\\eta\\). Then the drag force on the object is given by Stokes’ Law,\n\\[\n\\mathbf{F}_d = -6\\pi\\eta R \\mathbf{v}.\n\\] This force is linear in velocity, hence we can write \\(F_d = -6\\pi\\eta R v\\), which says the drag constant \\(b\\) is just\n\\[\nb = 6\\pi\\eta R.\n\\]\n\n\nExample: Dropping a ball in syrup\nSuppose a ball of radius \\(R\\) and mass \\(m\\) is dropped in a viscous syrup from rest at \\(x=0\\). Find the velocity and position of the ball as it moves through the fluid.\n\n\n\n\n\nThis is a 1-dimensional motion problem since the ball is dropped from rest under gravity, with \\(F=F_d + mg\\). Here Stoke’s law applies, so the drag force is \\(F_d = -bv = -b\\dot x\\). Plugging into Newton’s Second Law, we have \\[\nm\\ddot x + b \\dot x = g.\n\\] Re-writing this in terms of \\(v = \\dot x\\), we get \\[\nm \\dot v + bv = g,\n\\] which is a first order linear differential equation for the velocity \\(v(t)\\). Its general solution is given by \\[\nv(t) = v_0 e^{-\\frac{b}{m}t} + \\frac{mg}{b}(1-e^{-\\frac{b}{m}t}).\n\\] Notice that as \\(t \\rightarrow \\infty\\), \\(v(t) \\rightarrow \\frac{mg}{b}\\). That is, \\(v(t)\\) tends toward a terminal velocity \\[\nv_t = \\frac{mg}{b} = \\frac{mg}{6\\pi\\eta R}.\n\\] Since the ball is dropped from rest, \\(v_0=0\\). The velocity of the ball is thus given by \\[\nv(t) = v_t(1-e^{-\\frac{b}{m}t}).\n\\] Using this we can solve for the position to get \\[\nx(t) = v_t\\bigg(t - \\frac{b}{m}(1 - e^{-\\frac{b}{m}t})\\bigg).\n\\] Notice that drag causes the ball to fall much slower than it would in free-fall. Instead of being a quadratic function of time, \\(x(t)\\) is now approximately a linear function of time, with \\(x(t) \\sim v_t t\\) for large \\(t\\).\n\n\n\n\n\n\n\n\n\nQuadratic Drag\nWe’ll now look at quadratic drag, where \\(b=0\\). Then we get the differential equation, \\[\nm\\ddot x + c \\dot x^2 = 0.\n\\] This is no longer a linear differential equation due to the appearance of \\(\\dot x^2\\), but surprisingly we can still solve it using separation of variables. Again, let \\(v = \\dot x\\). Then we get \\[\nm\\dot v + cv^2 = 0.\n\\] Rearranging and solving for \\(v(t)\\), we have \\[\n\\frac{dv}{dt} = -\\frac{c}{m}v^2 \\quad \\Longrightarrow \\quad\n\\int_{v_0}^{v} \\frac{dv}{v^2} = -\\frac{c}{m} t \\quad \\Longrightarrow \\quad\nv(t) = \\frac{1}{\\frac{1}{v_0} + \\frac{c}{m}t}.\n\\] Integrating both sides and solving for the position, we get \\[\nx(t) = x_0 + \\int_0^t \\frac{dt}{\\frac{1}{v_0} + \\frac{c}{m}t} = x_0 + \\frac{m}{c}\\log\\bigg( 1 + \\frac{cv_0}{m}t \\bigg).\n\\] In this case, \\(v \\rightarrow 0\\), but \\(x \\rightarrow \\infty\\) as \\(t \\rightarrow \\infty\\). Evidently, while linear drag is strong enough to slow a moving particle back down to rest, quadratic drag is not.\n\n\n\n\n\nQuadratic drag is often used to model the drag experienced by objects moving through air or other media where pressure is more important than viscosity. For an object moving through air, drag is well-modeled by the drag equation, \\[\n\\mathbf{F}_d = -\\frac{1}{2}C \\rho A v^2 \\mathbf{e}_v,\n\\] where \\(\\rho\\) is the density of air, \\(A\\) is the cross-sectional area of the object in the direction of motion, and \\(C\\) is the drag coefficient. Since this force is proportional to \\(v^2\\), we evidently have \\[\nc = \\frac{1}{2}C \\rho A.\n\\]\n\n\nReynold’s Number\nIn practice, how can we tell if drag is in the linear or quadratic situation? A simple way to do this is by looking at the Reynold’s Number. Let’s go back to the full quadratic equation for drag, with \\(a\\) set to \\(0\\), \\[\nF_d = -bv - cv^2.\n\\] Notice that the ratio \\(\\frac{cv}{b}\\) gives the relative importance of the two drag terms. Using Stoke’s Law and the Drag Equation for the drag constants, we can re-write this expression as \\[\n\\frac{cv}{b} = \\frac{\\frac{1}{2}C \\rho Av}{6\\pi\\eta R} = \\frac{C \\rho Rv}{3\\eta}.\n\\] This ratio is usually rescaled by a factor of \\(\\frac{3}{C}\\) to get the Reynold’s number \\(r\\), \\[\nr = \\frac{\\rho Rv}{\\eta}.\n\\] The Reynold’s number is usually what’s used in practice to decide whether we’re in the linear or quadratic drag regime.\n\nWhen the Reynold’s number is low, \\(r \\ll 1\\), \\(v \\ll \\frac{\\eta}{R\\rho}\\), and we’re in the linear regime.\nWhen the Reynold’s number is high, \\(r \\gg 1\\), \\(v \\gg \\frac{\\eta}{R\\rho}\\), and we’re in the quadratic regime.\nThe edge case is when \\(r \\approx 1\\), or \\(v \\approx \\frac{\\eta}{R\\rho}\\). Then, we have to include both the linear and quadratic drag terms in the equation of motion. In this general case, there’s no analytic solution and we have to solve things numerically.\n\nThe Reynold’s number is usually easy to calculate since we can often at least roughly estimate the object’s velocity and radius, and we can usually look up the medium’s viscosity and density. For example, a baseball thrown in air at 100 mph would have a Reynold’s number of about \\(r \\approx 3 \\cdot 10^5 \\gg 1\\), which is solidly in the quadratic drag regime."
  },
  {
    "objectID": "classical-mechanics/simple-systems.html#harmonic-oscillation",
    "href": "classical-mechanics/simple-systems.html#harmonic-oscillation",
    "title": "Simple Systems",
    "section": "Harmonic Oscillation",
    "text": "Harmonic Oscillation\nThe next case we’ll consider is when the force is linear in position, \\[\n\\mathbf{F} = -k \\mathbf{x}.\n\\] This relationship is called Hooke’s Law. In the 1-dimensional case, it reduces to the equation of motion \\[\nm \\ddot x + kx = 0.\n\\] This is a second-order linear differential equation for \\(x(t)\\). The general solution to this differential equation depends on the sign of \\(k\\). If \\(k < 0\\), we have \\[\nx(t) = c_1 e^{\\frac{k}{m}t} + c_2 e^{-\\frac{k}{m}t}.\n\\] Since \\(x \\rightarrow \\infty\\) pretty quickly as \\(t \\rightarrow \\infty\\), this kind of solution is usually non-physical, except perhaps in situations where \\(x\\) is bounded between some known range.\nThe most important case by far is when \\(k > 0\\). In this setting, it’s typical to define \\(\\omega^2 \\equiv \\frac{k}{m}\\) and re-rewrite the equation as \\[\n\\ddot x + \\omega^2 x = 0.\n\\] This is called the simple harmonic oscillator or SHO. The canonical example of SHO is of course the motion of a mass attached to an ideal spring with spring constant \\(k\\).\nThe general solution to SHO is a linear combination of sine and cosine functions, \\[\nx(t) = c_1 \\cos \\omega t + c_2 \\sin \\omega t.\n\\] This trajectory is oscillatory and stable since it only involves sines and cosines, both of which are bounded periodic functions. It’s custom to re-write this equation in a more useful form using trig identities,\n\n\n\n\n\n\\[\n\\begin{align*}\nx(t) &= c_1 \\cos \\omega t + c_2 \\sin \\omega t \\\\\n&= A\\bigg(\\frac{c_1}{A}\\cos \\omega t + \\frac{c_2}{A}\\sin\\omega t \\bigg) \\\\\n&= A(\\cos\\delta \\cos \\omega t + \\sin\\delta\\sin\\omega t) \\\\\n&= A\\cos(\\omega t - \\delta). \\\\\n\\end{align*}\n\\] In this form, \\(A\\) is the amplitude of oscillation and \\(\\delta\\) is the phase of oscillation. The period of oscillation is given by \\[\n\\tau = \\frac{2\\pi}{\\omega} = 2\\pi\\sqrt{\\frac{m}{k}}.\n\\]\n\n\n\n\n\nIt’s usually convenient when dealing with harmonic oscillators to work in the complex plane. Consider the complex form of SHO, given by the differential equation \\[\n\\ddot z + \\omega^2 z = 0,\n\\] where \\(z = x+iy = |z|e^{i\\theta}\\) is a complex variable. Its general solution is given as a linear combination of complex exponentials, \\[\nz(t) = \\tilde c_1 e^{i\\omega t} + \\tilde c_2 e^{-i\\omega t}.\n\\] If we demand that the real solution we seek be given by \\(x(t) = \\text{Re}(z(t))\\), then\n\\[\n\\begin{align*}\nx(t) &= \\Re(c_1 e^{i \\omega t}) + \\Re(c_2 e^{-i \\omega t}) \\\\\n&= \\frac{1}{2}(c_1 + c_2^*)e^{i \\omega t} + \\frac{1}{2}(c_1^* + c_2)e^{-i \\omega t} \\\\\n&= \\frac{1}{2} C e^{i \\omega t} + \\frac{1}{2} C^* e^{-i \\omega t} \\\\\n&= A \\cdot \\Re(e^{i(\\omega t - \\delta)}) \\\\\n&= A \\cos(\\omega t - \\delta),\n\\end{align*}\n\\] where \\(C \\equiv Ae^{i \\delta}\\) is some complex number whose real and imaginary parts are \\(c_1+c_2^*\\) and \\(c_1^*+c_2\\) respectively. For the full complex solution we can similarly write \\[\nz(t) = A e^{i(\\omega t - \\delta)}\n\\] Evidently then, SHO is just a CCW circular rotation in the complex plane with radius \\(A\\).\n\n\n\n\n\n\n\nExample: Bottle sloshing in a bucket\nSuppose a bottle of mass \\(m\\) floats calmly in a bucket of water of density \\(\\rho\\) at some equilibrium depth of \\(d=d_0\\). Suppose we push down slightly on the bottle, perturbing its depth to \\(d = d_0 + x\\). The bottle will begin to oscillate. Find its period of oscillation \\(\\tau\\).\n\n\n\n\n\nThe forces on the bottle are gravity downward and an opposing buoyant force upward, \\[\nF = mg - \\rho g V_{sub} = mg - \\rho g A(d_0 + x).\n\\] At equilibrium, the forces must balance, so \\(0 = mg - \\rho g A d_0\\), which means \\(d_0 = \\frac{m}{\\rho A}\\) is the equilibrium depth. Simplifying, this says the equation of motion is given by \\[\nm \\ddot x = mg - \\rho g A(d_0 + x) = -\\rho g A x = -\\frac{mg}{d_0} x.\n\\] This is just SHO with spring constant \\(k = \\frac{mg}{d_0}\\), or angular frequency \\(\\omega = \\frac{g}{d}\\). Thus, the period of the bottle’s oscillation when \\(x\\) is small is given by \\[\n\\tau = \\frac{2 \\pi}{\\omega} = 2\\pi\\sqrt{\\frac{d_0}{g}}.\n\\]\n\n\n\nTwo-Dimensional Harmonic Oscillation\nSuppose now we allow a mass to move in two dimensions. Hooke’s Law becomes\n\\[\n\\begin{align*}\nm \\ddot x &= -k_x x, \\\\\nm \\ddot y &= -k_y y.\n\\end{align*}\n\\] Since the equation of motions are uncoupled, the solutions are simply given by\n\\[\n\\begin{align*}\nx(t) &= A_x \\cos(\\omega_x t - \\delta_x), \\\\\ny(t) &= A_y \\cos(\\omega_y t - \\delta_y).\n\\end{align*}\n\\] Despite what intuition might suggest, the motion of the mass is now quite non-trivial. In fact, the behavior of the trajectory depends entirely on the ratio of the frequencies \\(\\frac{\\omega_x}{\\omega_y}\\) and the relative phase between the two oscillations \\(\\delta = \\delta_x - \\delta_y\\).\nThe motion will only be periodic if \\(\\frac{\\omega_x}{\\omega_y}\\) is rational, i.e. if the frequencies are integer multiples of each other. The curves traced out by \\((x(t), y(t))\\) when \\(\\frac{\\omega_x}{\\omega_y}\\) is rational are called Lissajous curves. They can get quite complicated, but they’ll always be periodic. Here’s what a few of them look like for different\\(\\frac{\\omega_x}{\\omega_y}\\) and \\(\\delta\\).\n\n\n\n\n\n\n\nExample: Charged particle in a uniform magnetic field\nSuppose a particle with charge \\(q\\) and mass \\(m\\) is moving in the presence of a constant magnetic field \\(\\mathbf{B}\\). Find its equations of motion, solve for the trajectory, and describe what it looks like.\n\n\n\n\n\nIf \\(\\mathbf{v}\\) is the velocity of the particle, the magnetic force is given by \\(\\mathbf{F} = \\frac{q}{c} \\mathbf{v} \\times \\mathbf{B}\\). Suppose \\(\\mathbf{B} = B \\mathbf{e}_z\\). Then \\[\n\\mathbf{F} = \\frac{q}{c}\\mathbf{v} \\times \\mathbf{B} = \\frac{qB}{c}(\\dot y \\mathbf{e}_x - \\dot x \\mathbf{e}_y),\n\\] The equations of motion are thus\n\\[\n\\begin{align*}\nm \\ddot x &= \\frac{qB}{c} \\dot y , \\\\\nm \\ddot y &= -\\frac{qB}{c} \\dot x , \\\\\nm \\ddot z &=  0. \\\\\n\\end{align*}\n\\] Define \\(\\omega \\equiv \\frac{qB}{c}\\). The first two equations can be decoupled to give two independent SHO equations in the velocities,\n\\[\n\\begin{align*}\n\\ddot v_x &= -\\omega^2 v_x , \\\\\n\\ddot v_y &= -\\omega^2 v_y , \\\\\n\\end{align*}\n\\] with solutions\n\\[\n\\begin{align*}\nv_x(t) &= V_x \\cos(\\omega t - \\delta_x) , \\\\\nv_y(t) &= V_y \\cos(\\omega t - \\delta_y)  , \\\\\n\\end{align*}\n\\] Now, since \\(\\ddot v_x = \\omega \\dot v_y\\), we must have \\(V_x = V_y\\) and \\(\\delta_y = \\delta_x - \\frac{\\pi}{2}\\). Taking \\(\\delta_x=0\\) and \\(V_x = R\\omega\\) for convenience, we get\n\\[\n\\begin{align*}\nv_x(t) &= R\\omega \\cos(\\omega t) , \\\\\nv_y(t) &= -R\\omega \\sin(\\omega t)  , \\\\\n\\end{align*}\n\\] Finally, integrating the velocity equations gives the trajectory,\n\\[\n\\begin{align*}\nx(t) &=  x_0 + R \\sin(\\omega t), \\\\\ny(t) &= (y_0 - R) + R \\cos(\\omega t) , \\\\\nz(t) &=  z_0 + v_{0z} t. \\\\\n\\end{align*}\n\\] This is just a helix of radius \\(R\\) directed along the z-axis. That is, the particle will just spiral around in a helix directed along the line of the magnetic field. The frequency \\(\\omega\\) is called the cyclotron frequency. Since charge can be positive or negative, it carries a sign, which determines which way the particle will spiral. Notice that \\(R\\) is just the radius of orbit. It’s customarily expressed in terms of the tangential velocity \\(v_\\perp = \\sqrt{v_x^2 + v_y^2} = R\\omega\\), \\[\nR = \\frac{mcv_\\perp}{qB}.\n\\]"
  },
  {
    "objectID": "classical-mechanics/simple-systems.html#damped-harmonic-oscillation",
    "href": "classical-mechanics/simple-systems.html#damped-harmonic-oscillation",
    "title": "Simple Systems",
    "section": "Damped Harmonic Oscillation",
    "text": "Damped Harmonic Oscillation\nLet’s now combine the forces of linear drag with the forces of harmonic oscillation. In the 1-dimensional case, this gives the damped harmonic oscillator or DHO, \\[\nm \\ddot x + bx + kx = 0.\n\\] Define \\(\\beta \\equiv \\frac{b}{2m}\\) and \\(\\omega_0 \\equiv \\sqrt{\\frac{k}{m}}\\), called the damping constant and the natural frequency respectively. Then we can write the DHO equation of motion as \\[\n\\ddot x + 2\\beta x + \\omega_0^2 x = 0.\n\\] It’ll be insightful to solve this in its complex form. Consider instead the equation \\[\n\\ddot z + 2\\beta z + \\omega_0^2 z = 0,\n\\] where \\(z\\) is complex-valued. Let’s try and assume a trial solution of the form \\(z = A e^{i\\omega t - \\delta}\\). Plugging this into the differential equation, we get \\[\n(-\\omega^2 + 2\\beta + \\omega_0^2)A e^{i\\omega t - \\delta} = 0.\n\\] In the non-trivial case \\(A \\neq 0\\), this implies \\((-\\omega^2 + 2\\beta + \\omega_0^2) = 0\\), which we can solve for \\(\\omega\\) to get \\[\n\\omega = i\\beta \\pm \\sqrt{\\omega_0^2 - \\beta^2} \\equiv i\\beta \\pm \\omega',\n\\] where \\(\\omega' \\equiv \\sqrt{\\omega_0^2 - \\beta^2}\\). Plugging this into \\(z\\) then gives \\[\nz(t) = e^{-\\beta t}(c_1 e^{i\\omega' t} + c_2 e^{-i\\omega' t}).\n\\] When dealing with damped systems, it’s customary to define a quality factor \\(Q \\equiv \\frac{\\omega_0}{2\\beta}\\), which expresses in relative terms how much the system is being damped. We can re-write \\(\\omega'\\) in terms of the Q-factor as \\[\n\\omega' = \\omega_0 \\sqrt{1 - \\bigg(\\frac{1}{2Q}\\bigg)^2}.\n\\] Evidently, the form of the solutions divide into three cases depending on the sign of \\(\\omega'\\):\n\nUnderdamping (\\(\\omega' > 0\\) or \\(Q < \\frac{1}{2}\\)): In this case, \\(\\omega'\\) is real, which means we have a real solution \\[\nx(t) = A e^{-\\beta t} \\cos(\\omega't - \\delta).\n\\] This is an exponentially damped sinusoidal oscillation, where \\(x \\rightarrow 0\\) with time constant \\(\\tau = \\frac{1}{\\beta}\\). Notice \\(\\omega' < \\omega_0\\), which means the actual frequency of the oscillation is less than the natural frequency. When \\(Q \\gg 1\\) this distinction disappears, since \\(\\omega' \\approx \\omega_0\\). In practice this occurs frequently for underdamped solutions, and \\(Q\\) need not even be large for \\(\\omega' \\approx \\omega_0\\).\n\n\n\n\n\nOverdamping (\\(\\omega' < 0\\) or \\(Q > \\frac{1}{2}\\)): In this case, \\(\\omega'\\) is complex. Define \\(\\kappa \\equiv i\\omega'\\), which is real-valued. Then we have a solution of the form \\[\nx(t) = e^{-\\beta t}(c_1 e^{\\kappa t} + c_2 e^{-\\kappa t}).\n\\] Since \\(\\kappa < \\beta\\), \\(x \\rightarrow 0\\) monotonically, with time constant \\(\\tau = \\frac{1}{\\beta - \\kappa}\\).\n\n\n\n\n\nCritical damping (\\(\\omega' = 0\\) or \\(Q = \\frac{1}{2}\\)): This is the edge case where \\(\\omega_0 = \\beta\\) exactly. Here the solution is degenerate, with \\[\nx(t) = (c_1 + c_2 t) e^{-\\beta t}.\n\\] Again, \\(x \\rightarrow 0\\), but with time constant \\(\\tau = \\frac{1}{\\beta}\\). Evidently, the critically damped solution decays faster than the overdamped solution.\n\n\n\n\n\n\nIn all three cases the system must eventually come to rest due to the presence of damping. Only the “high Q” systems are allowed to oscillate. Note that as \\(\\beta \\rightarrow 0\\), \\(Q \\rightarrow \\infty\\). In this limit the solution turns into regular SHO, with \\(\\omega' \\rightarrow \\omega_0\\)."
  },
  {
    "objectID": "classical-mechanics/simple-systems.html#driven-damped-harmonic-oscillation",
    "href": "classical-mechanics/simple-systems.html#driven-damped-harmonic-oscillation",
    "title": "Simple Systems",
    "section": "Driven Damped Harmonic Oscillation",
    "text": "Driven Damped Harmonic Oscillation\nLet’s now add in the independent force term to the mix. In the 1-dimensional case, we get the equation of motion \\[\nm \\ddot x + b \\dot x + kx = F_0(t).\n\\] This is called the driven damped harmonic oscillator or DDHO. We imagine \\(F_0(t)\\) to be some external driving force acting on the system. It’s again convenient to rewrite things by defining \\(\\beta = \\frac{b}{2m}\\), \\(\\omega_0^2 = \\frac{k}{m}\\), and \\(f(t) = \\frac{F_0(t)}{m}\\). Then we have the linear differential equation \\[\n\\ddot x + 2\\beta\\dot x + \\omega_0^2 x = f(t).\n\\] Recall that the solutions of linear differential equations can be decomposed into two pieces, a homogenous solution and a particular solution. The homogenous solution is the general solution to \\[\n\\ddot x_h + 2\\beta\\dot x_h + \\omega_0^2 x_h = 0.\n\\] But this is just a DHO. We solved that part already. All we need to do now is find any particular solution that will solve \\[\n\\ddot x_p + 2\\beta\\dot x_p + \\omega_0^2 x_p = f(t).\n\\] Provided we do that, the full, general solution will be \\(x(t) = x_h(t) + x_p(t)\\). There are several ways to find a particular solution, including guessing methods and more systematic methods like Green functions and Fourier transforms. We’ll briefly touch on some of these.\n\n\nExample: Underdamped hanging spring\nSuppose a spring is suspended vertically from a ceiling under the presence of gravity. Find the position \\(x=x(t)\\). Also, find the amount that gravity changes the spring’s equilibrium length.\n\n\n\n\n\nThe forces in this problem are gravity, linear drag, and a spring force, so if \\(x\\) points downward, \\[\nF = mg - bv - kx.\n\\] The equation of motion is thus given by \\[\nm \\ddot x + b \\dot x + kx = mg.\n\\] This is just DDHO with \\(f(t) = g\\). We thus seek a particular solution \\(x_p(t)\\) such that \\[\n\\ddot x_p + 2\\beta\\dot x_p + \\omega_0^2 x_p = g.\n\\] Assume a trial solution of the form \\(x_p = c\\). Then, \\[\n\\omega_0^2 c = g \\Longrightarrow c = \\frac{g}{\\omega_0^2} = \\frac{gm}{k}.\n\\] Supposing the spring is underdamped, then \\[\nx(t) = Ae^{-\\beta t} \\cos(\\omega' t - \\delta) + \\frac{gm}{k}.\n\\] The new equilibrium occurs when the object is at rest, i.e. when \\(\\ddot x = \\dot x = 0\\). This occurs at \\(x = \\frac{gm}{k}\\) relative to the free equilibrium \\(x=0\\).\n\n\n\nSinusoidal Driving Forces\nThe most interesting driving forces in practice are ones that are periodic. Periodic driving functions lead to the important concept of resonance, which is a phenomenon that occurs when the driving frequency matches the natural frequency.\nConsider a sinusoidal driving force of the form \\(f(t) = f_0 \\cos\\omega t\\). In complex form, we can then write \\[\n\\ddot z + 2\\beta\\dot z + \\omega_0^2 z = f_0 e^{i \\omega t}.\n\\] Assume a particular solution of the form \\(z(t) = \\tilde A e^{i \\omega t}\\) where \\(\\tilde A = Ae^{-i\\delta}\\). Plugging this in, we have \\[\n(-\\omega^2 + 2\\beta\\omega i + \\omega_0^2)\\tilde A e^{-i\\omega t} = f_0 e^{i\\omega t},\n\\] which we can solve for the complex amplitude \\(\\tilde A\\) to get \\[\n\\tilde A = \\frac{f_0}{(\\omega_0^2-\\omega^2) + 2\\beta\\omega i}.\n\\] With the help of little trig, we can decompose this solution to get the real amplitude and phase,\n\n\n\n\n\n\\[\nA = \\frac{f_0}{\\sqrt{(\\omega_0^2-\\omega^2)^2 + 4\\beta^2\\omega^2}}, \\quad \\delta = \\tan^{-1} \\frac{2\\beta\\omega}{\\omega_0^2-\\omega^2}.\n\\]\nWith these, the real particular solution is given by \\(x_p(t) = A \\cos(\\omega t - \\delta)\\). Since the homogenous solution decays to zero exponentially, \\(x \\rightarrow x_p\\) as \\(t \\rightarrow \\infty\\). That is, \\(x_p(t)\\) describes the steady state solution of the DDHO. In this sense, the system evidently “forgets” its own natural frequency and starts to oscillate at the driving frequency as time goes on and the transient state dies off.\n\n\n\n\n\nInterestingly, the memory of the transient dynamics is preserved in the amplitude and phase. As a rule of thumb, the number of oscillations \\(N\\) until \\(x \\approx x_p\\) is basically just the Q-factor, \\[\nN \\approx \\frac{Q}{\\pi}.\n\\] Let’s now look more deeply at the amplitude and phase. It’s worth asking how they depend on the external driving frequency \\(\\omega\\).\n\nWhen \\(\\omega \\ll \\omega_0\\), \\(A \\approx \\frac{f_0}{\\omega_0^2}\\) and \\(\\delta \\approx 0\\).\nWhen \\(\\omega \\gg \\omega_0\\), \\(A \\approx 0\\) and \\(\\delta \\approx \\pi\\).\nWhen \\(\\omega \\approx \\omega_0\\), \\(A \\approx \\frac{f_0}{2\\beta\\omega_0}\\) and \\(\\delta \\approx \\frac{\\pi}{2}\\).\n\n\n\n\n\n\nEvidently, \\(A\\) is maximized at \\(\\omega_R = \\sqrt{\\omega_0^2-2\\beta^2} = \\omega_0 \\sqrt{1-\\frac{1}{2Q^2}}\\). When \\(Q>1\\), \\(\\omega_R \\approx \\omega_0\\), so this distinction doesn’t really matter. This frequency \\(\\omega_R \\approx \\omega_0\\) is called the resonance frequency of the system. When the driver is operating near the resonance frequency, the system responds extremely well to the driving force. It turns out that when a spectrum of frequencies is dumped on an oscillating system, the system tends to pick out the resonance frequencies and respond to those. This fact makes resonance a very important topic in physics and engineering.\nIn practice, high-Q systems are very common. In those cases, \\(A\\) dies off quickly when the driving frequencies aren’t close to \\(\\omega_0\\). That means practically all the interesting behavior of a high-Q systems is in the band around the resonance frequency. Suppose \\(\\Delta \\equiv \\omega - \\omega_0\\) is small. Then we can write \\[\n(\\omega_0^2-\\omega^2) = (\\omega_0-\\omega)(\\omega_0+\\omega) = -\\Delta(2\\omega_0+\\Delta) \\approx -2\\omega_0 \\Delta.\n\\] That means we can write the complex amplitude \\(\\tilde A\\) as \\[\n\\tilde A = \\frac{f_0}{(\\omega_0^2-\\omega^2) + 2\\beta\\omega i} \\approx -\\frac{\\frac{f_0}{2\\omega_0}}{\\Delta-\\beta i} = \\frac{f_0}{2\\omega_0}\\bigg(-\\frac{\\Delta}{\\Delta^2+\\beta^2} + i\\frac{\\beta}{\\Delta^2+\\beta^2} \\bigg).\n\\] This function on the right is called the Lorentzian. Using this, we can see \\[\nA \\approx \\frac{f_0}{2\\omega_0} \\frac{1}{\\sqrt{\\Delta^2 + \\beta^2}} \\quad \\Longrightarrow \\quad A^2 \\approx \\bigg(\\frac{f_0}{2\\omega_0}\\bigg)^2 \\frac{1}{\\Delta^2 + \\beta^2}.\n\\] Since the energy in a harmonic oscillator is just \\(E = \\frac{1}{2}kA^2 \\propto A^2\\), it’s common to look at plots of \\(A^2\\) when plotting these resonance curves. Evidently, \\(A^2 \\rightarrow 0\\) as \\(\\Delta \\rightarrow \\infty\\), and it’s maximized when \\(\\Delta = 0\\), which is when \\(\\omega = \\omega_0\\) and \\(A_{max}^2 = \\big(\\frac{f_0}{2\\omega_0 \\beta}\\big)^2\\).\nIt’s common to measure the width of the resonance curve by using the full width at half maximum or FWHM. The FWHM is defined as the difference between the left and right points around the maximum whose height is half the maximum, \\[\nFWHM \\equiv \\Delta \\omega \\equiv \\omega_R - \\omega_L, \\quad \\text{where} \\quad A^2(\\omega_L) = A^2(\\omega_R) = \\frac{1}{2}A_{max}.\n\\] Solving for these left and right points gives \\(\\omega_L = \\omega_0 - \\beta\\) and \\(\\omega_R = \\omega_0 + \\beta\\), so \\[\n\\Delta \\omega = (\\omega_0 + \\beta) - (\\omega_0 - \\beta) = 2\\beta.\n\\] The resolving power of the resonance curve is then \\[\n\\frac{\\omega_0}{\\Delta} = \\frac{\\omega_0}{2\\beta} = Q.\n\\] Evidently then, \\(Q\\) represents the resolving power of the resonance curve. The higher the Q-factor is, the more sharply peaked the resonance curve is, and the easier we can pinpoint the resonance frequency exactly. Indeed, this is why \\(Q\\) is called a quality factor.\n\n\n\n\n\nOne very important system where we want a high-Q is a clock. To keep precise time, we need to make sure that it’s oscillating pretty much exactly at its resonance frequencies. This is because we need to keep a regular period, so \\(\\tau = \\frac{2\\pi}{\\omega}\\) can’t be allowed to vary very much from the true period \\(\\tau_0 = \\frac{2\\pi}{\\omega_0}\\). There’s a tradeoff though. Since the decay time of the transient behavior is \\(N\\tau_0 \\equiv \\frac{1}{\\beta}\\), it takes about \\(N=\\frac{Q}{\\pi}\\) cycles of ringing for the system to come to steady state. So the better precision we want, the longer we’ll have to wait for the system to come to steady state.\nNote that \\(Q\\) also affects the phase curve of the system, since \\[\n\\delta = \\tan^{-1} \\frac{2\\beta\\omega}{\\omega_0^2-\\omega^2} \\approx \\tan^{-1} \\frac{2\\omega}{Q\\Delta}.\n\\] Evidently as \\(Q\\) increases, the system becomes more responsive to sudden changes in phase around \\(\\omega_0\\).\n\n\n\n\n\n\n\nArbitrary Driving Forces\nThe situation where resonance occurs in a DDHO system is not just confined to sinusoidal driving forces. Using Fourier analysis, we can decompose more arbitrary driving forces into a sum of sinusoidal driving forces of different frequencies. The simplest of these cases is when the driving force is periodic with some period \\(\\tau\\). Even if the driver isn’t sinusoidal, we can decompose it into a linear combination of cosines of different frequencies, i.e. a Fourier Series, \\[\nf(t) = \\sum_{n=-\\infty}^{\\infty} f_n e^{i \\omega n t},\n\\] where each \\(f_n\\) can be found via the formula \\[\nf_n = \\langle f(t), e^{i \\omega n t} \\rangle = \\frac{\\omega}{\\pi} \\int_0^{\\tau} f(t) e^{i \\omega n t} dt.\n\\] Using the principle of superposition, we could then find the solution for each of the sinusoidal drivers term by term, and then sum them together to get the full solution. Each term will have amplitude and phase \\[\nA_n = \\frac{f_n}{\\sqrt{(\\omega_0^2-\\omega^2 n^2)^2 + (2\\beta\\omega n)^2}}, \\quad \\delta_n = \\tan^{-1} \\frac{2\\beta\\omega n}{\\omega_0^2-\\omega^2n^2}.\n\\] Plugging these in will yield a general solution of the form \\[\nx(t) = x_h(t) + \\sum_{n=1}^{\\infty} A_n \\cos(\\omega n t - \\delta_n).\n\\] Each component will yield its own resonance frequency where \\(\\omega n \\approx -\\omega_0\\). As a function of the main driving frequency, this means there will resonances at each \\(\\omega_n = \\frac{\\omega_0}{n}\\). The resonance curve for \\(A^2\\) can be found via Parseval’s Theorem, which says \\[\nA^2 = \\langle x^2(t) \\rangle = \\frac{1}{2} \\sum_{n=0}^\\infty A_n^2.\n\\] The peaks evidently go to zero as \\(n \\rightarrow \\infty\\) since each \\(A_n^2 \\propto f_n^2\\) and each \\(f_n \\rightarrow 0\\) by the Riemann–Lebesgue lemma.\n\n\n\n\n\nWhat if the driving force isn’t periodic? In this case we have a few options. One would be to decompose it into its Fourier transform, \\[\nf(t) = \\int_{-\\infty}^{\\infty} f(\\omega) e^{i\\omega t} dt.\n\\] Then each term gives a set of complex amplitudes and phases that can be solved for and stitched back together to get \\(x(t)\\). Another solution that’s perhaps more common is to use Green’s functions. Instead of decomposing the driver into a linear combination of periodic functions, we’ll decompose it into a linear combination of impulse responses or delta functions, \\[\nf(t) = \\int_{-\\infty}^{\\infty} f(t') \\delta(t-t') dt'.\n\\] To find the solution \\(x(t)\\), we first need to find the particular solution \\(G(t-t')\\) to the DDHO with an impulse response, \\[\n\\ddot G + 2\\beta\\dot G + \\omega_0^2 G = \\delta(t - t').\n\\] Once this is found, we can stitch together the full solution as \\[\nx(t) = \\int_{-\\infty}^{\\infty} f(t') G(t-t') dt'.\n\\] Note the Green’s function solution already incorporates in the homogeneous solution since \\(G(t-t')\\) must itself satisfy the initial conditions."
  },
  {
    "objectID": "classical-mechanics/reference-frames.html#orthogonal-transformations",
    "href": "classical-mechanics/reference-frames.html#orthogonal-transformations",
    "title": "Reference Frames",
    "section": "Orthogonal Transformations",
    "text": "Orthogonal Transformations\nSuppose \\(\\mathbf{x}\\) is some vector, and \\(\\{\\mathbf{e}_i\\}\\) and \\(\\{\\mathbf{e}_i'\\}\\) are two orthonormal bases for \\(\\mathbb{R}^3\\), then \\[\n\\mathbf{x} = \\sum_{j=1}^3 x_j \\mathbf{e}_j = \\sum_{i'=1}^3 x_{i'} \\mathbf{e}_{i'},\n\\] where \\(x_i = \\mathbf{x} \\cdot \\mathbf{e}_i\\) and \\(x_i' = \\mathbf{x} \\cdot \\mathbf{e}_i'\\).\nNotation: From now on we’ll use the Einstein summation convention. If a repeated index occurs in a sum, we’ll omit the \\(\\sum\\) symbol. For example, we can re-write the above line as the following, where it’s understood we’re summing over \\(j\\) and \\(i'\\) in each case, \\[\n\\mathbf{x} = x_j \\mathbf{e}_j = x_{i'} \\mathbf{e}_{i'}.\n\\] Now, observe we can write one component in terms of the other as \\[\nx_i' = \\mathbf{x} \\cdot \\mathbf{e}_{i'} = (\\mathbf{e}_j \\cdot \\mathbf{e}_{i'}) x_j \\equiv R_{i'j} x_j,\n\\] where \\(R_{i'j} = \\mathbf{e}_j \\cdot \\mathbf{e}_i'\\) defines a matrix \\(\\mathbf{R}\\) called an orthogonal transformation.\nNotice that if we take the inner product of two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we get \\[\n\\mathbf{x} \\cdot \\mathbf{y} = x_{i'} \\delta_{i' j'} x_{j'} = R_{ii'} \\delta_{i'j'} R_{j'j} x_i x_j = \\mathbf{x} \\cdot (\\mathbf{R}^\\top \\mathbf{R}) \\mathbf{y}.\n\\] Thus, an orthogonal transformation \\(\\mathbf{R}\\) satisfies the property that \\[\n\\mathbf{R}^\\top \\mathbf{R} = \\mathbf{I}, \\quad \\text{or} \\quad R_{ij} R_{jk} = \\delta_{ik}.\n\\] Due to the inner product preserving nature of orthogonal transformations, they can in a sense be used to define what we mean by a scalar or vector or tensor in classical mechanics. They’re objects that transform a certain way under an orthogonal transformation:\n\nA scalar is any object \\(\\alpha\\) that is invariant under an orthogonal transformation, \\[\n\\alpha' = \\alpha.\n\\]\nA vector is any object \\(\\mathbf{v}\\) that transforms under an orthogonal transformation as, \\[\nv_{i'} = R_{i'i} v_i.\n\\]\nA tensor of order \\(k\\) is any object \\(\\mathbf{T}\\) that transforms under an orthogonal transformation as, \\[\nT_{i_1' i_2' \\cdots i_k'} = R_{i_1' i_1} R_{i_2' i_2} \\cdots R_{i_k' i_k} T_{i_1 i_2 \\cdots i_k}.\n\\]\n\nNotice since \\(\\mathbf{R}^\\top \\mathbf{R} = \\mathbf{I}\\), we can take the determinant of both sides to get \\(\\det(\\mathbf{R}^\\top \\mathbf{R}) = \\det^2(\\mathbf{R}) = 1\\), which implies that \\(\\det(\\mathbf{R}) = \\pm 1\\). This fact divides orthogonal transformations into two distinct classes:\n\nProper Rotations (\\(\\det(\\mathbf{R}) = 1\\)): These correspond to pure rotations in space. They preserve the handedness of the underlying coordinate system.\nImproper Rotations (\\(\\det(\\mathbf{R}) = -1\\)): These correspond to reflections in space, which are transformations \\(\\mathbf{v} \\Rightarrow -\\mathbf{v}\\) combined with a pure rotation. These transformations permute the handedness of the underlying coordinate system.\n\n\n\n\n\n\nMost vector operations are proper, in the sense that they preserve the handedness of the underlying coordinate system. If \\(\\mathbf{v}\\) is a vector, they’ll transform under a reflection to \\(-\\mathbf{v}\\). The one major exception is the cross product, which reverses the handedness. Under a reflection, it keeps its sign, \\[\n\\mathbf{v} \\times \\mathbf{w} \\Rightarrow (-\\mathbf{v}) \\times (-\\mathbf{w}) = \\mathbf{v} \\times \\mathbf{w}.\n\\] For this reason, cross products are sometimes called pseudovectors or axial vectors to distinguish them from ordinary vectors that transform under a reflection as \\(\\mathbf{v} \\Rightarrow -\\mathbf{v}\\).\nAside: It turns out that the set of all orthogonal transformations on \\(\\mathbb{R}^3\\) form a group \\(G\\), in the sense that it satisfies the following special “symmetry” properties:\n\nClosure: If \\(A, B \\in G\\) , then \\(AB \\in G\\) also.\nAssociativity: For any \\(A,B,C \\in G\\), we have \\((AB)C = A(BC)\\).\nIdentity: There is a unique element \\(I \\in G\\) satisfying \\(IA = AI\\) for any \\(A \\in G\\).\nInvertibility: For any \\(A \\in G\\), there is an inverse element \\(A^{-1}\\) such that \\(A^{-1} A = A A^{-1} = I\\).\n\nThe group of orthogonal transformations under matrix multiplication is called the orthogonal group, denoted \\(O(3)\\). The subset of \\(O(3)\\) where \\(\\det(\\mathbf{R})=1\\) happens to form a subgroup, i.e. a subset of \\(O(3)\\) that’s closed under group operations. It’s called the special orthogonal group, denoted \\(SO(3)\\). This is essentially the group of all rotations in 3 dimensions. The orthogonal groups turn out to be very important in understanding the theory of angular momentum, especially in quantum mechanics.\n\nExample: Rotations in two dimensions\nWe can easily figure out what proper rotations look like in 2D space by looking at how to relate one basis with another. Suppose \\(\\{\\mathbf{e}_x, \\mathbf{e}_y \\}\\) is the standard basis for \\(\\mathbb{R}^2\\), and \\(\\{\\mathbf{e}_{x'}, \\mathbf{e}_{y'}\\}\\) is some other orthonormal basis. Suppose \\(\\varphi\\) is the angle between \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_{x'}\\). Using a little geometry, we have,\n\n\n\n\n\n\\[\n\\begin{align*}\nR_{x'x} &= \\mathbf{e}_{x'} \\cdot \\mathbf{e}_x = \\cos\\varphi,\n&R_{x'y} &= \\mathbf{e}_{x'} \\cdot \\mathbf{e}_y = \\sin\\varphi, \\\\\nR_{y'x} &= \\mathbf{e}_{y'} \\cdot \\mathbf{e}_x = -\\sin\\varphi,\n&R_{y'y} &= \\mathbf{e}_{y'} \\cdot \\mathbf{e}_y = \\cos\\varphi. \\\\\n\\end{align*}\n\\] We can thus express any proper rotation in \\(\\mathbb{R}^2\\) using a \\(2 \\times 2\\) matrix of the form \\[\n\\mathbf{R}(\\varphi) =\n\\begin{pmatrix}\n\\cos\\varphi & \\sin\\varphi \\\\\n-\\sin\\varphi & \\cos\\varphi\n\\end{pmatrix}.\n\\] This is the matrix that rotates the underlying basis \\(\\{\\mathbf{e}_x, \\mathbf{e}_y \\}\\) to the new, rotated basis \\(\\{\\mathbf{e}_{x'}, \\mathbf{e}_{y'} \\}\\).\n\n\nActive vs Passive Transformations\nThe previous example suggests that we can think about a rotation in space two different ways. One way is to rotate the underlying basis and keep the vector \\(\\mathbf{v}\\) fixed. That is, \\(\\mathbf{e}_i \\Rightarrow R_{i'i} \\mathbf{e}_i\\).. This way of looking at a rotation is called a passive transformation. It rotates the coordinate system under \\(\\mathbf{v}\\), not \\(\\mathbf{v}\\) itself.\nAnother way of looking at a rotation is to imagine keeping the coordinate system fixed, but rotating the components of the vector \\(\\mathbf{v}\\) directly. That is, \\(v_i \\Rightarrow R_{i'i}v_i\\). This way of looking at a rotation is called an active transformation. Despite sounding semantically different, these two ways are physically equivalent. Note though that if a vector rotates actively under \\(\\mathbf{R}(-\\varphi)\\), it will rotate passively under \\(\\mathbf{R}(\\varphi)\\).\n\n\n\n\n\nIt’s usually more convenient to assume rotations are active transformations. The major exception is when dealing with rigid bodies, where it’s more convenient to passively transform to body coordinates. Note the same logic applies to 3D transformations. In that case, there are now three angles of rotation to deal with, not just one."
  },
  {
    "objectID": "classical-mechanics/reference-frames.html#linearly-accelerating-frames",
    "href": "classical-mechanics/reference-frames.html#linearly-accelerating-frames",
    "title": "Reference Frames",
    "section": "Linearly Accelerating Frames",
    "text": "Linearly Accelerating Frames\nLet’s now examine the motion of systems in non-inertial reference frames. In examining accelerating frames, we’ll treat them as static coordinate systems, completely ignoring the forces that cause the reference frame to accelerate in the first place. We’ll start with linearly accelerating frames.\nSuppose a system is “locked into” a reference frame \\(S_{rel}\\), which is itself moving at a velocity \\(\\mathbf{v}_0\\) with respect to an inertial lab frame \\(S\\). We’ll seek out the equations of motion with respect to the non-inertial frame \\(S_{rel}\\).\n\n\n\n\n\nEvidently, \\(\\mathbf{x} = \\mathbf{x}_{rel} + \\mathbf{v}_0t\\), which means \\(\\mathbf{v} = \\mathbf{v}_{rel} + \\mathbf{v}_0\\), and \\(\\mathbf{a} = \\mathbf{a}_{rel} + \\mathbf{a}_0\\). When \\(\\mathbf{a}_0=\\mathbf{0}\\) we recover the special case of a Galilean transformation. In that case, \\(\\mathbf{F} = m \\mathbf{a} = m \\mathbf{a}_{rel}\\), which means \\(S_{rel}\\) is by definition an inertial frame.\nIf \\(\\mathbf{a}_0 \\neq \\mathbf{0}\\), we get \\(\\mathbf{F} = m(\\mathbf{a}_0 + \\mathbf{a}_{rel})\\), or \\(m\\mathbf{a}_{rel} = \\mathbf{F} - m \\mathbf{a}_0\\). We can think about this in another way, by defining a relative force \\(\\mathbf{F}_{rel} = m\\mathbf{a}_{rel}\\) and thinking of it as being composed of an inertial force \\(\\mathbf{F}\\) along with a fictitious force \\(\\mathbf{F}_{lin} = -m\\mathbf{a}_0\\), \\[\n\\mathbf{F}_{rel} = \\mathbf{F} + \\mathbf{F}_{lin} = \\mathbf{F} - m \\mathbf{a}_0.\n\\] One special case of a linear accelerating frame is an object free-falling under gravity. In that case, \\(\\mathbf{a}_0=-\\mathbf{g}\\) is constant. In some sense, this means we can treat gravity as a kind of generalized coordinate transformation that shifts the acceleration from \\(\\mathbf{a}\\) to \\(\\mathbf{a}_{rel} = \\mathbf{a} + \\mathbf{g}\\). This curious fact arises due to the equivalence principle, which says the gravitational force is proportional to the inertial mass \\(m\\). This curious fact causes the \\(m\\) to cancel from both sides of \\(m\\mathbf{a} = m\\mathbf{g}\\). As far as we know, gravity is the only force in nature with this special property. The equivalence principle is essentially the launch point to Einstein’s general theory of relativity.\n\n\n\n\n\n\n\nExample: Pendulum in an accelerating railcar\nA railcar is moving along the x-axis at a constant acceleration \\(\\mathbf{a}_0\\) with respect to the lab frame. Inside the railcar, a pendulum with mass \\(m\\) and length \\(\\ell\\) is attached to the ceiling and allowed to swing freely. Find the equations of motion for the swinging pendulum. Also, find the equilibrium position of the pendulum.\n\n\n\n\n\nWorking in the frame of the railcar, we have \\(\\mathbf{F}_{rel} = \\mathbf{F} + \\mathbf{F}_{lin}\\). The inertial forces on the pendulum are the string tension \\(\\mathbf{T}\\) and gravity \\(m\\mathbf{g}\\). We thus have \\[\nm\\mathbf{a}_{rel} = \\mathbf{T} + m\\mathbf{g} - m\\mathbf{a}_0 \\equiv \\mathbf{T} + m\\mathbf{g}_{eff},\n\\] where \\(\\mathbf{g}_{eff} \\equiv \\mathbf{g} - \\mathbf{a}_0\\) acts as an effective gravity on the pendulum inside the moving railcar. We can thus use the standard method to solve for the pendulum, but replacing \\(\\mathbf{g}\\) with \\(\\mathbf{g}_{eff}\\), to get \\[\n\\ddot \\theta = -\\omega^2 \\sin \\theta, \\quad \\text{where} \\quad \\omega^2 \\equiv \\frac{|\\mathbf{g}_{eff}|}{\\ell} = \\frac{\\sqrt{g^2 + a_0^2}}{\\ell}.\n\\] The equilibrium position occurs when \\(\\mathbf{F}_{rel}=\\mathbf{0}\\), which is when \\(\\mathbf{T} = -m\\mathbf{g}_{eff}\\). Using a little trig, we can see the equilibrium angle will be shifted to the angle \\(\\theta_{eq}\\) given by\n\n\n\n\n\n\\[\n\\theta_{eq} = \\tan^{-1} \\frac{g}{a_0}.\n\\]\nNotice when \\(a_0\\) we get \\(\\theta_{eq}=0\\), which is what we’d expect if the railcar weren’t accelerating."
  },
  {
    "objectID": "classical-mechanics/reference-frames.html#rotating-frames",
    "href": "classical-mechanics/reference-frames.html#rotating-frames",
    "title": "Reference Frames",
    "section": "Rotating Frames",
    "text": "Rotating Frames\nLet’s now look at reference frames that are rotating about some axis. Without loss of generality, we’ll consider a reference frame \\(S_{rot}\\) that’s rotating about the z-axis with respect to the lab frame \\(S\\) at some angular velocity \\(\\boldsymbol{\\omega} = \\dot \\varphi \\mathbf{e}_z\\).\n\n\n\n\n\nLet \\(\\mathbf{A}\\) be some vector in this rotating frame that’s at an angle \\(\\theta\\) with the axis of rotation. The amount that \\(\\mathbf{A}\\) changes due to the frame’s rotation by an amount \\(\\delta\\varphi\\) is given by \\[\n\\delta A = A_\\perp \\delta\\varphi = A\\sin\\theta\\delta\\varphi = |\\mathbf{A} \\times \\delta\\boldsymbol{\\varphi}|,\n\\] so by the right-hand rule we have \\(\\delta\\mathbf{A} = \\delta\\boldsymbol{\\varphi} \\times \\mathbf{A}\\). Dividing both sides by \\(dt\\), we finally have \\[\n\\frac{d\\mathbf{A}}{dt} = \\boldsymbol{\\omega} \\times \\mathbf{A}.\n\\] Remark:  Since velocities add as vectors, so too do angular velocities. This means if \\(S'\\) is a frame rotating relative \\(S\\), and \\(S''\\) is yet another frame that’s rotating to \\(S'\\), then we have \\[\n\\mathbf{v}_S'' = \\mathbf{v}_S' + \\mathbf{v}_{S'}'' \\quad \\Longrightarrow \\quad  \\boldsymbol{\\omega}_S'' \\times \\mathbf{r}_S = \\boldsymbol{\\omega}' \\times \\mathbf{r}_S + \\boldsymbol{\\omega}'' \\times \\mathbf{r}_{S'} \\quad \\Longrightarrow \\quad  \\boldsymbol{\\omega}_S'' = \\boldsymbol{\\omega}_S' + \\boldsymbol{\\omega}_{S'}''.\n\\] This fact allows us to easily solve problems involving complex hierarchies of rotations.\nNow, suppose \\(S_{rot}\\) is rotating with angular velocity \\(\\boldsymbol{\\omega}\\) with respect to the origin of \\(S\\). With respect to an observer in each frame, a vector \\(\\mathbf{A} = A_i \\mathbf{e}_i = A_i^{rot}\\mathbf{e}_i = \\mathbf{A}_{rel}\\) changes as\n\\[\n\\begin{align*}\n\\frac{d\\mathbf{A}}{dt}\\bigg|_{rot} &= \\dot A_i^{rot} \\mathbf{e}_i^{rot}, \\\\\n\\frac{d\\mathbf{A}}{dt}\\bigg|_{lab} &= \\dot A_i^{rot} \\mathbf{e}_i^{rot} + A_i^{rot} \\mathbf{\\dot e}_i^{rot}, \\\\\n\\frac{d\\mathbf{e}_i^{rot}}{dt}\\bigg|_{lab} &= \\boldsymbol{\\omega} \\times \\mathbf{e}_i^{rot}.\n\\end{align*}\n\\] Thus, we have \\[\n\\frac{d\\mathbf{A}}{dt}\\bigg|_{lab} = \\frac{d\\mathbf{A}}{dt}\\bigg|_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{A}_{rot}\n\\] This evidently defines a transport operation between the lab frame and the rotating frame. Namely, time derivatives in the lab frame are related to time derivatives in the rotating frame via \\[\n\\frac{d}{dt}\\bigg|_{lab} = \\frac{d}{dt}\\bigg|_{rot} + \\boldsymbol{\\omega} \\times.\n\\] This result is sometimes called the transport theorem.\nUsing the transport theorem we can now derive what the equations of motion look like inside the rotating frame. Plugging \\(\\mathbf{x}\\) into the transport equation, we get \\[\n\\frac{d\\mathbf{x}}{dt}\\bigg|_{lab} = \\frac{d\\mathbf{x}}{dt}\\bigg|_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{x}_{rot},\n\\] or \\[\n\\mathbf{v} = \\mathbf{v}_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{x}_{rot}.\n\\] To get the acceleration \\(\\mathbf{a}\\), we need to apply the transport equation again to the velocity vector,\n\\[\n\\begin{align*}\n\\frac{d\\mathbf{v}}{dt}\\bigg|_{lab} &= \\bigg(\\frac{d}{dt}\\bigg|_{rot} + \\boldsymbol{\\omega} \\times \\bigg) (\\mathbf{v}_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{x}_{rot}) \\\\\n&= \\mathbf{\\dot v}_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{v}_{rot} + \\boldsymbol{\\dot \\omega} \\times \\mathbf{x}_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{v}_{rot} + \\boldsymbol{\\omega} \\times (\\boldsymbol{\\omega} \\times \\mathbf{x}_{rot}).\n\\end{align*}\n\\] Or after cleaning up a bit, \\[\n\\mathbf{a} = \\mathbf{a}_{rot} + 2 \\boldsymbol{\\omega} \\times \\mathbf{v}_{rot} + \\boldsymbol{\\dot \\omega} \\times \\mathbf{x}_{rot} + \\boldsymbol{\\omega} \\times (\\boldsymbol{\\omega} \\times \\mathbf{x}_{rot}).\n\\]\nSince \\(\\mathbf{F} = m \\mathbf{a}\\) in the lab frame, we can multiply both sides by \\(m\\) and re-arrange terms to get the force vector \\(\\mathbf{F}_{rot}\\), \\[\n\\mathbf{F}_{rot} = \\mathbf{F} + m \\boldsymbol{\\omega} \\times (\\mathbf{x}_{rot} \\times \\boldsymbol{\\omega}) + 2m \\mathbf{v}_{rot} \\times \\boldsymbol{\\omega} + m \\mathbf{x}_{rot} \\times \\boldsymbol{\\dot \\omega}.\n\\] Evidently, there are three distinct fictitious force terms. Naturally, they each have special names:\n\nCentrifugal Force: \\(\\mathbf{F}_{cf} = m \\boldsymbol{\\omega} \\times (\\mathbf{x}_{rot} \\times \\boldsymbol{\\omega}) = m(\\boldsymbol{\\omega} \\cdot \\mathbf{x}_{rot})\\mathbf{x}_{rot} - m\\omega^2 \\mathbf{x}_{rot}\\).\nCoriolis Force: \\(\\mathbf{F}_{cor} = 2m \\mathbf{v}_{rot} \\times \\boldsymbol{\\omega}\\).\nEuler Force: \\(\\mathbf{F}_{eul} = m \\mathbf{x}_{rot} \\times \\boldsymbol{\\dot \\omega}\\).\n\nIn terms of these forces, we can finally write the force experienced in the rotating frame as \\[\n\\mathbf{F}_{rot} = \\mathbf{F} + \\mathbf{F}_{cf} + \\mathbf{F}_{cor} + \\mathbf{F}_{eul}.\n\\] The centrifugal force tends to push a rotating object outward radially from the origin, similar to how a centrifuge works. In the simple case when the position is perpendicular to the axis of rotation, the centrifugal force reduces to the more familiar form from elementary physics, \\[\n\\mathbf{F}_{cf} = - m\\omega^2 \\mathbf{x}_{rot} = -\\frac{mv_{rot}^2}{r_{rot}} \\mathbf{e}_r.\n\\] The Coriolis force tends to deflect a moving object away from its line of motion. It arises due to the fact that as the object moves, the frame under it is rotating underneath, which causes an apparent deflection sideward.\n\n\nExample: Throwing a baseball from the North Pole\nSuppose a baseball is thrown from the North Pole for a distance \\(\\ell\\) and a constant velocity \\(\\mathbf{v}_0\\) with respect to the lab frame. Find the deflection angle \\(\\delta\\theta\\) of the ball caused by the Coriolis force.\n\n\n\n\n\nThe rotating frame in this case is the Earth itself. The Earth rotates counterclockwise about the North Pole with an angular velocity of \\(\\omega_\\oplus = \\frac{2\\pi}{\\text{1 day}} \\approx 7 \\cdot 10^{-5} \\frac{\\text{rad}}{\\text{sec}}\\). Suppose \\(\\mathbf{e}_z\\) is the direction pointing skyward, with the origin at the North Pole. Then \\(\\boldsymbol{\\omega} = \\omega_{\\oplus} \\mathbf{e}_z\\). Suppose the ball is thrown initially along the positive x-axis, so \\(\\mathbf{x} = \\ell \\mathbf{e}_x\\). Since \\(\\mathbf{v} = \\mathbf{v}_{rot} + \\boldsymbol{\\omega} \\times \\mathbf{x}_{rot}\\), we have \\[\n\\mathbf{v}_0 = \\mathbf{v}_{rot} + \\omega_{\\oplus} v_0 t (\\mathbf{e}_z \\times \\mathbf{e}_x) = \\mathbf{v}_{rot} + \\omega_{\\oplus} v_0 t \\mathbf{e}_y.\n\\] On time scales \\(t \\ll \\text{1 day}\\), we can say \\(\\mathbf{v}_{rot} \\approx \\mathbf{v}_0\\) since in that case \\(\\omega_{\\oplus} t\\) becomes small. Then we have \\[\n\\mathbf{F}_{cor} = 2m \\mathbf{v}_{rot} \\times \\boldsymbol{\\omega} \\approx 2m v_0 \\omega_{\\oplus} (\\mathbf{e}_x \\times \\mathbf{e}_z) = -2m v_0 \\omega_{\\oplus} \\mathbf{e}_y,\n\\] Evidently, the Coriolis force in this case is constant, which means the acceleration \\(\\mathbf{a}_{cor}\\) is constant too. We thus get a simple constant equation of motion in \\(y\\), \\[\n\\ddot y = -2v_0 \\omega_{\\oplus}.\n\\] Solving this EOM gives a deflection distance of \\[\nd = |y| = \\frac{1}{2}(2v_0 \\omega_{\\oplus})^2 = v_0 \\omega_{\\oplus} t^2.\n\\] Finally, we can use this to calculate the deflection angle \\(\\delta\\theta\\), \\[\n\\delta\\theta \\approx \\frac{d}{\\ell} = \\frac{\\omega_{\\oplus}v_0 t^2}{v_0 t} = \\omega_{\\oplus} t.\n\\] To plug in some numbers, suppose the ball stays in the air for \\(t = \\text{100 sec}\\). Then we’d get \\(\\delta\\theta \\approx 0.4^\\circ\\), indeed a very small deflection.\n\n\n\nExample: Hurricanes\nIt turns out that hurricanes rotate the direction they do due to the Coriolis force of the Earth. Pressure gradients cause water currents flowing east-west to spiral inward. In the Northern hemisphere, water deflects rightward, causing the gradients (or “hurricanes”) to spiral counterclockwise. Whereas in the Southern hemisphere, water deflects leftward, causing gradients (or “typhoons”) to spiral clockwise.\n\n\n\n\n\n\n\n\nExample: The Foucalt pendulum\nThe Foucalt pendulum is a classic problem that’s often used to demonstrate that the Earth rotates. Suppose a very long pendulum of length \\(l\\) and mass \\(m\\) is fixed near the Earth’s surface at some latitude \\(\\lambda\\) above the equator. Here’s a picture of what’s going on.\n\n\n\n\n\nIt’s reasonable to assume that \\(\\omega_{\\oplus}\\) is constant, so the Euler force is zero. It’s also reasonable to assume the centrifugal force is zero since \\(\\omega_{\\oplus}\\) is small. Thus, in the frame of the rotating Earth, we’re left with the inertial forces on the pendulum and the Coriolis force, \\[\nm\\mathbf{a}_{rot} = m\\mathbf{g} + \\mathbf{T} - 2m\\boldsymbol{\\omega} \\times \\mathbf{v}_{rot}.\n\\] Choose the axes such that the z-axis is pointing outward from the Earth’s surface at the pendulum and the other axes are planar to the surface. Now, we can write \\(\\mathbf{g} = -g \\mathbf{e}_z\\), and using some trig it’s not too hard to show that \\[\n\\mathbf{T} \\approx -\\frac{T}{\\ell} (x \\mathbf{e}_x + y \\mathbf{e}_y + \\ell \\mathbf{z}).\n\\] Using the latitude angle \\(\\lambda\\) we can also express the angular velocity \\(\\boldsymbol{\\omega}\\) as \\[\n\\boldsymbol{\\omega} = -\\omega_{\\oplus}\\cos\\lambda\\mathbf{e}_x + \\omega_{\\oplus}\\sin\\lambda\\mathbf{e}_z.\n\\] Since the pendulum approximately speaking only moves in the xy-plane, we also have \\[\n\\mathbf{v}_{rot} = \\dot x \\mathbf{e}_x + \\dot y \\mathbf{e}_y.\n\\] Together, these together imply \\[\n\\boldsymbol{\\omega} \\times \\mathbf{v}_{rot} \\approx -\\dot y \\omega_{\\oplus} \\sin\\lambda \\mathbf{e}_x + \\dot x \\omega_{\\oplus} \\sin\\lambda \\mathbf{e}_y - \\dot y \\omega_{\\oplus} \\cos\\lambda \\mathbf{e}_z.\n\\] This means \\[\n\\mathbf{a}_{rot} = \\bigg(-\\frac{Tx}{m\\ell} + 2\\dot y \\omega_{\\oplus} \\sin\\lambda \\bigg) \\mathbf{e}_x + \\bigg(-\\frac{Ty}{m\\ell} - 2\\dot x \\omega_{\\oplus} \\sin\\lambda \\bigg) \\mathbf{e}_x,\n\\] which gives equations of motion\n\\[\n\\begin{align*}\n\\ddot x &= -\\frac{T}{m\\ell} \\cdot x + 2\\omega_{\\oplus}\\sin\\lambda \\cdot \\dot y \\\\\n\\ddot y &= -\\frac{T}{m\\ell} \\cdot y - 2\\omega_{\\oplus}\\sin\\lambda \\cdot \\dot x. \\\\\n\\end{align*}\n\\] If we define \\(\\omega_0^2 \\equiv \\frac{T}{m\\ell} = \\frac{g}{\\ell}\\), we can re-arrange and write the equations of motion in the form\n\\[\n\\begin{align*}\n\\ddot x + \\omega_0^2 \\cdot x &= 2\\omega_z \\dot y \\\\\n\\ddot y + \\omega_0^2 \\frac{T}{m\\ell} \\cdot y &= -2\\omega_z \\dot x, \\\\\n\\end{align*}\n\\] where \\(\\omega_z = \\omega_{\\oplus}\\sin\\lambda\\). If we combine these two equations, this is equivalent to a complex DHO problem with imaginary damping, \\[\n\\ddot z + 2i\\omega_z \\dot z + \\omega_0^2 z = 0.\n\\] This means solutions will have the form \\[\nz(t) = e^{-i\\omega_z t}(A e^{i\\omega't} + B e^{-i\\omega't}).\n\\] where \\(\\omega' \\equiv \\sqrt{\\omega_z^2 + \\omega_0^2}\\). If we assume the pendulum swings much faster than the Earth rotates, we have \\(\\omega_0 \\gg \\omega_{\\oplus}\\), which means we can approximate \\(z(t)\\) as \\[\nz(t) \\approx e^{-i\\omega_z t}(A e^{i\\omega_0 t} + B e^{-i\\omega_0 t}).\n\\] If we define \\(z'(t) \\equiv A e^{i\\omega_0 t} + B e^{-i\\omega_0 t}\\), then \\(z(t) = e^{-i\\omega_z t} z'(t)\\), and we can write the real solutions as\n\\[\n\\begin{align*}\nx(t) &= x'(t) \\cos\\omega_z t + y'(t) \\sin\\omega_z t, \\\\\ny(t) &= -x'(t) \\sin\\omega_z t + y'(t) \\cos\\omega_z t. \\\\\n\\end{align*}\n\\] This says that the plane of oscillation itself undergoes a rotation in the xy-plane. That is, the plane of the pendulum’s orbit precesses with a frequency given by \\[\n\\omega_z = \\omega_{\\oplus} \\sin\\lambda = 2\\pi\\frac{\\sin\\lambda}{\\text{1 day}}.\n\\] For example, at a latitude of \\(\\lambda = 34.5^\\circ\\), the pendulum precesses counterclockwise with a frequency of \\(\\omega_z \\approx \\text{3.86 rad/sec}\\), or about \\(8.5^\\circ\\) per hour. It appears precession is non-existent at the equator and highest at the poles, where precession happens exactly with the Earth’s rotation."
  },
  {
    "objectID": "classical-mechanics/reference-frames.html#general-non-inertial-frames",
    "href": "classical-mechanics/reference-frames.html#general-non-inertial-frames",
    "title": "Reference Frames",
    "section": "General Non-Inertial Frames",
    "text": "General Non-Inertial Frames\nMore generally, we can combine linearly accelerating and rotating frames by just adding the fictitious forces together. If \\(S_{rel}\\) is both accelerating and rotating about some axis with respect to \\(S\\), we’d have \\[\n\\mathbf{F}_{rel} = \\mathbf{F} + \\mathbf{F}_{lin} + \\mathbf{F}_{cf} + \\mathbf{F}_{cor} + \\mathbf{F}_{eul}.\n\\] This general form for a force in a non-inertial frame can be used to analyze a surprisingly large number of practical problems, where complicated forces can often be decomposed into a sum of linear forces and rotational forces.\nOne fact to be aware of about non-inertial frames is that energy need not be conserved. It’s only true in inertial frames that energy must be conserved. This has to do with the fact that in the relative frame we’re ignoring the forces on the relative frame itself, i.e. the forces that cause the frame to accelerate or rotate. We can generally recover the conservation of energy by transforming back to an inertial frame."
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#configuration-space",
    "href": "classical-mechanics/lagrangian-mechanics.html#configuration-space",
    "title": "Lagrangian Mechanics",
    "section": "Configuration Space",
    "text": "Configuration Space\nMany forces acting on a system do no work. They serve only to keep particles confined to some surface in space. Such forces are called forces of constraint. Examples of forces of constraint include the tension in a string and the normal force keeping an object on a physical surface.\nSuppose we have a system of \\(N\\) particles with positions \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N\\) respectively. Taken together, these positions can be thought of as defining a trajectory in the \\(3N\\)-dimensional space \\(\\mathbb{R}^{3N}\\). A holonomic constraint is a constraint that keeps the \\(N\\) particles confined to some lower-dimensional sub-manifold \\(\\mathcal{Q}\\) of \\(\\mathbb{R}^{3N}\\). Equivalently, it’s a (possibly time-dependent) function of the form \\[\nf(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N, t) = 0.\n\\] The dimension of \\(\\mathcal{Q}\\) is \\(n=3N-C\\), where \\(C\\) is the total number of constraints on the system. These are the number of degrees of freedom of the system. This sub-manifold is called the configuration space of the system. Since \\(\\mathcal{Q}\\) is \\(n\\)-dimensional, we should be able to parametrize it with \\(n\\) coordinates \\(q_1, q_2, \\cdots, q_n\\). We call these generalized coordinates. They’re not ordinary coordinates in real space. They’re a way of describing where in configuration space the system is at a given point in time.\n\n\n\n\n\nHolonomicity requires that we be able to find a 1-1 map going back and forth between generalized coordinates and the position vectors, \\[\nq_i = q_i(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N, t), \\quad \\mathbf{x}_\\alpha = \\mathbf{x}_\\alpha(q_1, q_2, \\cdots, q_n, t).\n\\] When the holonomic constraint isn’t time-dependent, they’re called scleronomic constraints. Otherwise they’re called rheonomic constraints. A system that’s not holonomic is called non-holonomic.\n\n\nExample: Simple Pendulum\nAs an easy example, consider the simple pendulum. Since there’s only one particle, \\(N=1\\). Since the length of the pendulum is fixed, that’s one constraint. Since the motion is confined to a plane, that’s another constraint. We thus have \\(n=3N-C=3-2=1\\) degrees of freedom, which we can of course take to be the angle \\(\\theta\\).\n\n\n\nExample: Rigid Bodies\nA more interesting example is the rigid body. A rigid body is a system of \\(N\\) particles whose particles are always a fixed distance apart, i.e. \\(d_{ij} = |\\mathbf{x}_i - \\mathbf{x}_j|\\) is fixed for all \\(i, j\\). This fixed distance requirement introduces a lot of constraints on the system. To see this, suppose \\(N=4\\). Then there are \\(C=6\\) constraints, since each particle must connect to each other particle. This means there are \\(n=3N-C=6\\) degrees of freedom.\n\n\n\n\n\nIt turns out this fact extends to rigid bodies with arbitrarily many particles as well since adding a new particle gives 3 more coordinates, but also 3 more constraints. A rigid body will always have exactly 6 degrees of freedom, which we usually take to be the 3 center of mass coordinates and the three Euler angles."
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#principle-of-virtual-work",
    "href": "classical-mechanics/lagrangian-mechanics.html#principle-of-virtual-work",
    "title": "Lagrangian Mechanics",
    "section": "Principle of Virtual Work",
    "text": "Principle of Virtual Work\nSuppose we have a system of \\(N\\) particles in mechanical equilibrium, so \\(\\mathbf{F}_i=\\mathbf{0}\\) for all \\(i\\). Let’s imagine we perturb each particle \\(\\mathbf{x}_i\\) by some amount \\(\\delta \\mathbf{x}_i\\), but only in a way that doesn’t change the configuration space. This means each perturbation must be a function of the generalized coordinates, \\(\\delta \\mathbf{x}_i = \\delta \\mathbf{x}_i(q_1, q_2, \\cdots, q_n, t)\\). Define the virtual work done on the system by, \\[\n\\delta W \\equiv \\sum \\mathbf{F}_i \\cdot \\delta\\mathbf{x}_i\n\\] Now, let’s decompose each force \\(\\mathbf{F}_i\\) into a sum of two components, an applied force \\(\\mathbf{F}_i^{app}\\) and a constraint force \\(\\mathbf{F}_i^{con}\\). The applied forces are the ones that do work on each particle, while the constraint forces are the ones that keep them confined to the configuration space. If the system is exactly in equilibrium, then \\(\\mathbf{F}_i = \\mathbf{F}_i^{app} + \\mathbf{F}_i^{con} = \\mathbf{0}\\), which means \\(\\delta W = 0\\) in equilibrium. But since constraint forces do no work, we get \\[\n\\delta W = \\sum \\mathbf{F}_i^{app} \\cdot \\delta\\mathbf{x}_i = 0\n\\] This is called the principle of virtual work.\nNote: Sometimes constraint forces do in fact do work on a system. One major example is a system in rolling motion, e.g. a wheel rolling down a ramp. We’ll mostly ignore these situations in this lesson.\nMore generally, if a system is not in equilibrium, we have \\(\\mathbf{F}_i = m_i \\mathbf{\\dot v}_i\\). If we insist the principle of virtual work must apply to these situations as well, we have \\[\n\\begin{align*}\n0 = \\delta W &= \\sum_i (\\mathbf{F}_i^{app} - m_i \\mathbf{\\dot v}_i) \\cdot \\delta \\mathbf{x}_i \\\\\n&= \\sum_i (\\mathbf{F}_i^{app} - m_i \\mathbf{\\dot v}_i) \\cdot \\sum_j\\frac{\\partial \\mathbf{x}_i}{\\partial q_j} \\delta q_j \\\\\n&= \\sum_j \\bigg(\\sum_i \\mathbf{F}_i^{app} \\cdot \\frac{\\partial \\mathbf{x}_i}{\\partial q_j}  - m_i \\mathbf{\\dot v}_i \\cdot \\frac{\\partial \\mathbf{x}_i}{\\partial q_j} \\bigg) \\delta q_j \\\\\n&= \\sum_j \\bigg[ Q_j - \\bigg(\\frac{d}{dt} \\frac{\\partial T}{\\partial \\dot q_j} - \\frac{\\partial T}{\\partial q_j} \\bigg) \\bigg] \\delta q_j.\n\\end{align*}\n\\] Here I defined \\(Q_j \\equiv \\mathbf{F}_i^{app} \\cdot \\frac{\\partial \\mathbf{x}_i}{\\partial q_j}\\). This term is called the generalized force. It acts as a force, but on the generalized coordinates instead of the position vectors directly. The other thing I did was re-wrote the momentum term by using the total kinetic energy \\(T = \\frac{1}{2} \\sum_i m_i \\mathbf{v}_i^2\\). Now, if we insist that all the \\(q_i\\) are independent of each other, then the terms in the sum must vanish individually, which means for all \\(j=1,\\cdots,n\\) we have \\[\nQ_j = \\frac{d}{dt} \\frac{\\partial T}{\\partial \\dot q_j} - \\frac{\\partial T}{\\partial q_j}.\n\\] In the special case where the forces on the system are conservative, we can use the potential energy \\(V\\) to express the generalized forces as \\(Q_j = -\\frac{\\partial V}{\\partial q_i}\\). Defining a function \\(L \\equiv T - V\\) called the Lagrangian and re-arranging terms, we finally have \\[\n\\frac{\\partial L}{\\partial q_j} - \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot q_j} = 0.\n\\] This gives a set of \\(n\\) equations for the generalized coordinates, called Lagrange’s equations.\nTo see why Lagrange’s equations are useful, consider the case when \\(T=\\frac{1}{2} \\sum_i m_i \\dot x_i^2\\) and \\(V = V(x_1, x_2, \\cdots, x_n)\\). Then we have a Lagrangian of the form \\[\nL = T - V = \\frac{1}{2} \\sum_i m_i \\dot x_i^2 - V(x_1, x_2, \\cdots, x_n),\n\\] which we can plug into the Euler-Lagrange Equations to get \\[\nm \\ddot x_i = - \\frac{\\partial V}{\\partial x_i} \\quad \\forall i=1,2,\\cdots,n.\n\\] But this is just \\(\\mathbf{F} = m \\mathbf{a}\\)! Evidently we’ve managed to reproduce Newton’s Laws from Lagrange’s equations. This in some sense suggest that Lagrange’s equations might be more general than Newton’s Laws, and in fact they are as we’ll see later."
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#solving-lagranges-equations",
    "href": "classical-mechanics/lagrangian-mechanics.html#solving-lagranges-equations",
    "title": "Lagrangian Mechanics",
    "section": "Solving Lagrange’s Equations",
    "text": "Solving Lagrange’s Equations\nThe Lagrangian formulation is very useful for solving problems that would be very complicated to solve using Newtonian approaches. This is particular true when there are complex constraints present. It’s thus very helpful to see a bunch of examples showing how to solve problems using Lagrangian methods.\nTo solve a problem using Lagrange’s equations we need to do the following steps:\n\nFigure out how many degrees of freedom the system has using \\(n=3N-C\\).\nIdentify the generalized coordinates \\(q_1,q_2,\\cdots,q_n\\).\nExpress the velocity vectors as a function of the generalized coordinates, \\(\\mathbf{v}_i = \\mathbf{v}_i(q_1,q_2,\\cdots,q_n)\\).\nWrite down the kinetic energy \\(T = \\frac{1}{2}\\sum_i m_i \\mathbf{v_i}^2(q_1,q_2,\\cdots,q_n)\\), the potential energy \\(V=V(q_1,q_2,\\cdots,q_n)\\), and finally the Lagrangian \\[\nL = T - V.\n\\]\nUse Lagrange’s equations to derive the equations of motion for the generalized coordinates, \\[\n\\frac{\\partial L}{\\partial q_j} - \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot q_j} = 0 \\quad \\forall j=1,2\\cdots,n.\n\\]\nIntegrate the equations of motion to get the generalized trajectories \\(q_1(t),q_2(t),\\cdots,q_n(t)\\).\nIf desired, convert back to real space coordinates via \\(\\mathbf{x}_\\alpha = \\mathbf{x}_\\alpha(q_1,q_2,\\cdots,q_n)\\).\n\n\n\nExample: The Simple Spring\nSuppose a mass \\(m\\) is attached to an ideal spring with spring constant \\(k\\).\n\n\n\n\n\nIn this case, \\(N = 1\\), and the spring is constrained to move along, say, the x-axis, so \\(C=2\\), and there’s just \\(n=3N-C=1\\) degree of freedom (as expected). If the generalized coordinate is just \\(q=x\\), we have \\[\nT = \\frac{1}{2} m \\dot q^2, \\quad V = \\frac{1}{2}kq^2,\n\\] which means the Lagrangian is \\[\nL = \\frac{1}{2} m \\dot q^2 - \\frac{1}{2}kq^2.\n\\] Solving Lagrange’s equation in this case gives \\[\n-\\frac{\\partial}{\\partial q} \\frac{k q^2}{2} - \\frac{d}{dt} \\frac{\\partial}{\\partial \\dot q} \\frac{m\\dot q^2}{2} = 0 \\quad \\Rightarrow \\quad m\\ddot q = -k q.\n\\] We’ve already seen the solution to this equation is just the SHO solution \\[\nq(t) = A\\cos(\\omega t - \\delta), \\quad \\omega^2 \\equiv \\frac{k}{m}.\n\\] If desired, in this case we could convert back to real coordinates via \\[\n\\mathbf{x}(t) = q(t) \\mathbf{e}_x = A\\cos(\\omega t - \\delta)\\mathbf{e}_x.\n\\]\n\n\n\nExample: Simple Pendulum\nSuppose a mass \\(m\\) is attached to a massless string of fixed length \\(\\ell\\) and allowed to swing.\n\n\n\n\n\nIn this problem, there’s \\(N=1\\) particle. The string being fixed adds one constraint, and motion being confined to the plane adds another, so we have \\(n=1\\) degrees of freedom here, which we’ll take to be the angle \\(q=\\theta\\). Using polar coordinates, we can write the kinetic and potential energies as \\[\nT = \\frac{1}{2} m\\ell^2 \\dot q^2, \\quad V = -mg\\ell\\cos q,\n\\] which gives a Lagrangian \\[\nL = \\frac{1}{2} m\\ell^2 \\dot q^2 + mg\\ell\\cos q.\n\\] Solving Lagrange’s equation, we get the equation of motion \\[\nm\\ell^2 \\ddot q + mg\\ell\\sin q = 0,\n\\] which is of course the usual equation of motion for the pendulum when \\(q=\\theta\\).\n\n\n\nExample: Central Potential\nSuppose a particle of mass \\(m\\) is in the presence of a central force field \\(V=V(r)\\). There’s one constraint since the problem must be spherically symmetric, which means we have \\(n=2\\) degrees of freedom. Working in spherical coordinates, the kinetic and potential energies are given by \\[\nT = \\frac{1}{2} m (\\dot r^2 + r \\dot \\varphi^2), \\quad V = V(r).\n\\] Plugging these into Lagrange’s equation and solving gives two equations of motion for \\(r\\) and \\(\\varphi\\), \\[\nm \\ddot r = mr \\dot\\varphi^2 - \\frac{dV}{dr}\\\\\n\\frac{d}{dt} mr^2 \\dot\\varphi = 0.\n\\] The second equation is interesting. It says the quantity \\(\\ell = mr^2 \\dot\\varphi\\) must be conserved. But this is just the angular momentum of the system! Evidently, conservation laws somehow fall out of Lagrange’s equations provided the right generalized coordinates are chosen.\n\n\n\nExample: Double Pendulum\nThe examples considered so far are pretty easy to solve using Newtonian methods. Here’s an example where it’s far easier to write down the equations of motion in the Lagrangian formulation. Consider the double pendulum, where a mass is attached to the end of another pendulum and both are allowed to swing. Suppose both masses have mass \\(m\\) and both strings are a fixed length \\(\\ell\\).\n\n\n\n\n\nHere there are \\(N=2\\) particles, each of which has two constraints. That means there are \\(n=2\\) total degrees of freedom in this system. From the above diagram we can see \\[\n\\begin{align*}\n&x_1 = \\ell \\sin\\theta_1, \\quad &&x_2 = \\ell(\\sin\\theta_1 + \\sin\\theta_2), \\\\\n&y_1 = -\\ell \\cos\\theta_1, \\quad &&y_2 = -\\ell(\\cos\\theta_1 + \\cos\\theta_2). \\\\\n\\end{align*}\n\\] We’ll choose the two angles \\(\\theta_1, \\theta_2\\) to be the generalized coordinates. The energies for each mass are given by \\[\n\\begin{align*}\nT_1 &= \\frac{1}{2}m (\\dot x_1^2 + \\dot y_1^2) = \\frac{1}{2}m \\ell^2 \\dot \\theta_1^2, \\quad &&\nT_2 = \\frac{1}{2}m (\\dot x_2^2 + \\dot y_2^2) = \\frac{1}{2}m \\ell^2\\big(\\dot\\theta_1^2 + \\dot\\theta_2^2 + 2\\cos(\\theta_1-\\theta_2)\\dot\\theta_1\\dot\\theta_2\\big), \\\\\nV_1 &= mgy_1 = -mg\\ell\\cos\\theta_1, \\quad && V_2 = mgy_2 = -mg\\ell(\\cos\\theta_1 + \\cos\\theta_2).\\\\\n\\end{align*}\n\\]\nPutting these all into the Lagrangian and simplifying, we evidently get \\[\nL = \\frac{1}{2}m\\ell^2\\big(2\\dot \\theta_1^2 + \\dot \\theta_2^2 + 2\\dot\\theta_1\\dot\\theta_2\\cos(\\theta_1-\\theta_2) \\big) + mg\\ell(2\\cos\\theta_1 + \\cos\\theta_2).\n\\] This then gives the following two equations of motion \\[\n\\begin{align*}\n-m\\ell^2\\dot\\theta_1\\dot\\theta_2\\sin(\\theta_1-\\theta_2) - 2mg\\ell\\sin\\theta_1 &= m\\ell^2 \\frac{d}{dt} \\big(\\dot\\theta_1 + 2\\dot\\theta_2\\cos(\\theta_1-\\theta_2)\\big), \\\\\nm\\ell^2\\dot\\theta_1\\dot\\theta_2\\sin(\\theta_1-\\theta_2) - mg\\ell\\sin\\theta_2 &= m\\ell^2 \\frac{d}{dt} \\big(\\dot\\theta_2 + 2\\dot\\theta_1\\cos(\\theta_1-\\theta_2) \\big). \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#principle-of-least-action",
    "href": "classical-mechanics/lagrangian-mechanics.html#principle-of-least-action",
    "title": "Lagrangian Mechanics",
    "section": "Principle of Least Action",
    "text": "Principle of Least Action\nA more general, first principles way to derive Lagrange’s equations is via an action principle. Action principles are a very general and powerful tool that applies across pretty much all of modern physics. Suppose we have \\(n\\) generalized coordinates \\(q_1,q_2,\\cdots,q_n\\). For simplicity I’ll write these as a vector, \\[\n\\mathbf{q} = (q_1,q_2,\\cdots,q_n).\n\\] Define the action \\(S\\) as a functional of \\(\\mathbf{q}\\) of the form \\[\nS[\\mathbf{q}] \\equiv \\int_{t_1}^{t_2} L(\\mathbf{q}, \\mathbf{\\dot q}, t) dt.\n\\] We assume the times \\(t_1\\) and \\(t_2\\) are fixed. The integrand function \\(L(\\mathbf{q}, \\mathbf{\\dot q}, t)\\) is called the Lagrangian. Note in general the Lagrangian can depend explicitly on time.\n\n\n\n\n\nSimilar to how it’s useful to analyze a function by looking at its behavior around its minima, we’ll want to analyze the functional \\(S\\) by looking at its behavior around the stationary points of \\(\\mathbf{q}\\). To do so, consider a small perturbation \\(\\delta\\mathbf{q} \\equiv \\varepsilon\\boldsymbol{\\eta}\\) of the coordinates, where \\(\\boldsymbol{\\eta} = \\boldsymbol{\\eta}(t)\\) is a function that vanishes at the endpoints \\(t_1\\) and \\(t_2\\), and \\(\\varepsilon \\ll 1\\). To proceed, we’ll assume the following fundamental principle:\nPrinciple of Least Action:  Physical trajectories evolve in such a way that the action remains stationary, i.e. \\[\n\\delta S[\\mathbf{q}] \\equiv \\frac{\\partial S}{\\partial \\varepsilon} \\bigg|_{\\varepsilon=0} = 0.\n\\] This means if we want to figure out how physical trajectories evolve, we need to find the optimal \\(\\mathbf{q}\\) that make the action \\(S\\) stationary. To do that, let’s set \\(\\delta S[\\mathbf{q}] = 0\\) and solve. We have \\[\n0 = \\delta S[\\mathbf{q}] = \\int_{t_1}^{t_2} \\delta L(\\mathbf{q}, \\mathbf{\\dot q}, t) dt = \\int_{t_1}^{t_2}\\bigg(\\frac{\\partial L}{\\partial\\mathbf{q}} \\cdot \\delta \\mathbf{q} + \\frac{\\partial L}{\\partial\\mathbf{\\dot q}} \\cdot \\delta \\mathbf{\\dot q} \\bigg) \\cdot \\delta\\mathbf{q} dt\n\\] Now, if we perform integration by parts on the second term, we can move the time derivative from \\(\\delta\\mathbf{\\dot q}\\) to the derivative \\(\\frac{\\partial L}{\\partial\\mathbf{\\dot q}}\\) at the cost of a minus sign, so we have \\[\n0 = \\delta S[\\mathbf{q}] = \\int_{t_1}^{t_2}\\bigg(\\frac{\\partial L}{\\partial\\mathbf{q}} - \\frac{d}{dt}\\frac{\\partial L}{\\partial\\mathbf{\\dot q}} \\bigg) \\cdot \\delta \\mathbf{q} dt.\n\\] Note the boundary terms vanish since we require \\(\\boldsymbol{\\eta}\\) to vanish at the endpoints. Assuming each of the coordinates in \\(\\mathbf{q}\\) are functionally independent the integrand must vanish identically, so we have \\[\n\\frac{\\partial L}{\\partial\\mathbf{q}} - \\frac{d}{dt}\\frac{\\partial L}{\\partial\\mathbf{\\dot q}} = \\mathbf{0}.\n\\] Of course, this is just the vector formulation of Lagrange’s equations. We’ve thus fully recovered Lagrange’s equations, and by extension Newton’s Laws for conservative systems, purely from the Principle of Least Action.\nNotice how general this derivation was. We didn’t even assume any specific form of the Lagrangian like \\(L = T-V\\). We only assumed it was a function of the generalized positions, velocities, and time. If we believe the Principle of Least Action, evidently any Lagrangian \\(L(\\mathbf{q}, \\mathbf{\\dot q}, t)\\) will produce equations of motion for some system, not necessarily mechanical, or even classical."
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#symmetries-and-conservation-laws",
    "href": "classical-mechanics/lagrangian-mechanics.html#symmetries-and-conservation-laws",
    "title": "Lagrangian Mechanics",
    "section": "Symmetries and Conservation Laws",
    "text": "Symmetries and Conservation Laws\nIt’s a deep fact of physics that the symmetries of a system are intimately connected with its conservation laws. It’s both practically and theoretically useful to better understand this connection.\n\nCyclic Coordinates\nConsider a general Lagrangian of the form \\(L = L(\\mathbf{q}, \\mathbf{\\dot q}, t)\\). Without loss of generality, suppose the Lagrangian happens to not be a function of the coordinate \\(q_1\\), so \\[\nL = L(q_2,\\cdots,q_n,\\dot q_1,\\dot q_2, \\cdots, \\dot q_n, t).\n\\] Note it can still be a function of the first coordinate’s velocity \\(\\dot q_1\\). In this situation, we’d say the coordinate \\(q_1\\) is a cyclic coordinate or ignorable coordinate. Evidently, it follows that \\[\n\\frac{\\partial L}{\\partial q_1} = 0 \\quad \\Longrightarrow \\quad 0 = \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot q_1} \\quad \\Longrightarrow \\quad p_1 \\equiv \\frac{\\partial L}{\\partial \\dot q_1} = const.\n\\] We call \\(p_1\\) the conjugate momentum to \\(q_1\\). We’ve thus shown that if \\(q_1\\) is cyclic, then its conjugate momentum \\(p_1\\) is conserved.\n\n\nExample: Free Particle\nFor a free particle, we have \\(L = \\frac{1}{2}m \\dot x^2\\). Since \\(L\\) is not a function of \\(x\\), evidently \\(x\\) is cyclic, which means its conjugate momentum \\(p_x\\) is conserved, \\[\np_x = \\frac{\\partial L}{\\partial \\dot x} = m \\dot x = const.\n\\] Notice in this case the conjugate momentum is the same thing as the ordinary linear momentum. This need not always be true. The conjugate momentum is more general than this.\n\n\n\nExample: Central Force in a Plane\nIn this case the Lagrangian is given by \\[\nL = \\frac{1}{2} m (\\dot r^2 + r^2 \\dot\\varphi^2) - V(r).\n\\] Since \\(\\varphi\\) is cyclic, its conjugate momentum \\(p_\\varphi\\) must evidently be conserved, \\[\np_\\varphi = \\frac{\\partial L}{\\partial \\dot \\varphi} = mr^2\\dot\\varphi = const.\n\\] Notice this is just the \\(z\\)-component of angular momentum \\(L_z = \\ell\\).\n\n\n\n\nNoether’s Theorem\nWe can still have conserved quantities in a given system even if none of its generalized coordinates are explicitly cyclic. This will happen, for example, if we didn’t happen to choose the natural choice of coordinates for some given problem, the ones that take advantage of the system’s symmetries.\nTo find conserved quantities, we need to find the symmetries. Formally, we’ll say a symmetry is any continuous transformation on the generalized coordinates that leaves the Lagrangian invariant. That is, for some parameter \\(s\\), if \\(\\mathbf{q}\\) is continuously transformed to \\(\\mathbf{q}_s\\), then \\(s\\) is a symmetry provided \\[\nL(\\mathbf{q}_s, \\mathbf{\\dot q}_s, t) = L(\\mathbf{q}, \\mathbf{\\dot q}, t).\n\\] Noether’s Theorem: If \\(s\\) is a symmetry of a system, then the quantity given by \\[\nQ \\equiv \\mathbf{p} \\cdot \\frac{\\partial \\mathbf{q}}{\\partial s} = p_i \\frac{\\partial q_i}{\\partial s}\n\\] must be conserved under transformation by \\(s\\). We call \\(Q\\) the Noether charge associated with \\(s\\).\n\nProof: To prove this theorem we need to show that \\(\\dot Q = 0\\). Since \\(s\\) is a symmetry, we must have \\[\n\\frac{\\partial}{\\partial s} L(\\mathbf{q}_s, \\mathbf{\\dot q}_s, t) = \\frac{\\partial}{\\partial s} L(\\mathbf{q}, \\mathbf{\\dot q}, t) = 0.\n\\] Using the chain rule together with Lagrange’s equations on \\(L(\\mathbf{q}_s, \\mathbf{\\dot q}_s, t)\\), we finally get \\[\n\\begin{align*}\n\\mathbf{0} &= \\frac{\\partial}{\\partial s} L(\\mathbf{q}_s, \\mathbf{\\dot q}_s, t) \\\\\n&= \\frac{\\partial L}{\\partial \\mathbf{q}_s} \\cdot \\frac{\\partial \\mathbf{q}_s}{\\partial s} + \\frac{\\partial L}{\\partial \\mathbf{\\dot q}_s} \\cdot \\frac{\\partial \\mathbf{\\dot q}_s}{\\partial s} \\\\\n&= \\frac{d}{dt} \\frac{\\partial L}{\\partial \\mathbf{\\dot q}_s} \\cdot \\frac{\\partial \\mathbf{q}_s}{\\partial s} + \\frac{\\partial L}{\\partial \\mathbf{\\dot q}_s} \\cdot \\frac{d}{dt} \\frac{\\partial \\mathbf{q}_s}{\\partial s} \\\\\n&= \\frac{d}{dt} \\bigg(\\frac{\\partial L}{\\partial \\mathbf{\\dot q}_s} \\cdot \\frac{\\partial \\mathbf{q}_s}{\\partial s}\\bigg) \\\\\n&= \\frac{d}{dt} \\bigg(\\mathbf{p} \\cdot \\frac{\\partial \\mathbf{q}}{\\partial s}\\bigg). \\tag*{$\\square$}\n\\end{align*}\n\\]\n\n\nExample: Conservation of Linear Momentum\nSuppose a single particle is moving through space. Suppose the space is homogeneous, that is, the particle’s Lagrangian is invariant to translations in space, \\[\nL(\\mathbf{x}, \\mathbf{\\dot x}, t) = L(\\mathbf{x} + \\delta\\mathbf{x}, \\mathbf{\\dot x}, t).\n\\] This is equivalent to saying the particle has a symmetry of the form \\(\\mathbf{x}(s) = \\mathbf{x} + s\\boldsymbol{\\varepsilon}\\), where \\(\\boldsymbol{\\varepsilon}\\) is constant. In this case, we’d evidently have \\[\nQ = \\mathbf{p} \\cdot \\frac{\\partial \\mathbf{x}}{\\partial s} = \\mathbf{p} \\cdot \\boldsymbol{\\varepsilon} = const.\n\\] Since \\(\\boldsymbol{\\varepsilon}\\) is an arbitrary constant, we must have \\(\\mathbf{p} = m \\mathbf{\\dot x} = const\\). That is, the linear momentum of the particle is conserved. Equivalently, if space is homogeneous, then the total linear momentum will be conserved.\n\n\n\nExample: Conservation of Angular Momentum\nSuppose again a single particle is moving through space. Suppose this time that space is isotropic, that is, the particle’s Lagrangian is invariant to rotations in space, \\[\nL(\\mathbf{x}, \\mathbf{\\dot x}, t) = L\\big(\\delta\\mathbf{R}\\mathbf{x}, \\mathbf{\\dot x}, t\\big).\n\\] Now, we can always think of a rotation in space as a rotation about some axis. For simplicity we’ll choose that axis to be the \\(z\\)-axis, in which case we can think of \\(\\delta\\mathbf{R}\\) as rotating \\(\\mathbf{x}\\) by some azimuthal angle \\(\\delta\\varphi\\). Take as generalized coordinates the spherical coordinates \\(r, \\theta, \\varphi\\). Then rotational invariance is equivalent to saying the particle has a symmetry of the form \\(\\varphi(s) = \\varphi + s\\). We thus have \\[\nQ = p_r \\frac{\\partial r}{\\partial s} + p_\\theta \\frac{\\partial \\theta}{\\partial s} + p_\\varphi \\frac{\\partial \\varphi}{\\partial s} = p_\\varphi = const.\n\\] That is, the angular momentum \\(L_z = mr^2 \\dot \\varphi = const\\). Since choosing the \\(z\\)-axis as the rotation axis was arbitrary, this says the total angular momentum \\(\\mathbf{L} = const\\). That is, the angular momentum of the particle is conserved. Equivalently, if space is isotropic, then the total angular momentum will be conserved.\n\nWhat about energy though? When is it conserved? It turns out that energy conservation is connected to time translation invariance, which is a little bit more subtle. Suppose a Lagrangian had a time translational symmetry of the form \\(t(s) = t + s\\). Since all of \\(L\\), \\(\\mathbf{q}\\), and \\(\\mathbf{\\dot q}\\) depend explicitly on time, this means we’d have \\[\nL\\big(\\mathbf{q}(t), \\mathbf{\\dot q}(t), t\\big) = L\\big(\\mathbf{q}(t + s), \\mathbf{\\dot q}(t + s), t + s\\big).\n\\] Following along the proof of Noether’s Theorem, we’d have \\[\n0 = \\frac{\\partial L}{\\partial s} = \\frac{\\partial L}{\\partial t} \\frac{\\partial t}{\\partial s} = \\frac{\\partial L}{\\partial t}.\n\\] That is, \\(\\frac{\\partial L}{\\partial t} = 0\\), which means now time is cyclic, and the Lagrangian doesn’t have any explicit time dependence. Now, if we take the total time derivative of the Lagrangian, we get \\[\n\\begin{align*}\n\\frac{dL}{dt} &= \\frac{\\partial L}{\\partial \\mathbf{q}} \\cdot \\mathbf{\\dot q} + \\frac{\\partial L}{\\partial \\mathbf{\\dot q}} \\cdot \\mathbf{\\ddot q} + \\frac{\\partial L}{\\partial t} \\\\\n&= \\frac{d}{dt} \\frac{\\partial L}{\\partial \\mathbf{\\dot q}} \\cdot \\mathbf{\\dot q} + \\frac{\\partial L}{\\partial \\mathbf{\\dot q}} \\cdot \\mathbf{\\ddot q} + \\frac{\\partial L}{\\partial t} \\\\\n&= \\mathbf{\\dot p} \\cdot \\mathbf{\\dot q} + \\mathbf{p} \\cdot \\mathbf{\\ddot q} + \\frac{\\partial L}{\\partial t} \\\\\n&= \\frac{d}{dt} \\mathbf{p} \\cdot \\mathbf{\\dot q} + \\frac{\\partial L}{\\partial t}.\n\\end{align*}\n\\] When time is cyclic, we must have \\[\nH \\equiv \\mathbf{p} \\cdot \\mathbf{\\dot q} - L = const.\n\\] This function is called the Hamiltonian. We’ve just shown that when a system is time translation invariant, its Hamiltonian must be conserved, whatever that is.\nTo see what exactly \\(H\\) is, let’s consider a Lagrangian of the form \\[\nL = T - V = \\frac{1}{2} \\mathbf{\\dot q} \\cdot \\mathbf{T}(\\mathbf{q}) \\cdot \\mathbf{\\dot q} - V(\\mathbf{q}).\n\\] In this case, the Hamiltonian would be \\[\n\\begin{align*}\nH &= \\frac{\\partial L}{\\partial \\mathbf{\\dot q}} \\cdot \\mathbf{\\dot q} - L \\\\\n&= \\mathbf{\\dot q} \\cdot \\mathbf{T}(\\mathbf{q}) \\cdot \\mathbf{\\dot q} - \\frac{1}{2} \\mathbf{\\dot q} \\cdot \\mathbf{T}(\\mathbf{q}) \\cdot \\mathbf{\\dot q} + V(\\mathbf{q}) \\\\\n&= \\frac{1}{2} \\mathbf{\\dot q} \\cdot \\mathbf{T}(\\mathbf{q}) \\cdot \\mathbf{\\dot q} + V(\\mathbf{q}) \\\\\n&= T + V. \\\\\n\\end{align*}\n\\] That is, if \\(L = T-V\\), then \\(H=T+V\\). But \\(T+V\\) is just the total energy \\(E\\). We thus finally have the conservation of energy. If \\(L = T-V\\) and time is homogeneous, then the total energy \\(E\\) is conserved."
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#non-conservative-forces",
    "href": "classical-mechanics/lagrangian-mechanics.html#non-conservative-forces",
    "title": "Lagrangian Mechanics",
    "section": "Non-Conservative Forces",
    "text": "Non-Conservative Forces\nAs we’ve derived them, Lagrange’s equations only hold for systems with conservative forces. In some special cases, we can augment non-conservative forces into Lagrange’s equations as well. Suppose for example we had \\(L = T - U\\), where \\(U = U(\\mathbf{q}, \\mathbf{\\dot q}, t)\\) is some kind of generalized potential energy. Then the generalized forces have the form \\[\n\\mathbf{Q} = - \\frac{\\partial U}{\\partial \\mathbf{q}} + \\frac{d}{dt} \\frac{\\partial U}{\\partial \\mathbf{\\dot q}}.\n\\] In this case, we can solve Lagrange’s equations for \\(L = T-U\\) and everything would work fine even though \\(U\\) is not a proper potential energy anymore.\n\n\nExample: Charged Particle in an Electromagnetic Field\nSuppose a particle of mass \\(m\\) and charge \\(q\\) is moving in the presence of an electromagnetic field. We already know that such a particle obeys the Lorentz force law \\[\n\\mathbf{F} = q \\mathbf{E}(\\mathbf{x},t) + \\frac{q}{c} \\mathbf{v} \\times \\mathbf{B}(\\mathbf{x},t).\n\\] Can we derive this from a Lagrangian? Notice that this force isn’t conservative since it’s a function of the particle’s velocity. However, we can still derive a generalized potential energy \\(U=U(\\mathbf{x},\\mathbf{v},t)\\) as follows. We know from electrodynamics that we can express the electric field \\(\\mathbf{E}\\) and magnetic field \\(\\mathbf{B}\\) as derivatives of a scalar potential \\(\\phi\\) and a vector potential \\(\\mathbf{A}\\), \\[\n\\begin{align*}\n\\mathbf{E}(\\mathbf{x},t) &= - \\nabla \\phi(\\mathbf{x},t) - \\frac{1}{c} \\frac{\\partial}{\\partial t} \\mathbf{A}(\\mathbf{x},t), \\\\\n\\mathbf{B}(\\mathbf{x},t) &= \\nabla \\times \\mathbf{A}(\\mathbf{x},t). \\\\\n\\end{align*}\n\\] Let’s define \\[\nU(\\mathbf{x},\\mathbf{v},t) = q \\phi(\\mathbf{x},t) - \\frac{q}{c} \\mathbf{v} \\cdot \\mathbf{A}(\\mathbf{x},t).\n\\] Then the Lagrangian for this system is then given by \\(L=T-U\\), or \\[\nL = \\frac{1}{2} m \\mathbf{v}^2 - q \\phi + \\frac{q}{c} \\mathbf{v} \\cdot \\mathbf{A}.\n\\] Though a little painful, it’s not hard to show that solving Lagrange’s equations reproduces the above Lorentz force law.\n\nMore generally, we can always just manually add in any non-conservative forces to the equations of motion after Lagrange’s equations have been solved, but they’d need to be converted to generalized forces first. Given some force \\(\\mathbf{F}\\), we can augment Lagrange’s equations by writing \\[\n\\frac{\\partial L}{\\partial\\mathbf{q}} - \\frac{d}{dt}\\frac{\\partial L}{\\partial\\mathbf{\\dot q}} = \\mathbf{Q},\n\\] where \\(\\mathbf{Q} = \\mathbf{F} \\cdot \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{q}}\\) is the associated generalized force.\nIn the special case that a non-conservative force is linear in the generalized velocities, we can augment the Lagrangian by defining the Rayleigh dissipation function \\(\\mathcal{F}\\) given by \\[\n\\mathcal{F} \\equiv \\frac{1}{2} \\mathbf{\\dot q} \\cdot \\mathbf{B}(\\mathbf{q}) \\cdot \\mathbf{\\dot q} = \\sum_{i,j} b_{ij}(\\mathbf{q}) \\dot q_i \\dot q_j.\n\\] In this case, the generalized forces are just the gradients of \\(\\mathcal{F}\\), \\[\n\\mathbf{Q} = -\\frac{\\partial \\mathcal{F}}{\\partial \\mathbf{q}} = -\\mathbf{B} \\cdot \\mathbf{\\dot q},\n\\] which we can plug into the modified Lagrange’s equations to give \\[\n\\frac{\\partial L}{\\partial\\mathbf{q}} - \\frac{d}{dt}\\frac{\\partial L}{\\partial\\mathbf{\\dot q}} = \\frac{\\partial \\mathcal{F}}{\\partial\\mathbf{\\dot q}}.\n\\]\n\n\n\nExample: Damped Hanging Spring\nSuppose we have a spring with mass \\(m\\) and spring constant \\(k\\) allowed to hang from the top of a closed lid of thick syrup. Assume the viscosity of the syrup is high enough that Stoke’s Law holds.\nIn this case, we simply have \\[\n\\begin{align*}\nT &= \\frac{1}{2} m \\dot x^2, \\\\\nV &= -mgx - \\frac{1}{2} kx^2, \\\\\nL &= \\frac{1}{2} m \\dot x^2 + mgx + \\frac{1}{2} kx^2, \\\\\n\\mathcal{F} &= \\frac{1}{2} b \\dot x^2. \\\\\n\\end{align*}\n\\] Plugging these into the modified Lagrange’s equations finally gives the DDHO equation of motion \\[\nm \\ddot x + b \\dot x + kx = mg.\n\\]"
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#invariant-transformations",
    "href": "classical-mechanics/lagrangian-mechanics.html#invariant-transformations",
    "title": "Lagrangian Mechanics",
    "section": "Invariant Transformations",
    "text": "Invariant Transformations\nIt’s natural to ask what kinds of transformations of a Lagrangian leave it invariant. We already saw that when a system has certain symmetries that the Lagrangian will be invariant under those symmetry transformations. In that case, the Lagrangian itself didn’t change. More general, we could ask what kinds of transformations will leave the equations of motion unchanged, even if the Lagrangian itself did change.\nOne natural question to ask is how the Lagrangian behaves under a coordinate transformation. After all, the choice of coordinates should not affect the physical behavior of the system. Suppose we have two sets of coordinates to describe the system, \\(\\mathbf{q}\\) and \\(\\mathbf{Q}\\). They’re related by a transformation \\(\\mathbf{Q} = \\mathbf{Q}(\\mathbf{q})\\), called a point transformation. The transformed velocities \\(\\mathbf{\\dot Q}\\) are a function of both \\(\\mathbf{q}\\) and \\(\\mathbf{\\dot q}\\), since \\[\n\\mathbf{\\dot Q} = \\mathbf{\\dot Q}(\\mathbf{q}, \\mathbf{\\dot q}) = \\mathbf{\\dot q}\\frac{\\partial \\mathbf{Q}}{\\partial \\mathbf{q}}.\n\\] Suppose the Lagrangian in each coordinate system is given by \\(L_q = L(\\mathbf{q}, \\mathbf{\\dot q}, t)\\) and \\(L_Q = L(\\mathbf{Q}, \\mathbf{\\dot Q}, t)\\) respectively. Assume that \\(L_q\\) satisfies Lagrange’s equations, so \\[\n\\frac{\\partial L_q}{\\partial \\mathbf{q}} + \\frac{d}{dt} \\frac{\\partial L_q}{\\partial \\mathbf{\\dot q}} = \\mathbf{0}.\n\\] What then can we conclude about \\(L_Q\\)? If we express \\(L_Q\\) in terms of \\(\\mathbf{q}\\), we’d have \\[\nL_Q = L\\big(\\mathbf{q}(\\mathbf{Q}), \\mathbf{\\dot q}(\\mathbf{Q}, \\mathbf{\\dot Q}), t\\big).\n\\] Taking derivatives of \\(L_Q\\) with respect to \\(\\mathbf{Q}\\), then \\[\n\\begin{align*}\n\\frac{\\partial L_Q}{\\partial \\mathbf{Q}} &=  \\frac{\\partial \\mathbf{q}}{\\partial \\mathbf{Q}} \\cdot \\frac{\\partial L_q}{\\partial \\mathbf{q}} + \\frac{\\partial \\mathbf{\\dot q}}{\\partial \\mathbf{Q}} \\cdot \\frac{\\partial L_q}{\\partial \\mathbf{\\dot q}} \\\\\n&= \\frac{\\partial \\mathbf{q}}{\\partial \\mathbf{Q}} \\cdot \\frac{d}{dt} \\frac{\\partial L_q}{\\partial \\mathbf{\\dot q}} + \\frac{d}{dt} \\frac{\\partial \\mathbf{q}}{\\partial \\mathbf{Q}} \\cdot \\frac{\\partial L_q}{\\partial \\mathbf{\\dot q}} \\\\\n&= \\frac{d}{dt} \\bigg(\\frac{\\partial \\mathbf{q}}{\\partial \\mathbf{Q}} \\cdot \\frac{\\partial L_q}{\\partial \\mathbf{\\dot q}}\\bigg) \\\\\n&= \\frac{d}{dt} \\bigg(\\frac{\\partial \\mathbf{\\dot q}}{\\partial \\mathbf{\\dot Q}} \\cdot \\frac{\\partial L_q}{\\partial \\mathbf{\\dot q}}\\bigg) \\\\\n&= \\frac{d}{dt} \\frac{\\partial L_Q}{\\partial \\mathbf{\\dot Q}}.\n\\end{align*}\n\\] Thus, if \\(L_q\\) satisfies Lagrange’s equations with respect to \\(\\mathbf{q}\\), then \\(L_Q\\) satisfies Lagrange’s equations with respect to \\(\\mathbf{Q}\\). That is, the equations of motion are invariant to point transformations of the coordinates.\nThis fact means we’re essentially free to write the Lagrangian of a system in whatever set of coordinates we wish. The underlying physics will stay the same. In relativistic language, this result says that the Lagrangian is a proper scalar. It doesn’t transform under coordinate transformations.\nIt’s also natural to ask if we can add functions to a Lagrangian in a way that leave the equations of motion invariant. This leads to the notion of a gauge transformation. To leave the equations of motion invariant, it’s important that any such transformation leave the action stationary.\nSuppose we transformed a valid Lagrangian by adding a total time derivative to it, \\[\n\\tilde L(\\mathbf{q}, \\mathbf{\\dot q}, t) = L(\\mathbf{q}, \\mathbf{\\dot q}, t) + \\frac{d}{dt} F(t).\n\\] If we assume the original equations of motion satisfy the principle of least action, we have \\(\\delta S=0\\), where \\[\nS = \\int_{t_1}^{t_2} L(\\mathbf{q}, \\mathbf{\\dot q}, t) dt.\n\\] Evidently, the modified action \\(\\tilde S\\) has the form \\[\n\\begin{align*}\n\\tilde S &= \\int_{t_1}^{t_2} \\tilde L(\\mathbf{q}, \\mathbf{\\dot q}, t) dt \\\\\n&= \\int_{t_1}^{t_2} \\bigg(L(\\mathbf{q}, \\mathbf{\\dot q}, t) +  \\frac{d}{dt} F(t) \\bigg)dt \\\\\n&= \\int_{t_1}^{t_2} L(\\mathbf{q}, \\mathbf{\\dot q}, t)dt + F(t) \\bigg|_{t_1}^{t_2} \\\\\n&= S + \\Delta F.\n\\end{align*}\n\\] Since \\(\\Delta F\\) is a constant that depends only on the endpoints, we must have \\[\n\\delta S = 0 \\quad \\Longleftrightarrow \\quad \\delta\\tilde S = 0.\n\\] That is, adding a total time derivative to the Lagrangian leaves the equations of motion invariant.\n\n\nExample: Gauge Transformations\nIn electrodynamics, Maxwell’s Equations are known to be invariant under a change of gauge. We can add the derivative of any scalar field \\(\\lambda(\\mathbf{x},t)\\) to the electromagnetic potentials in the following way and leave Maxwell’s Equations invariant, \\[\n\\begin{align*}\n\\phi'(\\mathbf{x},t) &= \\phi(\\mathbf{x},t) - \\frac{1}{c} \\frac{\\partial}{\\partial t} \\lambda(\\mathbf{x},t), \\\\\n\\mathbf{A}'(\\mathbf{x},t) &= \\mathbf{A}(\\mathbf{x},t) + \\nabla \\lambda(\\mathbf{x},t). \\\\\n\\end{align*}\n\\] Suppose we had a particle moving in the presence of an electromagnetic field. We already showed such a Lagrangian would have the form \\[\nL = \\frac{1}{2} m \\mathbf{v}^2 - q \\phi(\\mathbf{x},t) + \\frac{q}{c} \\mathbf{v} \\cdot \\mathbf{A}(\\mathbf{x},t).\n\\] Let’s ask what happens to the Lagrangian if we gauge-transform the potentials. Evidently, \\[\n\\begin{align*}\nL' &= \\frac{1}{2} m \\mathbf{v}^2 - q \\phi' + \\frac{q}{c} \\mathbf{v} \\cdot \\mathbf{A}' \\\\\n&= \\frac{1}{2} m \\mathbf{v}^2 - q \\bigg(\\phi - \\frac{1}{c} \\frac{\\partial \\lambda}{\\partial t} \\bigg) + \\frac{q}{c} \\mathbf{v} \\cdot (\\mathbf{A} + \\nabla \\lambda) \\\\\n&= \\bigg(\\frac{1}{2} m \\mathbf{v}^2 - q\\phi + \\frac{q}{c}\\mathbf{v} \\cdot \\mathbf{A}\\bigg) + \\frac{q}{c} \\bigg(\\mathbf{v} \\cdot \\nabla \\lambda + \\frac{\\partial \\lambda}{\\partial t}\\bigg) \\\\\n&= L + \\frac{q}{c} \\frac{d\\lambda}{dt}.\n\\end{align*}\n\\] Since the gauge-transformed Lagrangian is just the original Lagrangian plus a total time derivative, we can conclude the the Lorentz force law must be gauge invariant as well. Of course, this was already obvious from the fact that \\(\\mathbf{E}\\) and \\(\\mathbf{B}\\) were gauge invariant in Maxwell’s equations."
  },
  {
    "objectID": "classical-mechanics/lagrangian-mechanics.html#examples",
    "href": "classical-mechanics/lagrangian-mechanics.html#examples",
    "title": "Lagrangian Mechanics",
    "section": "Examples",
    "text": "Examples\nIt’s good to get very comfortable being able to find the equations of motion of systems using Lagrange’s equations. Here are some more complicated examples, many of which would be highly non-trival to solve using Newton’s Laws.\n\n\nExample: Uniform Rod on a Frictionless Table\n\n\n\n\n\n\n\n\nExample: Atwood Machine\n\n\n\n\n\n\n\n\nExample: Particle on a Cylinder\n\n\n\n\n\n\n\n\nExample: Block Sliding on Moving Wedge\n\n\n\n\n\n\n\n\nExample: Bead on a Wire"
  },
  {
    "objectID": "classical-mechanics/coupled-oscillations.html#one-dimensional-systems",
    "href": "classical-mechanics/coupled-oscillations.html#one-dimensional-systems",
    "title": "Coupled Oscillations",
    "section": "One-Dimensional Systems",
    "text": "One-Dimensional Systems\nConsider a system with one degree of freedom \\(q\\) in the presence of a potential energy \\(V(q)\\). We say such a system has an equilibrium point at \\(q=q_0\\) provided \\[\nF = -\\frac{dV}{dq} \\bigg|_{q=q_0} = 0.\n\\] Equivalently, \\(q_0\\) is an equilibrium point if it’s a stationary point of \\(V(q)\\). We say \\(q_0\\) is stable if it’s a local minimum of \\(V(q)\\), unstable if it’s a local maximum of \\(V(q)\\), and semi-stable if it’s a saddlepoint. In general, a potential energy \\(V(q)\\) may have many different equilibrium points.\n\n\n\n\n\nNow, if we expand \\(V(q)\\) in a Taylor Series about \\(q_0\\), we get \\[\nV(q) = V(q_0) + \\frac{dV}{dq}\\bigg|_{q_0} (q - q_0) + \\frac{1}{2} \\frac{d^2 V}{dq^2}\\bigg|_{q_0} (q - q_0)^2 + O\\big((q-q_0)^3\\big).\n\\] Since only differences in potential energy can affect the dynamics of a system, we can suppose without loss of generality that \\(V(q_0) = 0\\). Since \\(q_0\\) is an equilibrium point, we must also have \\(\\frac{dV}{dq}\\big|_{q_0} = 0\\). We’re thus left with \\[\nV(q) = \\frac{1}{2} \\frac{d^2 V}{dq^2}\\bigg|_{q_0} (q - q_0)^2 + O\\big((q-q_0)^3\\big).\n\\] We can always re-center the system so that \\(q_0=0\\). If we define \\(k \\equiv \\frac{d^2 V}{dq^2}\\big|_{q_0}\\), we evidently have \\[\nV(q) = \\frac{1}{2} k q^2.\n\\] But this is just the potential energy for Hooke’s Law, since \\(F = -\\frac{dV}{dq} = -kq\\). We’ve thus evidently defined Hooke’s Law from the assumption that a system is undergoing small motions near an equilibrium point.\nMotions will only be small oscillations if the equilibrium point \\(q_0\\) is stable, which is equivalent to requiring that \\(k > 0\\) since \\(V(q_0)\\) is locally convex. If \\(k < 0\\) the motion will be unstable since \\(V(q_0)\\) is locally concave. If \\(k=0\\) we run into a special case where we have to consider higher orders in the Taylor Series expansion. In this case, motion will be very near constant around \\(q_0\\), but can either grow or decay as \\(q\\) gets farther from \\(q_0\\).\nWe can also plug \\(V(q)\\) into the Lagrangian and get \\[\nL \\approx \\frac{1}{2}m \\dot q^2 - \\frac{1}{2} k q^2,\n\\] which is of course just the Lagrangian for SHO. We’ve thus derived the following important fact: Any 1-dimensional system undergoing small oscillations near a stable equilibrium point can be well-approximated by a simple harmonic oscillator.\n\n\nExample: Kepler Orbits\nRecall for Kepler orbits we have a Lagrangian \\(L = \\frac{1}{2}m\\dot r^2 - V_{eff}(r)\\), where \\[\nV_{eff}(r) = \\frac{\\ell^2}{2mr^2} - \\frac{GMm}{r}.\n\\] This effective potential has a stable equilibrium when the orbits are circular, i.e. \\(r=r_0\\).\nSetting the first derivative of \\(V_{eff}(q)\\) to zero gives \\[\n\\frac{dV_{eff}}{dr}\\bigg|_{r_0} = -\\frac{\\ell^2}{mr_0^3} + \\frac{GMm}{r_0^2} = 0 \\quad \\Longrightarrow \\quad r_0 = \\frac{\\ell^2}{GMm^2}.\n\\] Setting the second derivative to \\(k\\) gives \\[\nk = \\frac{d^2V_{eff}}{dr^2}\\bigg|_{r_0} = 3\\frac{\\ell^2}{mr_0^4} - 2\\frac{GMm}{r_0^3} = \\frac{GMm}{r_0^3} > 0.\n\\] Thus, the Kepler orbit undergoes stable oscillations about the point \\(r=r_0\\), with a force law \\(F(r) \\approx -k(r-r_0)\\). The oscillation frequency and period are given by \\[\n\\omega = \\sqrt{\\frac{k}{m}} = \\sqrt{GM}{r_0^3} \\quad \\Longrightarrow \\quad \\tau = \\frac{2\\pi}{\\sqrt{GM}} r_0^{3/2},\n\\] which is just Kepler’s Third Law for circular orbits.\n\n\n\nExample: Two Coupled Springs\nBefore deriving the general form for the solution of coupled linear systems, let’s try to solve the problem of two springs attached to each other in sequence. Assume both masses have mass \\(m\\). Assume the springs attached to the walls have spring constant \\(k\\), and the coupling spring constant between the two masses is \\(k_{12}\\).\n\n\n\n\n\nDenote the position of mass one relative to its equilibrium as \\(x_1\\), and the position of mass two relative to its equilibrium by \\(x_2\\). Then the Lagrangian is \\[\nL = \\frac{1}{2} m (\\dot x_1^2 + \\dot x_2^2) - \\frac{1}{2}(kx_1^2 + k_{12}(x_2-x_1)^2 + kx_2^2).\n\\] The equations of motion are thus given by \\[\n\\begin{align*}\nm \\ddot x_1 &= -kx_1 + k_{12}(x_2 - x_1), \\\\\nm \\ddot x_2 &= -kx_1 - k_{12}(x_2 - x_1).\n\\end{align*}\n\\] This is a coupled system of two linear differential equations. To solve, let’s suppose that both solutions are sinusoidal with the same frequency \\(\\omega\\), say \\(x_1 = A_1 \\cos\\omega t\\) and \\(x_2 = A_2 \\cos\\omega t\\). Then we have"
  },
  {
    "objectID": "classical-mechanics/coupled-oscillations.html#general-problem",
    "href": "classical-mechanics/coupled-oscillations.html#general-problem",
    "title": "Coupled Oscillations",
    "section": "General Problem",
    "text": "General Problem\nLet’s now consider a system with \\(n\\) degrees of freedom \\(q_1, q_2, \\cdots, q_n\\) given by a Lagrangian \\[\nL = \\frac{1}{2} \\dot q_i T_{ij}(q_1,\\cdots,q_n) \\dot q_j - V(q_1,\\cdots,q_n).\n\\] Let’s re-write this in vector notation by defining \\(\\mathbf{q} \\equiv (q_1,q_2,\\cdots,q_n)\\) and \\(\\mathbf{T} \\equiv (T_{ij})\\). Then we have \\[\nL = \\frac{1}{2}\\mathbf{\\dot q}^\\top \\mathbf{T}(\\mathbf{q}) \\mathbf{\\dot q} - V(\\mathbf{q}).\n\\] Now, suppose \\(\\mathbf{q}_0\\) is an equilibrium point of the system. Since the kinetic energy depends on \\(\\mathbf{q}\\) we’ll have to Taylor expand the entire Lagrangian about \\(\\mathbf{q}_0\\). For \\(V(\\mathbf{q})\\) we have \\[\nV(\\mathbf{q}) = V(\\mathbf{q}_0) + \\nabla V^\\top(\\mathbf{q}_0) (\\mathbf{q}-\\mathbf{q}_0) + \\frac{1}{2} (\\mathbf{q}-\\mathbf{q}_0)^\\top \\mathbf{H}(\\mathbf{q_0}) (\\mathbf{q}-\\mathbf{q}_0) + O\\big(||\\mathbf{q}-\\mathbf{q}_0||^3\\big).\n\\] Again, only differences in potential energy matter, so we can define \\(V(\\mathbf{q}_0) = 0\\). Furthermore, since \\(\\mathbf{q}_0\\) is an equilibrium point, we must have \\(\\nabla V(\\mathbf{q}_0) = \\mathbf{0}\\). Let’s define \\(\\mathbf{K} \\equiv \\mathbf{H}(\\mathbf{q}_0)\\). Then, to second-order in \\(\\mathbf{q}-\\mathbf{q}_0\\) we have \\[\nV(\\mathbf{q}) \\approx \\frac{1}{2} (\\mathbf{q}-\\mathbf{q}_0)^\\top \\mathbf{K} (\\mathbf{q}-\\mathbf{q}_0).\n\\] For the kinetic energy term, expanding \\(\\mathbf{T}(\\mathbf{q})\\) about \\(\\mathbf{q}_0\\) in a similar manner gives \\[\n\\mathbf{T}(\\mathbf{q}) = \\mathbf{T}(\\mathbf{q}_0) + O\\big(||\\mathbf{q}-\\mathbf{q}_0|| \\big).\n\\] Defining \\(\\mathbf{M} \\equiv \\mathbf{T}(\\mathbf{q}_0)\\) and dropping terms of higher order, we have \\(\\mathbf{T}(\\mathbf{q}) \\approx \\mathbf{M}\\). Plugging both of these terms into the Lagrangian and keeping only terms quadratic in \\(\\mathbf{q}\\) and \\(\\mathbf{q}_0\\), we finally have, \\[\nL \\approx \\frac{1}{2} \\mathbf{\\dot q}^\\top \\mathbf{M} \\mathbf{\\dot q} - \\frac{1}{2} (\\mathbf{q}-\\mathbf{q}_0)^\\top \\mathbf{K} (\\mathbf{q}-\\mathbf{q}_0).\n\\] This is the most general form of the Lagrangian for a many-body mechanical system when expanded to quadratic order about an equilibrium point. Most of the time we’ll want to re-center so that \\(\\mathbf{q}_0 = \\mathbf{0}\\). In that case, the Lagrangian reduces to just \\[\nL \\approx \\frac{1}{2} \\mathbf{\\dot q}^\\top \\mathbf{M} \\mathbf{\\dot q} - \\frac{1}{2} \\mathbf{q}^\\top \\mathbf{K} \\mathbf{q}.\n\\] Notice this looks exactly like the scalar Lagrangian for SHO, \\(L = \\frac{1}{2}m \\dot q^2 - \\frac{1}{2} k q^2\\), except everything is in matrix-vector notation now.\nWe can solve Lagrange’s equations in matrix-vector notation now, \\[\n\\frac{dL}{d\\mathbf{q}} + \\frac{d}{dt}\\frac{dL}{d\\mathbf{\\dot q}} = \\mathbf{0}.\n\\] Solving this system simply gives the vector equations of motion \\[\n\\mathbf{M}\\mathbf{\\ddot q} = -\\mathbf{K}\\mathbf{q},\n\\] which is the \\(n\\)-dimensional generalization of Hooke’s Law.\nAssuming \\(\\mathbf{M}\\) is invertible, we can define \\(\\mathbf{\\Omega}^2 \\equiv \\mathbf{M}^{-1} \\mathbf{K}\\), and write \\[\n\\mathbf{\\ddot q} = -\\mathbf{\\Omega}^2 \\mathbf{q}.\n\\] The nature of the solutions will depend on the definiteness of \\(\\mathbf{\\Omega}^2\\). Evidently, if \\(\\mathbf{\\Omega}^2\\) is positive definite, the solutions will be stable. If \\(\\mathbf{\\Omega}^2\\) is negative definite, the solutions will be unstable.\nSkip to the rest of the theory before doing more examples…\nPer ChatGPT: The general solution to the coupled oscillator \\(M \\ddot x = -K x\\) in closed form can be written as \\[\nx(t) = V \\cos(\\Omega t) c + V \\sin(\\Omega t) d,\n\\] where \\(V\\) is the matrix of eigenvectors of \\(\\Omega^2 = M^{-1} K\\), and \\(c, d\\) are initial condition vectors.\nVerify this!!!"
  },
  {
    "objectID": "circuits/circuit-abstraction.html#maxwells-equations",
    "href": "circuits/circuit-abstraction.html#maxwells-equations",
    "title": "The Lumped Circuit Abstraction",
    "section": "Maxwell’s Equations",
    "text": "Maxwell’s Equations\nMaxwell’s Equations are taught in electrodynamics courses. In SI units, they’re given as follows.\n\n\n\n\n\n\n\n\nName\nDifferential Form\nIntegral Form\n\n\n\n\nGauss’s Law\n\\(\\nabla \\cdot \\mathbf{E} = \\frac{\\rho}{\\varepsilon_0}\\)\n\\(\\oint \\mathbf{E} \\cdot d\\mathbf{a} = \\frac{q}{\\varepsilon_0}\\)\n\n\nFaraday’s Law\n\\(\\nabla \\times \\mathbf{E} = -\\frac{\\partial \\mathbf{B}}{\\partial t}\\)\n\\(\\oint \\mathbf{E} \\cdot d\\boldsymbol{\\ell} = -\\frac{\\partial \\Phi_M}{\\partial t}\\)\n\n\nNo Magnetic Monopoles\n\\(\\nabla \\cdot \\mathbf{B} = 0\\)\n\\(\\oint \\mathbf{B} \\cdot d\\mathbf{a} = 0\\)\n\n\nAmpere’s Law\n\\(\\nabla \\times \\mathbf{B} = \\mu_0 \\mathbf{J} + \\frac{1}{\\varepsilon_0 \\mu_0} \\frac{\\partial \\mathbf{E}}{\\partial t}\\)\n\\(\\oint \\mathbf{B} \\cdot d\\boldsymbol{\\ell} = \\mu_0 I + \\frac{1}{\\varepsilon_0 \\mu_0} \\frac{\\partial \\Phi_E}{\\partial t}\\)\n\n\nContinuity Equation\n\\(\\frac{\\partial \\rho}{\\partial t} - \\nabla \\cdot \\mathbf{J} = 0\\)\n\\(\\frac{\\partial q}{\\partial t} - \\oint \\mathbf{J} \\cdot d\\mathbf{a} = 0\\)\n\n\n\nRather than use these equations directly we’ll derive three simpler laws that hold for circuits:\n\nOhm’s Law: \\(v = iR\\).\nKirchoff’s Voltage Law (KVL): \\(\\sum_{loop} v = 0\\).\nKirchoff’s Current Law (KCL): \\(\\sum_{node} i = 0\\)."
  },
  {
    "objectID": "circuits/circuit-abstraction.html#ohms-law",
    "href": "circuits/circuit-abstraction.html#ohms-law",
    "title": "The Lumped Circuit Abstraction",
    "section": "Ohm’s Law",
    "text": "Ohm’s Law\nFor many materials, a linear relation holds between the electric field \\(\\mathbf{E}\\) inside the material and its current density \\(\\mathbf{J}\\). This is the Generalized Ohm’s Law: \\[\\mathbf{E} = \\rho \\mathbf{J},\\] where \\(\\rho\\) is the material’s resistivity. Consider a piece of cylindrical material, called a resistor, with a current \\(i\\) pumped through its ends.\n\n\n\n\n\nSince \\(\\mathbf{E} = E \\mathbf{e}_y\\) and \\(\\mathbf{J} = J \\mathbf{e}_y\\), and \\(A\\) and \\(\\ell\\) are constant, we have\n\\[\\begin{align*}\n\ni &= \\oint \\mathbf{J} \\cdot d\\mathbf{a} = J \\cdot A, \\\\\\\n\nv &= \\oint \\mathbf{E} \\cdot d\\mathbf{\\ell} = E \\cdot l.\n\n\\end{align*}\\]\nThus, we have \\(v = \\frac{\\rho \\ell}{A}i \\equiv Ri\\), where \\(R \\equiv \\frac{\\rho \\ell}{A}\\) is a constant, called the resistence of the material. The relation then becomes \\[v = iR,\\] which is the standard Ohm’s Law. Note Ohm’s Law as stated is only true for resistive materials."
  },
  {
    "objectID": "circuits/circuit-abstraction.html#the-lumped-circuit-abstraction",
    "href": "circuits/circuit-abstraction.html#the-lumped-circuit-abstraction",
    "title": "The Lumped Circuit Abstraction",
    "section": "The Lumped Circuit Abstraction",
    "text": "The Lumped Circuit Abstraction\nTo easily and reliably analyze circuits we make a number of simplifying assumptions, or abstractions. By restricting ourselves to situations where these abstractions hold, we set up a simpler playground in which to work.\nThe most fundamental abstraction in circuit analysis is the lumped circuit abstraction or LCA. In the LCA, we assume a circuit is made of a set of lumped elements that are connected to each other with ideal wires (i.e. wires with no voltage drop across any two points and a uniform current throughout).\nAs an example, let’s consider a lightbulb connected to a battery supplying a voltage \\(v\\), which causes a current \\(i\\) to flow across the bulb from the positive terminal of the battery to the negative terminal.\n\n\n\n\n\nWe’d like to solve for the current \\(i\\) as a function of the input voltage \\(v\\). How should we do this? The hard way would be to just use Maxwell’s Equations. But this is unnecessary.\nNotice that we don’t care about many of the physical properties of the circuit, including the bulb’s shape, temperature, filament design, or what the wires are made of. We only care about the bulb’s resistance, since Ohm’s law says \\(v=iR\\). We can thus abstract the details of the bulb and the battery away, treating the bulb as a resistor and the battery as a voltage source.\n\n\n\n\n\nOnce we’ve done this, we can simply solve for the current in terms of the voltage simply as \\[i = \\frac{v}{R}.\\]\nA more abstract way to express this simple circuit is to use special symbols for the resistor and the voltage source. We’d write the exact same setup like this.\n\n\n\n\n\nNow, how do we know we can do this? How do we even know that \\(v\\) and \\(i\\) are even defined? After all, neither voltage nor current need exist in a well-defined way. However, under certain conditions, they do exist. Consider the following setup, where a current \\(i\\) flows through a wire from \\(A\\) to \\(B\\). The voltage across the wire is \\(v\\). The cross-sectional areas through \\(A\\) and \\(B\\) are \\(s_A\\) and \\(s_B\\), respectively.\n\n\n\n\n\nBy the continuity equation, we have \\[i_A - i_B \\equiv \\int_{s_A} \\mathbf{J} \\cdot d\\mathbf{a} - \\int_{s_B} \\mathbf{J} \\cdot d\\mathbf{a} = \\frac{\\partial q}{\\partial t}.\\] Provided no charge can build up inside the wire, we have \\[\\frac{\\partial q}{\\partial t} = 0 \\Rightarrow i_A = i_B \\equiv i.\\] That is, we have a well-defined current \\(i\\) flowing through the wire provided we forbid a buildup of charge inside the wire.\nBy Faraday’s Law, we also have \\[v_A - v_B \\equiv \\int_A^B \\mathbf{E} \\cdot d \\boldsymbol{\\ell} = -\\frac{\\partial \\Phi_M}{\\partial t}.\\] Provided magnetic flux is constant outside the wires, we can conclude \\[\\frac{\\partial \\Phi_M}{\\partial t} = 0 \\Rightarrow v_A = v_b \\equiv v.\\] That is, we have a well-defined voltage \\(v\\) across the wire provided we forbid any change in magnetic flux outside the wire.\nThe last condition we must require is that currents move much slower than the speed of light. This says that currents aren’t allowed to radiate.\nThe requirement that circuits obey each of these properties is called the lumped matter discipline:\n\nElements are discrete and independent of each other.\nNo charge can build up inside of wires.\nMagnetic flux is constant outside of the circuit.\nCurrents must move much slower than the speed of light."
  },
  {
    "objectID": "circuits/circuit-abstraction.html#lumped-elements",
    "href": "circuits/circuit-abstraction.html#lumped-elements",
    "title": "The Lumped Circuit Abstraction",
    "section": "Lumped Elements",
    "text": "Lumped Elements"
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#thermodynamic-systems",
    "href": "statistical-mechanics/thermodynamics.html#thermodynamic-systems",
    "title": "Thermodynamics",
    "section": "Thermodynamic Systems",
    "text": "Thermodynamic Systems\nIn thermodynamics we seek to describe the macroscopic properties of a thermodynamic system. Unlike in classical mechanics, we won’t think of a system as a particle or a collection of particles, but rather as an object describable by a set of macroscopic properties. By macroscopic we mean properties that describe the state of the entire system, like its temperature, pressure, volume, etc. These properties are called state variables or thermodynamic coordinates.\nWe’ll think of two distinct types of thermodynamic systems:\n\nAn adiabatic system: A system isolated by “walls” that don’t allow heat to flow in or out.\nA diathermic system: A system with “permeable walls”, where heat is allowed to flow in or out.\n\nA diathermic system in which no particles are allowed to flow in or out is a closed system.\n\n\nWe say an adiabatic system is in equilibrium when its macroscopic properties have had sufficient time to relax to constant steady state values. By “constant”, we mean the properties don’t change appreciably over some given observation time. The specific properties we seek to measure depend on the type of system under consideration. Here are some examples of mechanical properties that might depend on the system:\n\nGas in a container: We might be interested in its volume or the pressure it exerts on the container.\nA wire under tension: We might be interested in its length or the tension forces exerted on it.\nA magnet in a field: We might be interested in its magnetization or its external magnetic field.\n\nOn top of these mechanical properties that vary by system, we also may be interested in a system’s thermal properties, i.e. properties that arise due to the system’s internal interactions. The macroscopic thermal properties might be temperature, entropy, or heat."
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#the-zeroth-law",
    "href": "statistical-mechanics/thermodynamics.html#the-zeroth-law",
    "title": "Thermodynamics",
    "section": "The Zeroth Law",
    "text": "The Zeroth Law\nSuppose we have three distinct systems \\(A\\), \\(B\\), and \\(C\\). The zeroth law of thermodynamics states that if \\(A\\) is in equilibrium with \\(B\\), and \\(B\\) is in equilibrium with \\(C\\), then \\(A\\) is in equilibrium with \\(C\\). That is, the property of equilibrium is transitive.\n\n\n\n\n\nNotice that for any two systems to be in equilibrium with each other they must be allowed to exchange heat. If they were isolated, even the smallest change to one system wouldn’t affect the other. The zeroth law evidently implies that this holds for any number of systems in equilibrium. You can’t isolate any one from the other, since heat can always flow through any two pairs of systems in equilibrium with each other.\nThe zeroth law implies the existence of a thermal quantity called the empirical temperature that’s the same among systems in equilibrium, a quantity that doesn’t change when no net heat is flowing between any two systems.\nTheorem: Suppose two systems \\(A\\) and \\(B\\) are in thermodynamic equilibrium with each other. Suppose \\(A\\) has thermodynamic coordinates \\(A_1, A_2, \\cdots, A_n\\) and \\(B\\) has thermodynamic coordinates \\(B_1, B_2, \\cdots, B_m\\). Then there exists a value \\(\\theta\\), called the empirical temperature, that depends only on the state of each system, and in equilibrium satisfies the property that for some functions of the coordinates \\[\n\\theta = \\theta_A(A_1, A_2, \\cdots, A_n) = \\theta_B(B_1, B_2, \\cdots, B_m).\n\\] Proof: Suppose a third system \\(C\\) is in equilibrium with \\(A\\) and \\(B\\) with coordinates \\(C_1, C_2, \\cdots, C_k\\). Since \\(A\\) is in equilibrium with \\(C\\), there must be some function of constraint \\(f_{AC}\\) such that \\[\nf_{AC}(A_1, A_2, \\cdots, A_n, C_1, C_2, \\cdots, C_k) = 0.\n\\] Similarly, if \\(B\\) is in thermal equilibrium with \\(C\\) then there is some other constraint function \\(f_{BC}\\) such that \\[\nf_{BC}(B_1, B_2, \\cdots, B_m, C_1, C_2, \\cdots, C_k) = 0.\n\\] Now, we can imagine solving for each function in terms of a common variable \\(C_1\\) to get new functions \\[\n\\begin{align*}\nC_1 &= g_{AC}(A_1, A_2, \\cdots, A_n, C_2, \\cdots, C_k), \\\\\nC_1 &= g_{BC}(B_1, B_2, \\cdots, B_m, C_2, \\cdots, C_k). \\\\\n\\end{align*}\n\\] Setting these two functions equal thus says that \\[\ng_{AC}(A_1, A_2, \\cdots, A_n, C_2, \\cdots, C_k) - g_{BC}(B_1, B_2, \\cdots, B_m, C_2, \\cdots, C_k) = 0.\n\\] By the zeroth law, we also know that \\(A\\) must be in thermal equilibrium with \\(B\\). This means there’s yet another function \\(f_{AB}\\) such that \\[\nf_{AB}(A_1, A_2, \\cdots, A_n, B_1, B_2, \\cdots, B_m) = 0.\n\\] Taken together, this says that we can take \\(f_{AB}\\) and spread it out into two functions \\(g_{AC}\\) and \\(g_{BC}\\), where \\(g_{AC}\\) depends only on the coordinates of \\(A\\) and \\(C\\), and \\(g_{BC}\\) depends only on the coordinates \\(B\\) and \\(C\\). If we imagine using the coordinates of \\(C\\) as some kind of reference values we can treat them as constants. That means we’re left with an expression of the form \\[\ng_{AC}(A_1, A_2, \\cdots, A_n, \\text{const}) - g_{BC}(B_1, B_2, \\cdots, B_k, \\text{const}) = 0.\n\\] This says we have a function of \\(A\\) that must equal a similar function of \\(B\\) at thermal equilibrium, \\[\n\\theta \\equiv \\theta_A(A_1, A_2, \\cdots, A_n) = \\theta_B(B_1, B_2, \\cdots, B_m). \\qquad \\text{Q.E.D.}\n\\] The empirical temperature is evidently reference dependent since we had to fix values for some third system \\(C\\) just to properly define it. We can choose \\(C\\) to be anything we like as long as we agree on a convention. The most common is the triple point of water, the state where water coexists in its gas, liquid, and solid forms simultaneously. This occurs at a temperature of about \\(T = 273.16 \\ \\degree \\text{K}\\) and pressure of about \\(p = 0.006 \\text{ atm}\\).\n\n\n\n\n\nThe condition that \\(\\theta = \\theta_A\\) says that in the space of coordinates of \\(A\\), in thermal equilibrium the system must be constrained to a surface of constant \\(\\theta_A = \\theta\\). This surface is called an isotherm, a surface of constant temperature. Similarly for \\(B\\).\nAnalogy: Think of defining temperature similarly to how one might empirically define mass by using a scale. You first establish a reference mass \\(C\\), for example some standard block of metal in a vault, and then use that to talk about how much the masses \\(A\\) and \\(B\\) weigh in units of \\(C\\).\n\nThe Ideal Gas\nOne practically useful way to define an empirical temperature scale uses the properties of the ideal gas. An ideal gas is a large number of dilute particles that satisfy the property that the product of the gas’s pressure \\(p\\) and volume \\(V\\) is proportional temperature in the dilute limit, i.e. \\[\n\\lim_{V \\rightarrow \\infty} pV = \\lim_{p \\rightarrow 0} pV \\propto \\theta.\n\\] We’ll assume the gas is allowed to interact diathermally with its environment, called a heat bath. That is, the gas is allowed to exchange energy with its environment, but nothing else.\nSuppose now that we submerge the gas in one heat bath and record values for \\(p, V, \\theta\\) once the system has reached equilibrium. Then, we take the gas and submerge it again in a different reference heat bath. Once the system has again reached equilibrium, we again record the new values \\(p_0, V_0, \\theta_0\\). Now, since the gas is ideal, we must have \\[\n\\frac{\\theta}{\\theta_0} = \\frac{pV}{p_0 V_0}.\n\\] Provided we’ve fixed a value for \\(\\theta_0\\), we can thus define the temperature \\(T\\) of the system by \\[\nT \\equiv \\theta \\equiv \\theta_0 \\frac{pV}{p_0 V_0} = \\frac{\\theta_0}{p_0} \\frac{pV}{V_0}.\n\\] In the Kelvin scale, \\(\\theta_0\\) and \\(p_0\\) are again defined by the triple point of water. This means that to measure the temperature, we’d need to first measure the pressure and volume of the gas in the heat bath of interest, and then compare that with the volume the same gas would have at the triple point.\nFor an ideal gas, we evidently have the relation then that \\(pV \\propto T\\). It turns out that \\(pV\\) is also proportional to the number of particles \\(N\\) in the gas, \\(pV \\propto N\\). We can write the full ideal gas law in the form \\[\npV = Nk_B T,\n\\] where \\(k_B\\) is a proportionality constant, called the Boltzmann constant. Its value is measured to be \\[\nk_B = 1.381 \\cdot 10^{-23} \\frac{\\text{J}}{\\degree \\text{K}} \\approx \\frac{1}{40} \\frac{\\text{eV}}{\\degree \\text{K}}.\n\\]"
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#the-first-law",
    "href": "statistical-mechanics/thermodynamics.html#the-first-law",
    "title": "Thermodynamics",
    "section": "The First Law",
    "text": "The First Law\nIn classical mechanics the conservation of energy is a fundamental principle of a microscopic system. We’d like to extend this idea to thermodynamics as well. Observations indicate that a similar principle operates at the level of macroscopic systems provided that the system is properly insulated, that is, when the only sources of energy are of mechanical origin.\n\nWork, Energy, Heat\nSuppose a thermodynamic system \\(A\\) is adiabatically isolated from its environment. If such a system is changed by some amount of work \\(\\Delta W\\), then the amount of work is only dependent on its initial and final state. That is, if \\(a_i=(A_{1,i}, \\cdots, A_{n,i})\\) is the initial state and \\(a_f=(A_{1,f}, \\cdots, A_{n,f})\\), then \\[\n\\Delta W = \\Delta E = E(a_f) - E(a_i),\n\\] where \\(E = E(a)\\) is some scalar function of state called the internal energy of the system. It’s the total energy of the system \\(A\\) when it’s adiabatically isolated.\nHaving a system be adiabatically isolated is a strong assumption that we’d like to relax, but doing so then means the system can exchange energy with its environment, which means \\(\\Delta W \\neq \\Delta E\\). There’s a flow of energy \\(\\Delta Q\\) in and out of the system now, called the heat. It’s the heat plus the work that’s conserved, \\[\n\\Delta E = \\Delta Q + \\Delta W.\n\\] This experimental fact is called the first law of thermodynamics. We assume this quantity called heat exists in such a fashion that the internal energy stays conserved. It’s not a theorem.\nAside: Note the work \\(\\Delta W\\) is done on the system, not by the system. Many engineering texts adopt the opposite convention, where they like to think of work being done by the system (e.g. by an engine). In that case, the \\(\\Delta W\\) would change its sign, in which case we’d write \\(\\Delta E = \\Delta Q - \\Delta W\\).\nSince thermodynamic state variables only make sense when a system is in equilibrium, if we want to think about the first law in differential form we have to imagine we can change the system differentially in such a way that it stays in equilibrium. Doing so is called a quasi-static process. We vary the system very slowly from its initial to its final state, allowing the system to come to equilibrium again at each point. This allows us to fill in the path continuously with points so we can then talk about differential changes.\nIn differential form, the first law of thermodynamics has the form \\[\ndE = \\delta Q + \\delta W.\n\\] The notation \\(\\delta Q\\) and \\(\\delta W\\) is used to make it explicit that those variables are path dependent. That is, they’re not a function of only the initial and final states. This means we can’t integrate them directly to get the total heat or work done. However, the energy is a state variable, it is a function only of its end points, and so we can integrate \\(dE\\) to get the total internal energy \\(\\Delta E\\), \\[\n\\Delta E = \\int dE = \\int (\\delta Q + \\delta W).\n\\]\n\n\nTypes of Work\nThe work done on the system is inherently mechanical in that it’s a sum of forces times displacements. Since we want to imagine generalized forces and generalized displacements, we’ll write it in the notation \\[\n\\delta W = \\sum_i J_i d q_i,\n\\] where \\(J_i\\) is a generalized force conjugate to some generalized displacement variable \\(q_i\\). Here are some of the most common conjugate force-displacement pairs:\n\n\n\n\n\n\n\n\nSystem\nGeneralized Force: \\(J\\)\nGeneralized Displacement: \\(q\\)\n\n\n\n\nWire\nTension: \\(F\\)\nLength: \\(L\\)\n\n\nFilm\nSurface Tension: \\(\\sigma\\)\nArea: \\(A\\)\n\n\nFluid\nPressure: \\(-p\\)\nVolume: \\(V\\)\n\n\nMagnet\nMagnetic Field: \\(B\\)\nMagnetization: \\(M\\)\n\n\nDielectric\nElectric Field: \\(E\\)\nPolarization: \\(P\\)\n\n\nChemical Reaction\nChemical Potential: \\(\\mu\\)\nParticle Number: \\(N\\)\n\n\n\nThe generalized forces have the property that their values are independent of the size of the system. Doubling the size of the system doesn’t double the forces acting on it. These are called intensive variables. Conversely, the generalized displacements are directly proportional to the size of the system. If the system’s size is doubled, so too are the displacements. These are called extensive variables. Intensive and extensive variables always tend to occur in conjugate pairs like this.\nUsing the new notation, we can re-write the first law in the form \\[\ndE = \\delta Q + \\sum_{i=1}^k J_i dq_i.\n\\] For reasons we’ll get into soon, it’s also convenient to break up the work component into non-chemical and chemical work components. If we explicitly split off the chemical work terms, we’d instead write \\[\ndE = \\delta Q + \\sum_{i=1}^n J_i dq_i + \\sum_{j=1}^m \\mu_i dN_i.\n\\] Of course, we still don’t know how to simplify \\(\\delta Q\\) into a useful form. We’ll deal with that soon.\n\n\nHeat Capacities\nSuppose we pump some amount of heat \\(\\delta Q\\) into the system. Provided heat is a function of temperature, we’d have \\(\\Delta Q = C \\Delta T\\), where \\(C = C(T)\\) is some function of temperature, called the heat capacity. The functional form of \\(C\\) depends on the nature of the system. Evidently, the heat capacity is given by \\[\nC(T) \\equiv \\frac{\\delta Q}{dT}.\n\\] Since heat is path dependent, the heat capacity must be too. If \\(\\gamma\\) is some path taken to get from the initial to the final point in state space, then we might write \\(C = C_\\gamma\\) to be explicit about this. The most important case is when we’re dealing with a gas. If a gas is only has work done \\(\\delta W = -pdV\\), then we can think of the gas as only being a function of two state variables, \\(p\\) and \\(V\\). Two paths of interest in \\(pV\\)-space are paths of constant \\(p\\) or \\(V\\). Using the first law, the heat capacity \\(C_V\\) at constant volume is evidently \\[\nC_V = \\frac{\\delta Q_V}{dT} = \\frac{dE + pdV}{dT} \\bigg |_V = \\frac{\\partial E}{\\partial T}\\bigg |_V.\n\\] Similarly, the heat capacity \\(C_p\\) at constant pressure is evidently \\[\nC_p = \\frac{\\delta Q_p}{dT} = \\frac{dE + pdV}{dT} \\bigg |_p =  \\frac{\\partial E}{\\partial T}\\bigg |_p + p \\frac{\\partial V}{\\partial T} \\bigg |_p.\n\\] It turns out that the two \\(\\frac{\\partial E}{\\partial T}\\) derivatives are the same. This follows empirically from the Joule Free Expansion Experiment: Suppose we have two chambers connected by a thin hole that’s initially closed. Initially, all the gas is in the left chamber at an equilibrium temperature \\(T\\). Suppose the hole is then suddenly opened, allowing the gas to adiabatically expand into the right chamber.\n\n\n\n\n\nSince the gas isn’t pushing on anything, it can’t do any work. Since the process is adiabatic, no heat is flowing either. This means the total energy isn’t changing either. Once the system has settle down to equilibrium, the temperature in the two chambers must be the same. This evidently implies the energy must be a function of temperature alone, i.e. \\[\nE = E(T) = E(pV).\n\\] Using this fact, for an ideal gas we can evidently write \\[\nC_p - C_V = p \\frac{\\partial V}{\\partial T} \\bigg |_p.\n\\] Since \\(V = \\frac{Nk_B T}{p}\\), this reduces to just \\[\nC_p - C_V = N k_B.\n\\] It turns out that in fact \\(E \\propto pV\\). This result is called the equipartition theorem. It must be taken as an empirical law in thermodynamics, but it can be proven with statistical mechanics. The equipartition theorem says that an ideal gas whose individual particles each have \\(n\\) degrees of freedom will have a total energy given by \\[\nE = \\frac{n}{2} Nk_B T = \\frac{n}{2} pV.\n\\] For example, a monoatomic gas is a gas whose particles only have \\(n=3\\) translational degree of freedom. In that case, we’d have \\(E = \\frac{3}{2} pV\\). A diatomic gas is a gas whose particles also have two rotational degrees of freedom, giving \\(n=3+2=5\\) total degrees of freedom, and \\(E = \\frac{5}{2} pV\\).\nAside: Suppose you wanted to know about how fast particles in a gas were moving. If the gas is roughly ideal, you can use the following rule of thumb: Since the kinetic energy per particle is \\(\\varepsilon=\\frac{1}{2}mv^2\\), the total energy is just \\(E = N\\varepsilon = \\frac{N}{2}mv^2\\). By the equipartition theorem, \\(E = \\frac{n}{2} Nk_B T\\). Equating both terms and solving for the velocity \\(v\\) gives \\[\nv = \\sqrt{\\frac{nk_B T}{m}}.\n\\] Note this formula is only approximately true, in that it actually gives an estimate of the RMS velocity.\nUsing the equipartition theorem, we can find the heat capacity of an ideal gas directly. Since \\[\n\\frac{d E}{d T} = \\frac{n}{2} Nk_B,\n\\] we evidently have \\[\nC_V = \\frac{n}{2} Nk_B, \\quad C_p = \\bigg(\\frac{n}{2}+1\\bigg) Nk_B.\n\\] Notice that these heat capacities are extensive since they’re both proportional to \\(N\\). In practice we’re interested in an intensive measure of how responsive heat is to changes in temperature. We can achieve this by dividing by \\(N\\) to get a specific heat. More commonly, specific heats are measured per unit mass, not per particle*. If the system has mass \\(m\\), its specific heat capacity is defined by \\(c \\equiv \\frac{C}{m}\\).\nUsually it’s the specific heats that are tabulated for various substances. We’d need to look them up to do any kind of numerical calculations. The most useful specific heat to remember is the specific heat of water at standard temperature and pressure or STP, i.e. \\(p=1 \\text{ atm}, T=298 \\ \\degree K\\). In energy units of calories, the specific heat of water at STP is just \\(c_p = 1 \\ \\frac{\\mathrm{cal}}{\\mathrm{g} \\mathrm{\\degree K}}\\). Note that the specific heat does depend on the phase of a substance. For example, ice has a specific heat of \\(c_p = 0.5 \\ \\frac{\\mathrm{cal}}{\\mathrm{g} \\mathrm{\\degree K}}\\).\nAnother important quantity that’s similar to the specific heat is the latent heat. It’s an intensive measure of how much heat is needed for a system to fully undergo a phase change, \\[\nL \\equiv \\frac{\\Delta Q}{m}.\n\\] In general, latent heat values will be different than the specific heat values. They’ll also be different for different phase changes. For example, the latent heat of melting ice is \\(L = 80 \\ \\frac{\\text{cal}}{g}\\), while the latent heat of boiling water is \\(L = 540 \\ \\frac{\\text{cal}}{g}\\). Again, we’d look these up in tables when we need them."
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#the-second-law",
    "href": "statistical-mechanics/thermodynamics.html#the-second-law",
    "title": "Thermodynamics",
    "section": "The Second Law",
    "text": "The Second Law\nWe saw that were able to break the work \\(\\delta W\\) up into a sum of generalized force-displacement pairs as \\(\\delta W = \\sum J_i dq_i\\). We’d like to be able to break up the heat \\(\\delta Q\\) somehow. It’s reasonable to assume that \\(T\\) is the generalized force for heat, but what is the generalized displacement? This leads us to the second law and the concept of entropy.\n\nEngines\nThe second law of thermodynamics arose historically out of an interest among engineers in converting back and forth between heat and mechanical work. A device that converts heat into mechanical work is called an engine. A device that converts mechanical work into heat is called a heat pump or a refrigerator (the difference between the two being whether we want to pump heat into or out of a system).\nSuppose an engine takes in heat \\(Q_H\\) from a heat reservoir, outputs some amount of work \\(W\\), and dumps any remaining output heat \\(Q_C\\) into a cold reservoir. By the first law, \\(Q_H = Q_C + W\\). Define the efficiency \\(\\eta\\) of the engine as the ratio of work extracted to the total amount of heat put in, \\[\n\\eta \\equiv \\frac{W}{Q_H}.\n\\] Since \\(W = Q_H - Q_C\\), we can also write the efficiency as \\[\n\\eta = 1 - \\frac{Q_C}{Q_H}.\n\\] Since we must have \\(Q_C \\leq Q_H\\) by the first law, this means \\(0 \\leq \\eta \\leq 1\\) generally speaking. A perfectly efficient engine would convert all heat into work, in which case \\(\\eta = 1\\).\nWe can define a similar measure of efficiency for a heat pump or a refrigerator. In that case, we’re interested in how much heat we can extract per unit work put in. This measure is called the coefficient of performance \\(\\omega\\), given by \\(\\omega_{fr} \\equiv \\frac{Q_C}{W}\\) for a refrigerator, and \\(\\omega_{hp} \\equiv \\frac{Q_H}{W}\\) for a heat pump. Again using the fact that \\(Q_H = Q_C + W\\), it’s easy to show that \\(\\omega_{fr} \\geq 1\\) and \\(0 \\leq \\omega_{hp} \\leq 1\\).\nHere’s a diagram showing the difference between an engine and a refrigerator. Notice that the refrigerator is just an engine with the arrows reversed. We’ll exploit this fact a good bit shortly.\n\n\n\n\n\n\n\nThe Second Law\nThe second law of thermodynamics can be stated in several ways that are all equivalent. I’ll state it first using the definition given by Kelvin, and then use that to prove it’s equivalent to a different statement made by Clausius.\nSecond Law (Kelvin): No thermodynamic process is possible whose sole result is the complete conversion of heat to work or work to heat. Equivalently, there is no ideal engine with efficiency \\(\\eta = 1\\).\nSecond Law (Clausius): No thermodynamic process is possible whose sole result is the transfer of heat from a colder body to a hotter body. Equivalently, there is no ideal refrigerator with performance \\(\\omega = \\infty\\).\nProof: We’ll prove these are equivalent by showing if Kelvin is false, then so is Clausius, and vice versa.\n\nIf Kelvin is false, so is Clausius: If Kelvin is false, then there exists an engine with that outputs heat \\(Q\\) to work \\(W\\) with 100% efficiency. We can use this \\(W\\) to then power a refrigerator. Suppose the refrigerator pumps heat \\(Q_C\\) from a cold reservoir to a new heat \\(Q_H\\) that dumps into the same hot reservoir as the engine. Then on net we have a system that pumps in a heat \\(Q_C\\) and pumps out a heat \\(Q_H-Q\\). That is, we’ve built a refrigerator that pumps heat from a cold body to a warm body, violating Clausius. \\(\\text{Q.E.D.}\\)\n\n\n\n\n\nIf Clausius is false, so is Kelvin: If Clausius is false, then it’s possible to build a heat pump to pump heat from a cold reservoir to a hot reservoir with no input work required. Let’s hook an engine up to the same reservoir, taking an input heat \\(Q_H\\) from the hot reservoir and converting it to some combination of work \\(W\\) and output heat \\(Q_C\\). Then on net we have a system that takes in heat \\(Q_H-Q\\) and converts it purely into work, i.e. \\(W = Q_H - Q\\), which violates Kelvin. \\(\\text{Q.E.D.}\\)\n\n\n\n\n\n\n\n\nThe Carnot Engine\nIf we can’t have an engine with perfect efficiency, what’s the highest possible efficiency we can possibly have? As we’ll soon prove, the highest efficiency engine is a Carnot engine. A Carnot engine is defined to be any engine that’s reversible, runs in a cycle, and whose reservoir temperatures are held fixed, with the hot reservoir at \\(T_H\\) and the cold reservoir at \\(T_C\\).\n\n\n\n\n\nA thermodynamic process is called reversible if it can be run backward in time by simply reversing the inputs and outputs. It’s the thermodynamic equivalent of frictionless motion in classical mechanics. Since reversibility implies the system stays in equilibrium, reversible processes must be quasi-static. However, not all quasi-static processes need be reversible. Any process that dissipates energy to its environment, even if done quasi-statically, is not reversible.\n\nExample: Carnot Cycle of an Ideal Gas\nTo make the topic somewhat more concrete, suppose we have an ideal gas inside a piston, consisting of a single type of molecule with no exchange of particles taking place. Then the only work being done is the work done by the piston to change the volume \\(V\\) and pressure \\(p\\) of the gas. Then by the first law, \\[\ndE = \\delta Q + \\delta W = \\delta Q - pdV.\n\\] This means the state variables are \\((p, V)\\). In \\(pV\\)-space, the Carnot engine will be a cycle consisting of two isotherms at \\(T_H\\) and \\(T_C\\) that are connected by curves where \\(\\delta Q = 0\\), called adiabatics.\nSuppose a cycle starts on the upper left point, say \\((p_A, V_A)\\). It expands isothermally to \\((p_B, V_B)\\), then adiabatically expands to \\((p_C, V_C)\\), then isothermally compresses to \\((p_D, V_D)\\), before finally adiabatically compressing back to \\((p_A, V_A)\\).\n\n\n\n\n\nAlong the isotherms, the ideal gas law says the curves must be hyperbolas, \\[\npV = N k_B T_H, \\quad pV = N k_B T_C.\n\\] Along the adiabatics, the condition \\(\\delta Q = 0\\) along with the equipartition theorem implies \\[\ndE = \\frac{n}{2} d(pV) = -pdV \\quad \\Longrightarrow \\quad \\bigg(\\frac{n}{2}+1\\bigg) pdV = -\\frac{n}{2} Vdp.\n\\] This is a differential equation for \\(p(V)\\). Using separation of variables on both sides gives \\[\npV^\\gamma = p_0 V_0^\\gamma = const, \\quad \\text{where} \\quad \\gamma \\equiv \\frac{2}{n}\\bigg(\\frac{n}{2}+1\\bigg).\n\\] For example, with a monoatomic gas we’d have \\(\\gamma = \\frac{5}{3}\\), so the adiabatics are \\(pV^{5/3} = const\\). For the two adiabatic curves in the cycle, taking \\((p_0, V_0)\\) to be the two initial points along the curves gives \\[\npV^\\gamma = p_B V_B^\\gamma, \\quad pV^\\gamma = p_D V_D^\\gamma.\n\\] Note that since the Carnot cycle is reversible, the total work done during a full cycle is zero.\n\nTheorem: Of all engines operating between two reservoir temperatures \\(T_H\\) and \\(T_C\\), the Carnot engine is the most efficient.\nProof: Suppose we had some arbitrary non-Carnot engine with efficiency \\(\\eta\\) that takes in heat \\(Q_H'\\) from the hot reservoir, generates work \\(W\\), and dumps the remaining heat \\(Q_C'\\) into the cold reservoir. Using the same trick, hook a reversed Carnot engine (i.e. a Carnot refrigerator) up to take in the output work \\(W\\) and use it to pump heat \\(Q_C\\) from the cold reservoir to a heat \\(Q_H\\) in the hot reservoir. On net, this gives a cycle that takes in heat \\(Q_H'-Q_H\\) and converts it to heat \\(Q_C'-Q_C\\) .\n\n\n\n\n\nBut by the second law, we must have \\(Q_H'-Q_H \\geq Q_C'-Q_C\\). Dividing both sides by \\(W\\) and reorganizing, we get \\[\n\\eta = \\frac{W}{Q_H'} \\leq \\frac{W}{Q_H} = \\eta_{carnot}.\n\\] That is, the Carnot engine is more efficient. \\(Q.E.D.\\)\nCorollary: All Carnot engines between \\(T_H\\) and \\(T_C\\) have the same efficiency.\nProof: Follow the previous proof, but this time hook up another Carnot engine to the Carnot refrigerator to get \\(\\eta = \\eta_{carnot}\\). \\(Q.E.D.\\)\nWe can use the Carnot engine to construct yet another temperature scale, except this time we can do it without reference to any material properties at all. This is called the thermodynamic temperature scale. What we can do is hook two Carnot engines up in series as follows. Suppose a Carnot engine \\(CE_1\\) takes heat from \\(T_1\\) to \\(T_2\\), and Carnot engine \\(CE_2\\) takes heat from \\(T_2\\) to \\(T_3\\). We can also think of the whole thing as a single Carnot engine \\(CE\\) that takes heat from \\(T_1\\) to \\(T_3\\).\n\n\n\n\n\nNow, if we look at the heat output for each engine, we have \\[\n\\begin{align*}\nCE_1: Q_2 &= Q_1 - W_{12} = Q_1(1 - \\eta_{12}), \\\\\nCE_2: Q_3 &= Q_2 - W_{23} = Q_2(1 - \\eta_{23}), \\\\\nCE: Q_3 &= Q_1 - W_{13} = Q_1(1 - \\eta_{13}). \\\\\n\\end{align*}\n\\] We can equate both terms for \\(Q_3\\) and simplify to get \\[\n1 - \\eta_{13} = (1 - \\eta_{12})(1 - \\eta_{23}).\n\\] Now, if we divide both sides by \\(1 - \\eta_{23}\\) we get \\[\n1 - \\eta_{12} = \\frac{Q_2}{Q_1} = \\frac{f(T_1)}{f(T_2)}.\n\\] The system must satisfy this constraint for any function \\(f(T)\\) we choose. We might as well just choose \\(f(T) \\equiv T\\), in which case we get \\[\n\\eta_{12} = 1 - \\frac{T_2}{T_1}.\n\\] That is, any Carnot engine between \\(T_H\\) and \\(T_C\\) must have an efficiency given by \\[\n\\eta = 1 - \\frac{T_C}{T_H}.\n\\] Notice that since \\(T_C < T_H\\), the Carnot efficiency can never be \\(1\\). For reasonable temperature ranges, say from freezing to boiling at STP, we’d have \\(\\eta \\approx 0.268\\). That’s under 27% efficiency! In fact, the Carnot engine, while the best we can do efficiency-wise, it’s not practical for real engines. One major reason for this is that isothermal processes are really slow, meaning it takes too impractically long to complete a single cycle.\nSince the Carnot efficiency \\(\\eta\\) between two temperatures is fixed, we can use it to define a temperature scale provided we fix a base temperature \\(T_0\\). We can define the temperature \\(T\\) as the value that gives a Carnot efficiency \\(\\eta\\) between \\(T\\) and \\(T_0\\). That is, \\[\nT \\equiv T_0 (1 - \\eta).\n\\] The thermodynamic definition also implies that temperature \\(T\\) must be positive. If we had \\(T < 0\\), then an engine operating between it and a positive temperature could extract heat from both reservoirs and convert the sum total to work, in violating of the second law.\n\n\nEntropy\nWe’re finally ready to construct the state function that’s conjugate to temperature. Let’s look again at the previous theorem that said \\(\\eta \\leq \\eta_{carnot}\\) for any engine between \\(T_H\\) and \\(T_C\\). We can rewrite this inequality in the form \\[\n\\frac{W}{Q_H} = 1 - \\frac{Q_C}{Q_H} \\leq 1 - \\frac{T_C}{T_H}.\n\\] Rearranging both sides, we get \\[\n\\frac{Q_H}{T_H} - \\frac{Q_C}{T_C} \\leq 0.\n\\] What’s interesting to notice here is that the quantity \\(\\frac{Q}{T}\\), whatever it is, depends only on the initial and final points. That is, it’s a state function. In fact, the above statement is extremely general.\nClausius’ Theorem: For any cyclic process (not necessarily quasi-static), if \\(\\delta Q\\) is an increment of heat delivered to a system at some temperature \\(T\\), then the sum total ratio of heat to temperature across the entire cycle is negative, i.e. \\[\n\\oint \\frac{\\delta Q}{T} \\leq 0.\n\\] Proof: What we’ll do is imagine pumping a heat increment \\(\\delta Q\\) into the system by hook a Carnot engine with hot reservoir temperature \\(T_0\\) , which takes input heat \\(\\delta Q_0\\) and uses that to generate some amount of work \\(\\delta W\\), expelling the remaining heat into the system as \\(\\delta Q\\) at a cold reservoir temperature \\(T\\).\n\n\n\n\n\nNow, since the engine is a Carnot engine, we have \\[\n1 - \\eta = \\frac{\\delta Q}{\\delta Q_0} = \\frac{T}{T_0} \\quad \\Longrightarrow \\quad \\delta Q_0 = T_0 \\frac{\\delta Q}{T}.\n\\] At the end of a full cycle, the net effect of the combined process is the extraction of heat \\(Q_0 = \\oint \\delta Q_0\\) from the hot reservoir, which is converted purely to external work \\(W = \\oint \\delta W\\). The total work \\(W\\) is the sum of the work done by the engine and the work done by the system. Now, by the second law, we must have \\(Q_0 = W \\leq 0\\), i.e. \\[\nQ_0 = T_0 \\oint \\frac{\\delta Q}{T} \\leq 0 \\quad \\Longrightarrow \\quad \\oint \\frac{\\delta Q}{T} \\leq 0. \\quad \\text{Q.E.D.}\n\\] Corollary: For a reversible process, we must have exact equality, i.e. \\[\n\\oint \\frac{\\delta Q_{rev}}{T} = 0.\n\\] Proof: This is easy to see. If we run the process forward we get \\(\\frac{\\delta Q_{rev}}{T} \\leq 0\\). By reversibility though, we can also run the process backwards, in which case \\(\\delta Q_{rev} \\rightarrow -\\delta Q_{rev}\\), and so \\(\\frac{\\delta Q_{rev}}{T} \\geq 0\\). This implies the integral between any two points \\(A\\) and \\(B\\) must be path independent, since for any two paths \\(\\mathcal{C}\\) and \\(\\mathcal{C}'\\), we have \\[\n\\int_A^B \\frac{\\delta Q_{rev}^{{\\mathcal{C}}}}{T_{{\\mathcal{C}}}} + \\int_B^A \\frac{\\delta Q_{rev}^{{\\mathcal{C}'}}}{T_{{\\mathcal{C}'}}} = 0 \\quad \\Longrightarrow \\quad \\oint \\frac{\\delta Q_{rev}}{T} = 0. \\quad \\text{Q.E.D.}\n\\] This corollary implies the existence a state function \\(S\\), defined by the path integral \\[\n\\Delta S = \\int_A^B \\frac{\\delta Q_{rev}}{T}.\n\\] This state function is called the entropy of the system. Since \\(\\delta Q_{rev} = TdS\\), we’ve finally found the conjugate to temperature we were seeking. It’s just the entropy. Plugging this into the first law, we have that for any quasi-static, reversible process in equilibrium, \\[\ndE = TdS + \\sum_{i=1}^n J_i dq_i + \\sum_{j=1}^m \\mu_i dN_i.\n\\] This formula is without doubt the most useful identity in thermodynamics. Notice that this implies that we only need \\(n+m+1\\) total quantities to completely specify the state of the system. We can obtain the rest by partial differentiation. Assuming the mechanical displacements are independent, we have \\[\nT = \\frac{\\partial E}{\\partial S} \\bigg |_{\\{q_i\\}}, \\quad J_i = \\frac{\\partial E}{\\partial q_i} \\bigg |_{S, \\ \\{q_k: \\ k \\neq i\\}, \\ \\{N_j\\}}, \\quad \\mu_j = \\frac{\\partial E}{\\partial q_i} \\bigg |_{S, \\ \\{q_i\\}, \\ \\{N_k: \\ k \\neq j\\}}.\n\\]\n\nExample: Entropy of a Monatomic Ideal Gas\nSuppose we have a monatomic ideal gas in a closed system with work \\(\\delta W = -pdV\\). Then \\[\ndE = TdS - pdV.\n\\] What is the change \\(\\Delta S\\) in the entropy along any path in \\(pV\\)-space?\nSolving for \\(dS\\) and using the fact that \\(dE = \\frac{3}{2} Nk_B dT\\) gives \\[\ndS = \\frac{1}{T}dE - \\frac{p}{T} dV = Nk_B \\bigg[\\frac{3}{2} \\frac{dT}{T} + \\frac{dV}{V} \\bigg].\n\\] Integrating both sides and simplifying terms, we finally have \\[\n\\Delta S = Nk_B \\bigg[\\frac{3}{2} \\log \\frac{T}{T_0} + \\log \\frac{V}{V_0} \\bigg] = Nk_B \\log\\bigg[ \\frac{V}{V_0} \\bigg(\\frac{T}{T_0}\\bigg)^{3/2} \\bigg].\n\\] It’s interesting to note from this formula that the entropy is extensive since it depends linearly on \\(N\\). It also seems to increase logarithmically with the volume and the temperature. Since \\(k_B\\) has units of energy over temperature, so too does the entropy.\nNotice this formula only gives the change in entropy. It doesn’t give a function for the entropy itself. For that we’d need a zero-point for \\(S\\), which comes from the third law of thermodynamics. It turns out that for an ideal gas, the entropy function \\(S\\) is given in exact form by the Sackur–Tetrode equation, \\[\nS = Nk_B \\log\\bigg[\\frac{V}{N} \\bigg(\\frac{mE}{3\\pi \\hbar^2 N}\\bigg)^{3/2} \\bigg] + \\frac{5}{2} Nk_B.\n\\] We’ll be able to derive this equation when we get to statistical mechanics.\n\nCorollary: For an irreversible process, we have the inequality \\[\n\\int_A^B \\frac{\\delta Q}{T} \\leq \\Delta S.\n\\] Proof: This proof is similar to the previous corollary. What we’ll do is close the cycle by taking a reversible process backwards, which by Clausius’ theorem gives \\[\n\\int_A^B \\frac{\\delta Q}{T} + \\int_B^A \\frac{\\delta Q_{rev}}{T} \\leq 0. \\quad \\text{Q.E.D.}\n\\] In differential form, this corollary implies that \\(dS \\geq \\frac{\\delta Q}{T}\\) for any transformation. Suppose we take some number of adiabatically isolated systems each in equilibrium and bring them all together to thermally interact. Such a system is called a closed system, in that the subsystems are allowed to interact thermally, but not exchange matter. Once the joint system has settled down to equilibrium, the total heat must still be \\(\\delta Q = 0\\), which means that \\(\\delta S \\geq 0\\).\nThis result implies that the net adiabatic system attains its maximum entropy at equilibrium, since any spontaneous change can only act to further increase \\(S\\). This implies that the second law is not time reversible. The direction of increasing entropy points out the arrow of time in its path to equilibrium.\nAnalogy: Compare the statement that entropy increases up to thermal equilibrium with a mechanical statement. Suppose we drop an object some distance above the Earth’s surface, allowing it to free fall under gravity. As the object falls, it will only settle down once it’s reached its mechanical equilibrium, when the total forces are zero. This happens when the potential energy is minimized. In this sense, the statement that entropy increases is no more mysterious than the observation that objects tend to fall downwards under gravity so as to minimize their potential energy."
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#thermodynamic-potentials",
    "href": "statistical-mechanics/thermodynamics.html#thermodynamic-potentials",
    "title": "Thermodynamics",
    "section": "Thermodynamic Potentials",
    "text": "Thermodynamic Potentials\nLet’s look more closely again at the differential of the energy. To keep notation compact, we’ll use \\(J, q, \\mu, N\\) to denote the abstract vectors of these quantities. Then in general we can write \\(dE\\) as \\[\ndE = TdS + J \\cdot dq + \\mu \\cdot dN.\n\\] This differential implies that \\(E = E(S, q, N)\\) explicitly, with \\(T, J, \\mu\\) determined implicitly by partial differentiation. Suppose, however, that we wanted the energy as an explicit function of other variables instead. For example, it may be easier to control the temperature or pressure of a gas in the lab than entropy or volume. We can go back and forth between conjugate pairs using Legendre transformations.\nSuppose we have some function \\(f(x, y)\\). Suppose \\(x\\) is conjugate to another variable \\(p\\) in the sense that \\[\ndf = pdx + vdy.\n\\] Notice if we add and subtract \\(xdp\\) to both sides and rearrange, we get a new differential of the form \\[\ndg \\equiv d(f-px) = -xdp + vdy.\n\\] This evidently defines a new function \\(g(p,y) = f(x,y) - px\\) that’s now an explicit function of \\(p\\) and \\(y\\). This new function is called a Legendre transformation of \\(f(x,y)\\). We created a new dual function by swapping \\(x\\) with its conjugate variable \\(p\\). This dual function is completely equivalent in content to the original function since we can always go back and forth between the two via the same kind of transformation.\nWe can apply the Legendre transformation to the energy \\(E=E(S,J,N)\\) to get the energy as a function of the other state variables. The only thing is that these new functions won’t be the original energy exactly, but rather shifted versions of the energy called thermodynamic potentials. In total there are four valid thermodynamic potentials other than the energy: enthalpy, Helmholtz free energy, Gibbs free energy, and the grand potential. Note that all of these potentials still have units of energy.\n\nEnthalpy\nSuppose we wanted to swap \\(J\\) with \\(q\\) to express the energy as a function \\(H = H(S, q, N)\\). We can figure out the form of \\(H\\) by doing a Legendre transformation between \\(J\\) and \\(q\\). Adding \\(q \\cdot dJ\\) to both sides of the first law and rearranging gives \\[\ndH = d(E - J \\cdot x) = TdS - q \\cdot dJ + \\mu \\cdot dN.\n\\] That is, the equation for \\(H\\) is evidently \\[\nH \\equiv E - J \\cdot x.\n\\] This function is called the enthalpy. We can think of it as a form of energy where the mechanical work gets subtracted out. When dealing with a gas, we’d have \\(J \\cdot dx = -pdV\\), in which case the enthalpy would be \\[\nH = E + pV.\n\\] The enthalpy is perhaps most useful when dealing with adiabatic systems. In that case, \\(\\delta Q = 0\\) means the enthalpy is just the work done, i.e. \\(dH = -q \\cdot dJ + \\mu \\cdot dN\\). Adiabatic processes tend to happen very quickly, like the combustion of gas in a cylinder.\n\n\nHelmholtz Free Energy\nSuppose now we wanted to instead swap \\(T\\) with \\(S\\) to get a function \\(F = F(T, q, N)\\). If we add and subtract \\(SdT\\) to both sides of \\(dE\\) and rearrange, we get \\[\ndF = -SdT + J \\cdot dq + \\mu \\cdot dN.\n\\] The function \\(F\\) is called the Helmholtz free energy, evidently given by \\[\nF = E - TS.\n\\] We can think of the Helmholtz free energy as a kind of energy in which the heat has been subtracted out of the system. The Helmholtz free energy is perhaps most useful when dealing with isothermal processes, in which case \\(dF\\) reduces to just \\(dF = J \\cdot dq + \\mu \\cdot dN\\). Isothermal processes happen very slowly, so slowly they’re impractical for real-world engines.\n\n\nGibbs Free Energy\nSuppose now we wanted to swap both \\(q\\) with \\(J\\) as well as \\(T\\) with \\(S\\) to get a function \\(G = G(T, J, N)\\). If we start with the enthalpy \\(dH\\) and add and subtract \\(SdT\\) to both sides and rearrange, we get \\[\ndG = d(H - TS) = -SdT - q \\cdot dJ + \\mu \\cdot dN.\n\\] The function \\(G\\) is called the Gibbs free energy, evidently given by \\[\nG = H - TS = E - TS - J \\cdot q.\n\\] We can think of the Gibbs free energy as a kind of energy in which both the mechanical work done as well as the heat have been subtracted out of the system. When dealing with a gas, \\(G\\) takes the form \\[\nG = E - TS + pV.\n\\] The Helmholtz free energy is perhaps most useful when dealing with processes that take place at fixed temperature and pressure, e.g. processes that take place at STP. These often include, for example, biological processes, like the thermodynamics in and around a cell.\n\n\nThe Grand Potential\nSo far we haven’t touched the chemical work terms at all. Suppose now though that we want a kind of Gibbs free energy that swaps \\(\\mu\\) with \\(N\\) instead of \\(J\\) with \\(q\\) to get a function \\(\\mathcal{G} = \\mathcal{G}(T,q,\\mu)\\). If we this time start with the Helmholtz free energy and add and subtract \\(N \\cdot d\\mu\\) to both sides and re-arrange, we get \\[\nd\\mathcal{G} = d(F - \\mu \\cdot N) = -SdT + J \\cdot dq - N \\cdot d\\mu.\n\\] This function \\(\\mathcal{G}\\) is called the grand potential, evidently given by \\[\n\\mathcal{G} = F - \\mu \\cdot N = E - TS - \\mu \\cdot N.\n\\] We can think of the grand potential as a kind of energy in which both the heat and the chemical work have been subtracted out of the system.\n\n\nExtensivity\nIf you look carefully, you’ll see that all of the thermodynamic potentials we defined are a function of at least one extensive variable. It’s fair to ask why we didn’t consider a potential function of all the intensive variables, i.e. some \\(L = L(T,J,\\mu)\\). The reason for this has to do with a mathematical relationship known as extensivity. We say a system is extensive if its energy satisfies the property of homogeneity. That is, for any scalar \\(\\lambda\\), we must have \\[\nE(\\lambda S, \\lambda q, \\lambda N) = \\lambda E(S, q, N).\n\\] Note that extensivity is not a required property of every thermodynamic system. It doesn’t follows from the laws of thermodynamics. It’s in fact an extra constraint that’s satisfied by most systems of real world interest. One example of a system that’s not extensive is a star where gravitational work is being done.\nWe can derive a useful relationship by differentiating both sides of this definition with respect to \\(\\lambda\\), \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\lambda} \\lambda E(S, q, N) &= \\frac{\\partial}{\\partial\\lambda} E(\\lambda S, \\lambda q, \\lambda N), \\\\\n\\Longrightarrow E(S, q, N) &= \\frac{\\partial E}{\\partial S} \\bigg |_{q,N} S + \\frac{\\partial E}{\\partial q} \\bigg |_{S,N} \\cdot q + \\frac{\\partial E}{\\partial N} \\bigg |_{S,q} \\cdot N, \\\\\n\\Longrightarrow E(S, q, N) &= TS + J \\cdot q + \\mu \\cdot N. \\\\\n\\end{align*}\n\\] That is, for an extensive system, the energy is just given directly by \\[\nE = TS + J \\cdot q + \\mu \\cdot N.\n\\] If we take the differential of both sides and apply the first law, we get \\[\n\\begin{align*}\ndE &= d(TS) + d(J \\cdot q) + d(\\mu \\cdot N) \\\\\n&= (TdS + J \\cdot dq + \\mu \\cdot dN) + (SdT + q \\cdot dJ + N \\cdot d\\mu) \\\\\n&= TdS + J \\cdot dq + \\mu \\cdot dN. \\\\\n\\end{align*}\n\\] This means the second term must be zero for an extensive system, \\[\nSdT + q \\cdot dJ + N \\cdot d\\mu = 0.\n\\] This relation is called the Gibbs-Dunham relation. Notice it’s just the differential of a function \\(L = L(T,J,\\mu)\\) of the intensive variables. We’ve thus shown that no thermodynamic potential of the intensive variables alone can exist for an extensive system.\nExtensivity gives us a new constraint that we can often use to solve problems. Here’s an example.\n\nExample: Chemical potential of an ideal gas along an isotherm\nSuppose we wanted to find \\(\\mu\\) for an ideal gas consisting of a single molecule. Since an ideal gas is extensive, along an isotherm we must have the simplified constraint \\[\n-Vdp + N d\\mu = 0.\n\\] Now, by the ideal gas law, \\(\\frac{V}{N} = \\frac{k_B T}{p}\\). We can thus re-write this expression as \\[\nd\\mu = \\frac{k_B T}{p} dp.\n\\] Integrating both sides and solve for \\(\\mu\\), we finally have that along an isotherm \\[\n\\mu = \\mu_0 + k_B T \\log \\frac{p}{p_0}.\n\\] Evidently, the chemical potential is an increasing function of temperature, pressure, and volume.\n\nThe Gibbs free energy can be used to give a useful interpretation of the chemical potential \\(\\mu\\) of a gas. By extensivity, we must have \\[\nG = E - TS - J \\cdot q = \\mu N.\n\\] That is, the chemical potential of a gas can be thought of as the Gibbs free energy per particle. If there is a mixture of \\(m\\) types of particles in the gas, then \\(\\mu_i\\) is the Gibbs free energy per particle \\(i\\)."
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#maxwell-relations",
    "href": "statistical-mechanics/thermodynamics.html#maxwell-relations",
    "title": "Thermodynamics",
    "section": "Maxwell Relations",
    "text": "Maxwell Relations\nRecall from calculus that for any function with continuous second partial derivatives, the mixed second partial derivatives commute. For example, a function \\(z = f(x,y)\\) would have \\[\n\\frac{\\partial^2 z}{\\partial x \\partial y} = \\frac{\\partial^2 z}{\\partial y \\partial x}.\n\\] We generally assume that the state functions in thermodynamics are sufficiently smooth enough that their mixed partial derivatives all commute like this. This condition imposes another set of constraints on the potentials, which we can use to find interesting, non-trivial relationships between various state variables. They’re called the Maxwell relations. If \\(dE = TdS + J \\cdot dq + \\mu \\cdot dN\\), then there are in total 3 Maxwell relations per potential, which means there are \\(3 \\cdot 5 = 15\\) relations across all 5 potentials, though some of these are duplicates. Here are the differential forms of all 5 potentials again, \\[\n\\begin{align*}\ndE &= \\quad TdS + J \\cdot dq + \\mu \\cdot dN, \\\\\ndH &= \\quad TdS - q \\cdot dJ + \\mu \\cdot dN, \\\\\ndF &= \\;-SdT + J \\cdot dq + \\mu \\cdot dN, \\\\\ndG &= \\;-SdT - q \\cdot dJ + \\mu \\cdot dN, \\\\\nd\\mathcal{G} &= \\;-SdT + J \\cdot dq - N \\cdot d\\mu. \\\\\n\\end{align*}\n\\]\nIn the simple case of a closed system, we’d have \\(dN=0\\), which reduces the total number of relations to \\(1 \\cdot 4 = 4\\). Those 4 Maxwell relations are evidently \\[\n\\begin{align*}\n&\\frac{\\partial^2 E}{\\partial S \\partial q}& &=& &\\frac{\\partial T}{\\partial q} \\bigg |_{S,N}& &=& &\\frac{\\partial J}{\\partial S} \\bigg |_{q,N}& \\\\\n&\\frac{\\partial^2 H}{\\partial S \\partial J}& &=& -&\\frac{\\partial T}{\\partial J} \\bigg |_{S,N}& &=& &\\frac{\\partial q}{\\partial S} \\bigg |_{J,N}& \\\\\n&\\frac{\\partial^2 F}{\\partial T \\partial q}& &=& -&\\frac{\\partial S}{\\partial q} \\bigg |_{T,N}& &=& &\\frac{\\partial J}{\\partial T} \\bigg |_{q,N}& \\\\\n&\\frac{\\partial^2 G}{\\partial T \\partial J}& &=& &\\frac{\\partial S}{\\partial J} \\bigg |_{T,N}& &=& &\\frac{\\partial q}{\\partial T} \\bigg |_{J,N}&. \\\\\n\\end{align*}\n\\] Though the relations themselves are non-intuitive, the process for deriving them is straight forward. Suppose for example you wanted to find a Maxwell relation for \\[\n\\frac{\\partial \\mu}{\\partial p} \\bigg |_{T,N}.\n\\] To get a relation like this, we’d need a potential that’s an explicit function of \\(p, T, N\\). That’s of course the Gibbs free energy. In this case, we’d have \\[\n\\frac{\\partial \\mu}{\\partial p} \\bigg |_{T,N} = \\frac{\\partial}{\\partial p} \\bigg |_{T,N} \\frac{\\partial G}{\\partial N} \\bigg |_{T,p} = \\frac{\\partial}{\\partial N} \\bigg |_{T,p} \\frac{\\partial G}{\\partial p} \\bigg |_{T,N} = \\frac{\\partial V}{\\partial N} \\bigg |_{T,p}.\n\\] Compare this relation with the one for an extensive system that we saw in a previous example, \\[\n\\frac{\\partial \\mu}{\\partial p} \\bigg |_{T,N} = \\frac{V}{N}.\n\\]"
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#thermodynamic-stability",
    "href": "statistical-mechanics/thermodynamics.html#thermodynamic-stability",
    "title": "Thermodynamics",
    "section": "Thermodynamic Stability",
    "text": "Thermodynamic Stability\nThermodynamics depends on systems being in equilibrium. We already know that systems at equilibrium with each other will have the same temperature, but what else can we say? Recall from classical mechanics what it means for a classical system to be in mechanical equilibrium. A classical system is said to be in mechanical equilibrium when the total forces acting on the system are zero, i.e. \\(\\mathbf{F} = \\mathbf{0}\\). For conservative systems, that’s equivalent to saying the potential \\(V=V(\\mathbf{x})\\) has gradient zero, i.e. \\(\\nabla V(\\mathbf{x}) = \\mathbf{0}\\).\nThe equilibrium point \\(\\mathbf{x}^*\\) is a stable equilibrium if \\(V(\\mathbf{x}^* )\\) is a local minimum. A sufficient condition for this to be true is that the second-order deviations around \\(V(\\mathbf{x}^* )\\) are positive, or equivalently that the Hessian of \\(V(\\mathbf{x})\\) is positive definite about \\(\\mathbf{x}^*\\), i.e. \\[\n\\delta^2 V \\equiv \\delta\\mathbf{x} \\cdot \\frac{d^2}{d\\mathbf{x}^2} V(\\mathbf{x}^*) \\cdot \\delta\\mathbf{x} = \\sum_{i,j=1}^3\\frac{\\partial^2 V}{\\partial x_i \\partial x_j}\\delta x_i \\delta x_j > 0 \\quad \\forall\\delta\\mathbf{x} \\neq \\mathbf{0}.\n\\] Intuitively, a stable equilibrium means that if the system is nudged by a small displacement it will experience a tension force pulling it back to equilibrium. Think of a spring as the canonical example.\nWe’d like to derive an analogue of this formula to characterize what it means for a thermodynamic system to be in a stable equilibrium. To do that, it’s convenient to symmetrize the positive definite expression with respect to \\(\\mathbf{F}\\) and \\(\\mathbf{x}\\). Notice that if we let \\[\n\\delta \\mathbf{F} \\equiv \\frac{d^2 V}{d\\mathbf{x}^2} \\cdot \\delta\\mathbf{x} = \\sum_{j=1}^3 \\frac{\\partial^2 V}{\\partial x_i \\partial x_j} \\delta x_j,\n\\] then we can re-write the condition for mechanical stability as \\[\n\\delta \\mathbf{F} \\cdot \\delta\\mathbf{x} = \\sum_{i=1}^3 \\delta F_i \\delta x_i > 0.\n\\] We can extend this same idea to thermodynamical systems, except there we require that all equilibrium points be stable. That is, the stability condition is required to be in thermodynamic equilibrium.\nTheorem: Any thermodynamic system in equilibrium must satisfy the stability condition \\[\n\\delta T \\delta S + \\delta J \\cdot \\delta q + \\delta \\mu \\cdot \\delta N \\geq 0.\n\\] Proof: Consider an isolated system in equilibrium. Then any two subsystems \\(A\\) and \\(B\\) must be in equilibrium with each other. It must be the case then that their intensive quantities are identical, i.e. \\[\nT \\equiv T_A = T_B, \\quad J \\equiv J_A = J_B, \\quad \\mu \\equiv \\mu_A = \\mu_B.\n\\] It must also be the case that their extensive quantities add to give the ones for the full system, \\[\nE \\equiv E_A + E_B, \\quad S \\equiv S_A + S_B, \\quad q \\equiv q_A + q_B, \\quad N \\equiv N_A + N_B.\n\\] Suppose that \\(B\\) spontaneously transfers energy to \\(A\\) in the form of both heat and work. Let’s look at the first order change in the system’s total entropy. Evidently, we’d have \\[\n\\begin{align*}\n\\delta S &= \\delta S_A + \\delta S_B \\\\\n&= \\delta\\bigg(\\frac{E_A}{T_A} - \\frac{J_A}{T_A} \\cdot q_A - \\frac{\\mu_A}{T_A} \\cdot N_A \\bigg) + \\delta\\bigg(\\frac{E_B}{T_B} - \\frac{J_B}{T_B} \\cdot q_B - \\frac{\\mu_B}{T_B} \\cdot N_B \\bigg) \\\\\n&= 2\\bigg[\\delta\\bigg(\\frac{1}{T_A}\\bigg) \\delta E_A - \\delta\\bigg(\\frac{J_A}{T_A}\\bigg)\\cdot \\delta q_A - \\delta\\bigg(\\frac{\\mu_A}{T_A}\\bigg)\\cdot \\delta N_A \\bigg] \\\\\n&= -\\frac{2}{T_A}\\bigg[\\delta T_A \\bigg(\\frac{\\delta E_A - J_A \\cdot \\delta q_A - \\mu_A \\cdot \\delta N_A}{T_A}\\bigg) + \\delta J_A \\cdot \\delta q_A + \\delta\\mu_A \\cdot \\delta N_A\\bigg] \\\\\n&= -\\frac{2}{T_A}\\big[\\delta T_A \\delta S_A + \\delta J_A \\cdot \\delta q_A + \\delta \\mu_A \\cdot \\delta N_A\\big].\n\\end{align*}\n\\] To be in equilibrium, any change to the system should lead to a decrease in entropy since entropy is maximized at equilibrium. This implies that \\(\\delta S \\leq 0\\), or equivalently that \\[\n\\delta T_A \\delta S_A + \\delta J_A \\cdot \\delta q_A + \\delta \\mu_A \\cdot \\delta N_A \\geq 0.\n\\] This condition should apply for any subsystem, which means it should apply to the whole system as well, \\[\n\\delta T \\delta S + \\delta J \\cdot \\delta q + \\delta \\mu \\cdot \\delta N \\geq 0.\n\\] The above condition was obtained assuming the system’s extensive variables \\(E,q,N\\) were held constant. In fact, since all coordinates appear symmetrically in the expression, the same result is obtained for any other set of constraints as well. \\(\\text{Q.E.D.}\\)\nAnother way of expressing the stability condition is that any second order deviations in the energy around equilibrium must be positive, i.e. \\(\\delta^2 E \\geq 0\\). This also means that the energy function should be convex about its equilibrium states.\n\nExample: Stability of a Gas\nSuppose we have some gas that’s kept a constant temperature \\(T\\) and particle number \\(N\\). If we apply the stability condition to a gas, in general we’d have \\[\n\\delta T \\delta S - \\delta p \\delta V + \\delta \\mu \\delta N \\geq 0.\n\\] Since \\(\\delta T = \\delta N = 0\\), this simplifies to just \\(-\\delta p \\delta V \\geq 0\\), or equivalently \\[\n\\delta p = \\frac{\\partial p}{\\partial V} \\bigg |_{T,N} \\delta V \\leq 0.\n\\] This says that evidently \\(p\\) must be a decreasing function of \\(V\\). If we define the compressibility of a gas by \\[\n\\kappa_T \\equiv -\\frac{1}{V} \\frac{\\partial V}{\\partial p} \\bigg |_{T,N},\n\\] then the stability condition evidently says that \\(\\kappa_T \\geq 0\\) at equilibrium. That is, the gas must be compressible, i.e. increasing the pressure on the gas should decrease its volume.\nIt’s interesting to examine the special isotherm \\(T=T_c\\) where \\(\\frac{\\partial p}{\\partial V} \\big |_{T,N} = 0\\). Along this isotherm there’s a flat spot near some critical point \\((p_c,V_c)\\). Around this point \\(\\delta p = 0\\), which means we need to look at the higher-order deviations in \\(p(V)\\). If we expand to third order about \\(V_c\\), we’d have \\[\n\\delta p \\approx \\frac{\\partial p}{\\partial V} \\bigg |_{T_c,N} \\delta V + \\frac{1}{2} \\frac{\\partial^2 p}{\\partial V^2} \\bigg |_{T_c,N} \\delta V^2 + \\frac{1}{6} \\frac{\\partial^3 p}{\\partial V^3} \\bigg |_{T_c,N} \\delta V^3.\n\\] To satisfy the stability condition we can only keep terms with odd powers in \\(\\delta V\\), since otherwise \\(\\delta p \\delta V\\) wouldn’t be non-negative. Evidently then, to maintain stability, about the critical point we must have \\[\n\\delta p \\approx \\frac{1}{6} \\frac{\\partial^3 p}{\\partial V^3} \\bigg |_{T_c,N} \\delta V^3, \\quad \\text{where} \\quad \\frac{\\partial^3 p}{\\partial V^3} \\bigg |_{T_c,N} \\geq 0.\n\\] This means that the isotherm along \\(T=T_c\\) must be a decreasing cubic with stationary point at \\((V_c,p_c)\\).\nIn reality, this condition requires that \\(p(V)\\) be an analytic function around \\(T_c\\). But it turns out that it’s not analytic around this point. There’s a phase transition. In fact, near \\(T_c\\) it’s the case that \\(\\delta p \\propto \\delta V^\\gamma\\), where \\(\\gamma \\approx 4.7\\) is an experimentally determined constant. To understand this better we’d need field theory.\n\n\n\n\n\n\nSuppose a system is closed, so \\(dN = 0\\). Then the first law says \\(dE = TdS + J \\cdot dq\\), and the stability condition says \\(\\delta T \\delta S + \\delta J \\cdot \\delta q \\geq 0\\). We can always solve for any two variables in terms of the rest. For example, we can write \\[\n\\begin{align*}\n\\delta S &= \\frac{\\partial S}{\\partial T} \\bigg |_{q,N} \\delta T + \\frac{\\partial S}{\\partial q} \\bigg |_{T,N} \\delta q, \\\\\n\\delta J &= \\frac{\\partial J}{\\partial T} \\bigg |_{q,N} \\delta T + \\frac{\\partial J}{\\partial q} \\bigg |_{T,N} \\delta q. \\\\\n\\end{align*}\n\\] Substituting these into the stability condition, we can write \\[\n\\begin{align*}\n0 &\\leq \\delta T \\bigg(\\frac{\\partial S}{\\partial T} \\bigg |_{q,N} \\delta T + \\frac{\\partial S}{\\partial q} \\bigg |_{T,N} \\delta q\\bigg) + \\bigg(\\frac{\\partial J}{\\partial T} \\bigg |_{q,N} \\delta T + \\frac{\\partial J}{\\partial q} \\bigg |_{T,N} \\delta q\\bigg) \\delta q \\\\\n&\\leq \\frac{\\partial S}{\\partial T} \\bigg |_{q,N} \\delta T^2 + \\bigg(\\frac{\\partial S}{\\partial q} \\bigg |_{T,N} + \\frac{\\partial J}{\\partial T} \\bigg |_{q,N} \\bigg) \\delta T \\delta q + \\frac{\\partial J}{\\partial q} \\bigg |_{T,N} \\delta q^2 \\\\\n&\\leq \\frac{\\partial S}{\\partial T} \\bigg |_{q,N} \\delta T^2 + \\frac{\\partial J}{\\partial q} \\bigg |_{T,N} \\delta q^2. \\\\\n\\end{align*}\n\\] The last line follows from the fact that \\(\\frac{\\partial S}{\\partial q} \\big |_{T,N} = -\\frac{\\partial J}{\\partial T} \\big |_{q,N}\\) via a Maxwell relation. Let’s look at this in two cases, first when along curves of constant \\(q\\), and then along curves of constant \\(T\\). In the first case we’d have \\(\\delta q = 0\\), which says \\[\n\\frac{\\partial S}{\\partial T} \\bigg |_{q,N} \\delta T^2 \\geq 0.\n\\] This says that along curves of constant \\(q\\), the entropy must be an increasing function of temperature. This evidently implies that the heat capacity along constant \\(q\\) must be non-negative, since \\[\nC_q = \\frac{\\delta Q}{\\partial T} \\bigg |_{q,N} = T \\frac{\\partial S}{\\partial T} \\bigg |_{q,N} \\geq 0.\n\\] Let’s now look at curves of constant \\(T\\), i.e. the isotherms. In that case we’d have \\[\n\\frac{\\partial J}{\\partial q} \\bigg |_{T,N} \\delta q^2 \\geq 0,\n\\] which evidently implies \\(J\\) must be an increasing function of \\(q\\) along the isotherms. In the case of a gas, this condition just says that pressure \\(p\\) must be a decreasing function of \\(V\\) along isotherms, which we’ve already seen."
  },
  {
    "objectID": "statistical-mechanics/thermodynamics.html#the-third-law",
    "href": "statistical-mechanics/thermodynamics.html#the-third-law",
    "title": "Thermodynamics",
    "section": "The Third Law",
    "text": "The Third Law\nSuppose we take a reversible system and change its state from \\(x_1\\) to \\(x_2\\). Then its entropy changes by \\[\n\\Delta S = S_2 - S_1 = \\int_{x_1}^{x_2} \\frac{\\delta Q_{rev}}{T}.\n\\] We can say this for any positive temperature \\(T\\). Now suppose we allow \\(T \\rightarrow 0\\). What happens to \\(\\Delta S\\)? It turns out experimentally that \\(\\Delta S \\rightarrow 0\\). This is an independent fact due to Nernst, which gives us a narrow statement of the third law of thermodynamics.\nThird Law (Nernst): The entropy of all systems at zero absolute temperature is a universal constant that can be taken to be zero. That is, between any two states we must have \\[\n\\lim_{T \\rightarrow 0} \\Delta S = 0.\n\\] This statement turns out to be experimentally equivalent to an even stronger statement. Not only does \\(\\Delta S \\rightarrow 0\\), but in fact \\(S \\rightarrow 0\\) for any substance.\nThird Law (General): The entropy of all substances at zero absolute temperature is the same universal constant, which can be defined to be zero. That is, for any system, \\[\n\\lim_{T \\rightarrow 0} S = S_0 \\equiv 0.\n\\] This extended version of the third law can be experimentally tested by looking at the behavior of certain materials like sulfur or phosphine, which can exist near absolute zero in multiple crystalline structures called allotropes. Each allotrope has its own heat capacity \\(C(T)\\). It’s been shown that as \\(T \\rightarrow 0\\), each of these paths sends \\(C \\rightarrow 0\\), which implies \\(S \\rightarrow 0\\) as well.\n\n\n\n\n\nHere are a few notable consequences that follow from the third law. First, since \\(S \\rightarrow 0\\) as \\(T \\rightarrow 0\\), it must also be true that any partial derivative of \\(S\\) must go to zero as well, \\[\n\\lim_{T \\rightarrow 0} \\frac{\\partial S}{\\partial X} \\bigg |_T = 0.\n\\] The heat capacities must go to zero as well since \\[\n\\Delta S = \\int_0^T \\frac{C(T')}{T'} dT' \\rightarrow 0.\n\\] This integral would diverge as \\(T \\rightarrow 0\\) unless \\(C \\rightarrow 0\\) as well.\nThe thermal expansion coefficients must also go to zero since by a Maxwell relation we have \\[\n\\alpha \\equiv \\frac{1}{q} \\frac{\\partial q}{\\partial T} \\bigg |_J = \\frac{1}{q} \\frac{\\partial S}{\\partial J} \\bigg |_T \\rightarrow 0.\n\\] The last consequence of note is that it must be impossible to cool any system to absolute zero in a finite number of steps, which for practical purposes means it’s impossible to cool a system to zero exactly. For example, suppose we tried to cool a gas by adiabatically reducing its pressure. Since all \\(S(T)\\) curves must intersect at \\(0\\), any successive step would involve progressively smaller changes in \\(S\\) and \\(T\\) as \\(T \\rightarrow 0\\).\n\n\n\n\n\nIt’s worth mentioning that in a certain sense the third law is less reliable than the other laws of thermodynamics since at its root its validity rests entirely on quantum mechanics, and the quantum mechanical behavior of different systems can vary wildly near absolute zero. This contrasts with the other laws, which at a microscopic level only depend on things like the conservation of energy, or the emergent effect of a large number of degrees of freedom. We’ll see a microscopic derivation of each of these laws in future chapters."
  }
]